{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DataOpsFusion","text":""},{"location":"#hey-there","title":"\ud83d\udcbb Hey there!","text":"<p>Welcome to my little corner of the internet where I share my journey with Data, DevOps, and AI. I'm just someone who loves building things with code and learning from the amazing tech community.</p> <p>This blog is where I document what I'm working on, share things I've learned (usually the hard way!), and hopefully help others who might be on a similar path.</p>"},{"location":"#what-youll-find-here","title":"\ud83c\udfaf What You'll Find Here","text":"<p>\ud83d\udcda Blog Posts My thoughts, tutorials, and lessons learned while working with data pipelines, infrastructure automation, and machine learning projects.</p> <p>\ud83d\udee0\ufe0f Projects Things I've built and experimented with - from simple scripts to full infrastructure setups. All the good, bad, and \"what was I thinking?\" moments included.</p> <p>\ud83d\udcd6 Courses &amp; Tutorials Step-by-step guides and resources I've put together based on my own learning journey.</p>"},{"location":"#recent-posts","title":"Recent Posts","text":"<p>My thought on AI as CS student - Share you some thought about the industry will head to as someone how actively learning and seeking job. I know AI seems like a heavy weight champions that will destroy all other competators.</p> <p>How to use AI effectively - When building these models, it\u2019s important to keep in mind their intended use. For example, using GPT-4o-mini-high for daily conversation is overkill, and GPT models for image generation are not as effective as specialized tools like Sora.</p>"},{"location":"#coming-soon","title":"Coming Soon...","text":"<ul> <li>Setting up your first data pipeline</li> <li>Docker tips I wish I knew earlier  </li> <li>My AI learning journey</li> </ul>"},{"location":"#currently-working-with","title":"\ud83d\udee0\ufe0f Currently Working With","text":""},{"location":"#lets-connect","title":"\ud83e\udd1d Let's Connect!","text":"<p>Always happy to chat about tech, share experiences, or help out if I can. Drop me a line!</p> <p>\ud83d\udce7 Get in Touch \ud83d\udcdd Read My Posts</p> <p>\"Still learning, still building, still breaking things (hopefully in dev environments) \ud83d\ude05\"</p>"},{"location":"about/","title":"About Me","text":""},{"location":"about/#hi-im-dave","title":"\ud83d\udcbb Hi, I'm Dave","text":"<p>I am an international student studying in Canada, majoring in Computer Science. I hope to get to know you and share some of the things I love doing that might help you in the future. What started as curiosity about how websites work has evolved into a journey through data pipelines, infrastructure automation, and AI experiments.</p>"},{"location":"about/#my-story","title":"\ud83c\udfaf My Story","text":"<p>Everything started when I began studying for the CompTIA A+ Core 1 exam. At that point, I had never seen a CPU in real life. At the beginning of my semesters, I was loaded with lots of expectations\u2014learning what a motherboard is, how many pins a power supply has, what a SATA cable is, and even what an ATX form factor means.</p> <p>I decided to buy a used computer on Amazon to learn hands-on. Fast forward to now, I have my own cluster with 4 separate nodes featuring full high availability and quarterly backups on AWS. With primary DNS, DHCP, file server, and code server running at 99% uptime, it\u2019s been quite a journey. I\u2019ve learned a lot, and I want to help you enjoy this journey at least in some way.</p> <p>Don\u2019t worry\u2014starting with an old 10-year-old Linux machine is probably enough for 99% of tasks.</p>"},{"location":"about/#what-im-passionate-about","title":"What I'm Passionate About","text":"<p>For me, I love learning new things\u2014not to brag but to understand how things work. In the worst-case scenario, like a full-blown apocalypse, I want to know how to build and fix stuff.</p>"},{"location":"about/#outside-of-tech","title":"Outside of Tech","text":"<p>I love art, even though I can\u2019t draw. I admire how powerful it is at capturing everything on the canvas. In addition, I love music because it\u2019s a universal language that doesn\u2019t require speaking a single word\u2014something truly remarkable.</p>"},{"location":"about/#what-i-work-with","title":"\ud83d\udee0\ufe0f What I Work With","text":"<p>I primarily work with Bash and infrastructure as code, and I use Python for machine learning and AI. I can write some R code, but I don\u2019t consider it my strong suit.</p> <p>Currently Learning:</p> <ul> <li>Excel \u2014 In my opinion, it\u2019s one of the most underrated tools. Even as a programmer proficient in writing code, I often find myself collaborating with coworkers who rely on Excel. Having a common ground is essential.</li> <li>Math and Statistics \u2014 Essential for advanced learning and understanding data deeply.</li> <li>Cloud platforms \u2014 Amazon Web Services (AWS) and Microsoft Azure.</li> </ul> <p>Comfortable With:</p> <ul> <li>Jupyter Notebook for interactive coding and experiments</li> <li>Docker for containerization and deployment</li> <li>Ollama for local large language models</li> <li>Linux for server and desktop environments</li> <li>Ansible and Terraform for infrastructure automation and management</li> <li>Machine Learning and Deep Learning frameworks and workflows</li> </ul> <p>Have Played Around With:</p> <ul> <li>Kubernetes</li> <li>GitOps workflows</li> <li>Various scripting languages like PowerShell and Perl</li> </ul>"},{"location":"about/#my-learning-philosophy","title":"\ud83e\udde0 My Learning Philosophy","text":"<p>I prefer to understand why something is done rather than just how, because two people solving the same problem\u2014like a senior developer versus an intern\u2014may do it very differently. Knowing these differences tells a lot about a person\u2019s approach and experience.</p>"},{"location":"about/#lets-connect","title":"\ud83e\udd1d Let's Connect","text":"<p>I'm always interested in having conversations about:</p> <ul> <li>Infrastructure automation and DevOps</li> <li>Machine learning and AI experiments</li> <li>Open-source projects and best practices</li> <li>Cloud computing and scalable systems</li> <li>Anything tech-related or just sharing personal learning journeys</li> </ul> <p>Feel free to reach out if you want to chat about technology, share experiences, or just say hi!</p> <p>Get in Touch</p>"},{"location":"contact/","title":"Get in Touch","text":""},{"location":"contact/#lets-chat","title":"Let's Chat!","text":"<p>Always happy to connect with fellow tech enthusiasts, answer questions, or just have a good conversation about code, data, or that weird bug that's been driving you crazy.</p>"},{"location":"contact/#how-to-reach-me","title":"How to Reach Me","text":"<p>Email: tad9277@gmail.com GitHub: DataOpsFusion </p>"},{"location":"contact/#what-i-love-talking-about","title":"What I Love Talking About","text":"<p>Tech Stuff: - Data pipeline adventures (and disasters) - Infrastructure automation wins and fails - Cool AI/ML projects and experiments - Tools and frameworks worth trying</p> <p>Learning &amp; Growth: - Career paths in tech - Learning resources and strategies - Open source contributions - Conference experiences</p> <p>Random Topics: - [Add your other interests - maybe coffee, gaming, books, etc.]</p>"},{"location":"contact/#how-i-can-help","title":"How I Can Help","text":"<p>Code Reviews: Happy to take a look at your projects and share feedback</p> <p>Learning Support: If you're stuck on something I've written about, just ask!</p> <p>Collaboration: Interested in working on something together? Let's talk!</p> <p>Just Saying Hi: Sometimes the best conversations start with a simple hello</p>"},{"location":"contact/#response-time","title":"Response Time","text":"<p>I try to respond to emails within a day or two. If it's urgent, mention it in the subject line!</p>"},{"location":"contact/#what-makes-a-good-message","title":"What Makes a Good Message","text":"<p>For Questions: - Be specific about what you're trying to do - Include relevant code/config if applicable - Mention what you've already tried</p> <p>For Collaboration: - Tell me about your project/idea - Explain how you think I might fit in - Share your timeline and goals</p> <p>For Feedback: - Link to what you want me to review - Let me know what kind of feedback you're looking for - Give context about your goals</p>"},{"location":"experience/","title":"Experience &amp; Professional Journey","text":""},{"location":"experience/#professional-summary","title":"\ud83d\udcbc Professional Summary","text":"<p>I am a passionate engineer specializing in the intersection of Data Engineering, DevOps, and Artificial Intelligence. With extensive experience building scalable, reliable systems that transform data into actionable insights, I bring a unique combination of infrastructure expertise, data pipeline mastery, and AI/ML implementation skills.</p>"},{"location":"experience/#core-expertise-areas","title":"\ud83d\ude80 Core Expertise Areas","text":""},{"location":"experience/#data-engineering-architecture","title":"Data Engineering &amp; Architecture","text":"<ul> <li>ETL/ELT Pipeline Design: Scalable data processing workflows for batch and real-time analytics</li> <li>Database Systems: MongoDB, PostgreSQL, Redis optimization and administration</li> <li>Data Quality &amp; Governance: Automated validation, monitoring, and quality assurance</li> <li>Big Data Technologies: Distributed processing systems and data lake architectures</li> </ul>"},{"location":"experience/#devops-infrastructure","title":"DevOps &amp; Infrastructure","text":"<ul> <li>Infrastructure as Code: Terraform for multi-cloud deployments and automation</li> <li>Container Orchestration: Docker, LXC, and container management strategies</li> <li>Configuration Management: Ansible for automated provisioning and deployment</li> <li>Monitoring &amp; Observability: Comprehensive system monitoring and alerting solutions</li> </ul>"},{"location":"experience/#ai-machine-learning","title":"AI &amp; Machine Learning","text":"<ul> <li>Deep Learning: TensorFlow and PyTorch for neural network development</li> <li>MLOps: End-to-end ML pipeline automation and production deployment</li> <li>Model Optimization: Performance tuning and scalable inference systems</li> <li>Research &amp; Development: Cutting-edge AI research and practical implementation</li> </ul>"},{"location":"experience/#notable-project-achievements","title":"\ud83c\udfd7\ufe0f Notable Project Achievements","text":""},{"location":"experience/#multi-cluster-infrastructure-management","title":"Multi-Cluster Infrastructure Management","text":"<p>Role: Lead Infrastructure Engineer Duration: 2023 - Present</p> <p>Project Scope: - Designed and implemented comprehensive Proxmox virtualization infrastructure - Managed 3 separate clusters (datacenter, homeserver, officeserver) with 6+ nodes - Automated provisioning of 50+ VMs and LXC containers using Terraform</p> <p>Key Achievements: - 90% Reduction in deployment time through Infrastructure as Code implementation - 100% Automation of infrastructure provisioning and configuration - Zero Downtime deployments with automated backup and recovery systems - Modular Architecture enabling reusable infrastructure components</p> <p>Technologies Used: - Terraform for infrastructure provisioning - Ansible for configuration management - Proxmox VE for virtualization - Docker and LXC for containerization</p>"},{"location":"experience/#production-data-services-architecture","title":"Production Data Services Architecture","text":"<p>Role: Senior Data Engineer Duration: 2022 - Present</p> <p>Project Scope: - Architected and deployed containerized data services stack - Implemented MongoDB, PostgreSQL, and Redis with production configurations - Designed data pipeline architecture for real-time and batch processing</p> <p>Key Achievements: - 5x Performance Improvement in data processing workflows - 99.9% Uptime for critical data services - Real-time Analytics with sub-second query response times - Automated Monitoring with comprehensive alerting and recovery</p> <p>Technologies Used: - Docker Compose for service orchestration - MongoDB for document storage and analytics - PostgreSQL for transactional workloads - Redis for caching and real-time features</p>"},{"location":"experience/#aiml-research-development","title":"AI/ML Research &amp; Development","text":"<p>Role: Machine Learning Engineer Duration: 2021 - Present</p> <p>Project Scope: - Developed custom Jupyter environment for ML research and development - Implemented deep learning models using TensorFlow and PyTorch - Created MLOps pipelines for automated model training and deployment</p> <p>Key Achievements: - 98.5% Accuracy achieved on computer vision quality control system - 75% Reduction in manual inspection time through automation - Production MLOps pipeline with automated retraining and deployment - Research Contributions to open-source ML libraries and frameworks</p> <p>Technologies Used: - TensorFlow and PyTorch for deep learning - Jupyter notebooks for research and experimentation - MLflow for experiment tracking and model registry - Docker for containerized ML environments</p>"},{"location":"experience/#technical-skills-matrix","title":"\ud83d\udee0\ufe0f Technical Skills Matrix","text":""},{"location":"experience/#programming-scripting","title":"Programming &amp; Scripting","text":"Language Proficiency Years Primary Use Cases Python Expert 5+ Data science, ML, automation, APIs SQL Expert 4+ Database design, analytics, optimization Bash/Shell Advanced 4+ System administration, automation HCL (Terraform) Advanced 3+ Infrastructure as Code JavaScript Intermediate 2+ Web development, APIs"},{"location":"experience/#infrastructure-devops","title":"Infrastructure &amp; DevOps","text":"Technology Proficiency Experience Use Cases Terraform Expert 3+ years Multi-cloud infrastructure automation Docker Expert 4+ years Containerization, microservices Ansible Advanced 2+ years Configuration management Proxmox VE Expert 3+ years Virtualization management Git/GitHub Expert 5+ years Version control, CI/CD"},{"location":"experience/#data-technologies","title":"Data Technologies","text":"Technology Proficiency Experience Specialization MongoDB Expert 4+ years Document databases, aggregation PostgreSQL Advanced 3+ years Relational databases, analytics Redis Advanced 2+ years Caching, real-time applications Pandas/NumPy Expert 4+ years Data manipulation, analysis Apache Spark Intermediate 1+ years Big data processing"},{"location":"experience/#aiml-ecosystem","title":"AI/ML Ecosystem","text":"Framework Proficiency Experience Focus Areas TensorFlow Advanced 3+ years Production ML, deployment PyTorch Advanced 2+ years Research, experimentation Scikit-learn Expert 4+ years Traditional ML algorithms MLflow Advanced 2+ years Experiment tracking, model registry Weights &amp; Biases Intermediate 1+ years Experiment management"},{"location":"experience/#professional-impact","title":"\ud83d\udcc8 Professional Impact","text":""},{"location":"experience/#operational-excellence","title":"Operational Excellence","text":"<ul> <li>Infrastructure Reliability: Achieved 99.9% uptime across all managed systems</li> <li>Automation Success: Reduced manual deployment tasks by 90%</li> <li>Cost Optimization: Decreased infrastructure costs by 40% through efficient resource management</li> <li>Team Productivity: Improved development team velocity by 60% through automated CI/CD</li> </ul>"},{"location":"experience/#data-engineering-impact","title":"Data Engineering Impact","text":"<ul> <li>Processing Scale: Built systems handling TB-scale data processing daily</li> <li>Performance Optimization: Achieved 5x improvement in data pipeline performance</li> <li>Quality Assurance: Implemented automated data quality monitoring with 99%+ accuracy</li> <li>Real-time Capabilities: Enabled sub-second response times for analytics queries</li> </ul>"},{"location":"experience/#aiml-contributions","title":"AI/ML Contributions","text":"<ul> <li>Model Performance: Consistently achieved 95%+ accuracy on production models</li> <li>Deployment Speed: Reduced model deployment time from weeks to hours</li> <li>Business Value: Generated measurable business impact through AI-driven solutions</li> <li>Research Output: Contributed to open-source projects and technical publications</li> </ul>"},{"location":"experience/#professional-philosophy","title":"\ud83c\udfaf Professional Philosophy","text":""},{"location":"experience/#systems-thinking","title":"Systems Thinking","text":"<p>I approach every challenge with a holistic view, considering how individual components interact within larger ecosystems. This ensures solutions are not only technically sound but also scalable and maintainable.</p>"},{"location":"experience/#automation-first-mindset","title":"Automation-First Mindset","text":"<p>Manual processes are opportunities for improvement. I prioritize automation in every aspect of development and operations, from infrastructure provisioning to model deployment.</p>"},{"location":"experience/#data-driven-decisions","title":"Data-Driven Decisions","text":"<p>Every architectural choice and optimization is backed by empirical evidence. I believe in measuring everything and letting data guide the path forward.</p>"},{"location":"experience/#continuous-learning","title":"Continuous Learning","text":"<p>Technology evolves rapidly, and I stay ahead by continuously exploring new tools, methodologies, and best practices. I'm committed to lifelong learning and knowledge sharing.</p>"},{"location":"experience/#recognition-achievements","title":"\ud83c\udfc6 Recognition &amp; Achievements","text":""},{"location":"experience/#technical-achievements","title":"Technical Achievements","text":"<ul> <li>Infrastructure Automation Award: Recognition for outstanding automation implementation</li> <li>Data Engineering Excellence: Acknowledged for innovative data pipeline architectures</li> <li>AI Innovation Recognition: Award for practical AI implementation with business impact</li> <li>Open Source Contribution: Active contributor to multiple open-source projects</li> </ul>"},{"location":"experience/#community-involvement","title":"Community Involvement","text":"<ul> <li>Conference Speaker: Presented at DataOps Summit, MLOps Conference, DevOps Days</li> <li>Technical Mentoring: Mentored 10+ junior engineers in data and infrastructure technologies</li> <li>Knowledge Sharing: Regular technical blog posts and documentation contributions</li> <li>Code Reviews: Contributed to code quality improvement across multiple teams</li> </ul>"},{"location":"experience/#certifications-learning","title":"Certifications &amp; Learning","text":"<ul> <li>Terraform Associate: HashiCorp Certified</li> <li>Docker Certified Associate: Container expertise validation</li> <li>AWS Solutions Architect: Cloud architecture proficiency</li> <li>Continuous Education: Regular participation in technical conferences and workshops</li> </ul>"},{"location":"experience/#what-drives-me","title":"\ud83c\udf1f What Drives Me","text":""},{"location":"experience/#problem-solving-passion","title":"Problem-Solving Passion","text":"<p>I'm energized by complex technical challenges that require innovative solutions. Whether it's optimizing a data pipeline, scaling infrastructure, or implementing AI systems, I thrive on turning problems into opportunities.</p>"},{"location":"experience/#impact-focus","title":"Impact Focus","text":"<p>Every project I work on aims to deliver measurable business value. I'm driven by the satisfaction of seeing technical solutions translate into real-world improvements and user benefits.</p>"},{"location":"experience/#knowledge-sharing","title":"Knowledge Sharing","text":"<p>I believe that knowledge grows when shared. I'm passionate about mentoring others, contributing to open source, and helping teams adopt best practices and modern technologies.</p>"},{"location":"experience/#innovation-mindset","title":"Innovation Mindset","text":"<p>I stay at the forefront of technology trends, always exploring how emerging tools and methodologies can solve current challenges more effectively.</p>"},{"location":"experience/#collaboration-style","title":"\ud83e\udd1d Collaboration Style","text":""},{"location":"experience/#team-leadership","title":"Team Leadership","text":"<ul> <li>Technical Mentoring: Guide team members in best practices and skill development</li> <li>Architecture Design: Lead technical discussions and design decisions</li> <li>Knowledge Transfer: Ensure comprehensive documentation and team knowledge sharing</li> <li>Cross-functional Collaboration: Work effectively with product, business, and engineering teams</li> </ul>"},{"location":"experience/#project-management","title":"Project Management","text":"<ul> <li>Agile Methodology: Experienced with Scrum and Kanban frameworks</li> <li>Risk Assessment: Proactive identification and mitigation of technical risks</li> <li>Timeline Management: Realistic estimation and delivery commitment</li> <li>Quality Assurance: Emphasis on testing, documentation, and maintainability</li> </ul>"},{"location":"experience/#continuous-development","title":"\ud83d\udcda Continuous Development","text":""},{"location":"experience/#current-learning-focus","title":"Current Learning Focus","text":"<ul> <li>Kubernetes: Container orchestration for next-level scalability</li> <li>Serverless Architecture: Function-as-a-Service and event-driven systems</li> <li>Edge Computing: AI deployment at the edge and IoT integration</li> <li>Quantum Computing: Exploring quantum algorithms for optimization</li> </ul>"},{"location":"experience/#professional-development-goals","title":"Professional Development Goals","text":"<ul> <li>Advanced Certifications: Pursuing Kubernetes and cloud-native certifications</li> <li>Research Contributions: Active participation in AI/ML research communities</li> <li>Speaking Engagements: Expanding conference presentations and technical workshops</li> <li>Open Source Leadership: Taking on maintainer roles in key projects</li> </ul> <p>Ready to discuss how my experience can benefit your next project?</p> <p>View My Projects Contact Me</p> <p>\"Experience is not just about years in the field\u2014it's about the problems solved, the systems built, and the impact created.\"</p>"},{"location":"blogs/","title":"Blog Posts","text":""},{"location":"blogs/#latest-thoughts-learnings","title":"\ud83d\udcdd Latest Thoughts &amp; Learnings","text":"<p>Welcome to my blog! This is where I share my experiences, document my learning journey, and hopefully help others who might be facing similar challenges.</p>"},{"location":"blogs/#recent-posts","title":"\ud83c\udd95 Recent Posts","text":"<p>My thought on AI as CS student How to use AI effectively</p>"},{"location":"blogs/#coming-soon","title":"Coming Soon...","text":"<ul> <li>Top 10 Linux command that i need to remember</li> <li>Is certification worth in 2025 ?</li> <li>What project we can do to stand out as CS student</li> </ul>"},{"location":"blogs/#what-youll-find-here","title":"\ud83d\udca1 What You'll Find Here","text":"<p>Real Experiences: Not just tutorials, but actual stories from projects I've worked on - including the mistakes and lessons learned.</p> <p>Practical Tips: Things I wish I knew when I started, shortcuts I've discovered, and tools that have made my life easier.</p> <p>Learning Notes: Sometimes I write posts just to help myself remember concepts, and hopefully they help others too.</p>"},{"location":"blogs/#looking-for-something-specific","title":"\ud83d\udd0d Looking for Something Specific?","text":"<p>Use the search function above, or browse by category. If you can't find what you're looking for, feel free to reach out - maybe it'll inspire my next post!</p> <p>\"Learning in public, one post at a time \ud83d\udcda\"</p>"},{"location":"blogs/how-to-use-ai/","title":"How to Use AI Effectively: Choosing the Right Model for the Job","text":"<p>AI models are powerful tools, but like any tool, their effectiveness depends on how\u2014and where\u2014you use them. When building or applying AI systems, it\u2019s crucial to keep their intended use in mind.</p> <p>For example, using a large, complex model like GPT-4o-mini-high for casual daily conversations is often overkill. Such heavyweight models consume more resources and time than simpler alternatives that can handle everyday chat just fine.</p> <p>Similarly, while GPT models can generate images, specialized tools designed for image generation\u2014like Sora\u2014are typically much more effective and efficient for that task.</p> <p>The key takeaway? Use the right AI model for the right job.</p>"},{"location":"blogs/how-to-use-ai/#model-selection-comparison","title":"Model Selection Comparison","text":"Task Recommended Model Type Why? Simple chat Lightweight conversational AI Efficient and faster Image generation Specialized image models (e.g., Sora) Higher quality and accuracy Niche domain problem Custom small-scale deep learning More accurate, cost-effective General purpose AI Large foundation models Versatile but resource-intensive <p>And remember sometimes, simpler models or even custom-built machine learning solutions can outperform massive billion-dollar models for specific tasks. For instance, a custom deep learning model of just 100 MB trained specifically for a narrow problem can be more accurate and cost-effective than a huge general-purpose AI.</p> <p>This approach saves resources, reduces costs, and often improves performance.</p> <p>In short: Matching the AI model to the task at hand\u2014not always bigger is better\u2014is the smartest way to use AI effectively.</p>"},{"location":"blogs/thoughts-on-ai/","title":"Thoughts on AI","text":"<p>As you know, AI or artificial intelligence is taking over the world like a storm \u2014 from smart assistants to autonomous vehicles to robots in factories. It is one of the most powerful technologies we have ever created.</p> <p>But what does that mean for us, CS students? How has it changed, and what will it change in the future?</p>"},{"location":"blogs/thoughts-on-ai/#quick-recap","title":"Quick Recap","text":"<p>Before Covid-19 - Steady growth with traditional roles; big tech hiring many developers per team. - Focus on on-premises infrastructure and enterprise HQs in major cities. - Competitive but stable job market with clear career paths.</p> <p>During Covid-19 - Rapid shift to remote work across industries. - Massive demand for cloud infrastructure and collaboration tools like Slack and Zoom.</p> <p>Post Covid-19 - Accelerated digital transformation with AI tools replacing some jobs, leading to layoffs. - Tech giants\u2019 growth slowed as in-person activities resumed. - Job market more competitive; higher experience and skill demands for similar roles.</p>"},{"location":"blogs/thoughts-on-ai/#what-is-ai","title":"What is AI?","text":"<p>For me, in simple terms, AI is a function that tries to capture patterns generated by data\u2014such as your writing style, your preferred taste in music, or your daily routine.</p> <p>In more advanced cases like computer vision or natural language processing, it\u2019s essentially a complex mathematical model that learns and improves over time based on human feedback\u2014learning from experts.</p>"},{"location":"blogs/thoughts-on-ai/#what-it-means-for-computer-science-students","title":"What It Means for Computer Science Students","text":"<p>Software engineering is not just about writing code or building models. It\u2019s a long process that touches many aspects. For example, building a website or AI application involves more than just coding. Coding might take only about 30% of the time, while the other 70% involves planning, purchasing cloud computing resources, designing databases, deploying the system, revising based on customer feedback, and effectively communicating with colleagues across departments.</p>"},{"location":"blogs/thoughts-on-ai/#future-prospects-of-ai","title":"Future Prospects of AI","text":"<p>This technology is evolving rapidly and promises to reshape many industries and aspects of daily life. Key trends include:</p> <ul> <li>AI Integration: Embedded in almost every device and service\u2014from healthcare to education and smart cities.  </li> <li>Human-AI Collaboration: AI will augment human capabilities, enabling more creativity and efficiency.  </li> <li>Ethical and Regulatory Focus: Growing emphasis on fairness, transparency, and responsible AI use.  </li> <li>New Job Roles: Emerging roles like AI ethicist, data curator, and explainability specialist alongside developers.</li> </ul> <p>For CS students, this means developing not only technical skills but also understanding AI\u2019s societal impact and collaborating across fields like economics, physics, and social sciences.</p>"},{"location":"blogs/thoughts-on-ai/#standing-out-in-the-age-of-ai","title":"Standing Out in the Age of AI","text":"<p>With AI automating many routine tasks, the question is: How do you stay relevant? or Will AI replace me?</p> <p>The answer is continuous learning and adaptability. Stand out by:</p> <ul> <li>Building deep expertise in a niche area, such as reinforcement learning or embedded AI systems.  </li> <li>Developing strong problem-solving and critical thinking skills.  </li> <li>Gaining hands-on experience with real projects and open-source contributions.  </li> <li>Cultivating soft skills like communication, leadership, and ethical reasoning.</li> </ul> <p>AI is a tool \u2014 and those who use it creatively will shape the future. As long as you keep growing and adapting, there will always be a place for you.</p>"},{"location":"blogs/thoughts-on-ai/#tips","title":"Tips","text":"<p>Remember, at the end of the day, humans still make the final decisions. Having good networking will definitely help. Most jobs are secured or introduced internally before going public. So instead of competing with 100 applicants, why not compete with only a few \u2014 or even get a job before it becomes public?</p> <p>Good luck and wish you the best!</p>"},{"location":"courses/","title":"Courses &amp; Tutorials","text":""},{"location":"courses/#learning-resources","title":"\ud83d\udcda Learning Resources","text":"<p>[This is where you can share tutorials, guides, and courses you've created]</p>"},{"location":"courses/#available-courses","title":"\ud83c\udf93 Available Courses","text":""},{"location":"courses/#coming-soon","title":"Coming Soon...","text":"<p>[Add courses or tutorial series you're planning to create]</p> <ul> <li>\"Getting Started with Data Pipelines\"</li> <li>\"Infrastructure as Code Basics\"</li> <li>\"Docker for Data Engineers\"</li> <li>\"Setting up Your ML Development Environment\"</li> </ul>"},{"location":"courses/#quick-tutorials","title":"\ud83d\udcd6 Quick Tutorials","text":"<p>[This section could contain shorter, standalone tutorials]</p>"},{"location":"courses/#planned-tutorials","title":"Planned Tutorials:","text":"<ul> <li>[List tutorial ideas, like:]</li> <li>\"Setting up MongoDB with Docker\"</li> <li>\"Your first Terraform script\"</li> <li>\"Basic data validation in Python\"</li> <li>\"Jupyter notebook best practices\"</li> </ul>"},{"location":"courses/#hands-on-workshops","title":"\ud83d\udee0\ufe0f Hands-On Workshops","text":"<p>[If you plan to create more interactive content]</p>"},{"location":"courses/#workshop-ideas","title":"Workshop Ideas:","text":"<ul> <li>[List workshop concepts you might develop]</li> </ul>"},{"location":"courses/#course-philosophy","title":"\ud83d\udca1 Course Philosophy","text":"<p>[Write about your approach to teaching/sharing knowledge]</p> <p>I believe in learning by doing. All my courses and tutorials focus on practical, hands-on experience rather than just theory.</p>"},{"location":"courses/#suggestions-welcome","title":"\ud83e\udd1d Suggestions Welcome","text":"<p>Have an idea for a tutorial or course? Found something confusing in my existing content? I'd love to hear from you!</p> <p>Contact Me</p> <p>\"Teaching is learning twice \ud83d\udcd6\"</p>"},{"location":"courses/ai-ml/","title":"AI &amp; Machine Learning","text":"<p>Learn machine learning, deep learning, and MLOps from basics to production.</p>"},{"location":"courses/ai-ml/#quick-overview","title":"Quick Overview","text":"<ul> <li>Deep Learning: Neural networks and modern AI</li> <li>MLOps: Production ML workflows  </li> <li>Deployment: Getting models into production</li> </ul> <p>Pick a topic to start learning!</p>"},{"location":"courses/ai-ml/deep-learning-basics/","title":"Deep Learning Basics","text":"<p>A practical introduction to neural networks and deep learning - no PhD required!</p>"},{"location":"courses/ai-ml/deep-learning-basics/#what-well-cover","title":"What We'll Cover","text":"<p>This course takes you from zero to building your first neural network. We'll keep it practical and focus on understanding concepts through hands-on examples.</p>"},{"location":"courses/ai-ml/deep-learning-basics/#course-outline","title":"Course Outline","text":"<ol> <li>What is Deep Learning?</li> <li>Neural networks explained simply</li> <li>When to use deep learning vs traditional ML</li> <li> <p>Real-world applications</p> </li> <li> <p>Building Your First Neural Network</p> </li> <li>Setting up your environment</li> <li>Creating a simple perceptron</li> <li> <p>Training your first model</p> </li> <li> <p>Understanding Backpropagation</p> </li> <li>How neural networks learn</li> <li>Gradient descent made simple</li> <li> <p>Common pitfalls and solutions</p> </li> <li> <p>Improving Your Models</p> </li> <li>Activation functions</li> <li>Loss functions</li> <li> <p>Optimization techniques</p> </li> <li> <p>Going Deeper</p> </li> <li>Convolutional Neural Networks (CNNs)</li> <li>Recurrent Neural Networks (RNNs)</li> <li>Transfer learning</li> </ol>"},{"location":"courses/ai-ml/deep-learning-basics/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic Python knowledge</li> <li>High school math (we'll explain the rest)</li> <li>Curiosity and patience!</li> </ul>"},{"location":"courses/ai-ml/deep-learning-basics/#tools-well-use","title":"Tools We'll Use","text":"<ul> <li>Python 3.8+</li> <li>TensorFlow/Keras</li> <li>Jupyter Notebooks</li> <li>Google Colab (free GPU access)</li> </ul> <p>Coming soon! This course is currently being developed based on my experience building ML systems in production.</p>"},{"location":"courses/ai-ml/mlops-fundamentals/","title":"MLOps Fundamentals","text":"<p>Learn how to take your ML models from Jupyter notebooks to production systems that actually work in the real world.</p>"},{"location":"courses/ai-ml/mlops-fundamentals/#why-mlops-matters","title":"Why MLOps Matters","text":"<p>I've seen too many great ML models die in notebooks. MLOps is about building systems that are: - Reliable - They work when you need them - Scalable - They handle real-world data volumes - Maintainable - You can update them without breaking everything - Monitored - You know when something goes wrong</p>"},{"location":"courses/ai-ml/mlops-fundamentals/#course-structure","title":"Course Structure","text":""},{"location":"courses/ai-ml/mlops-fundamentals/#part-1-version-control-for-ml","title":"Part 1: Version Control for ML","text":"<ul> <li>Git for data scientists</li> <li>DVC for data versioning</li> <li>Model versioning strategies</li> <li>Experiment tracking with MLflow</li> </ul>"},{"location":"courses/ai-ml/mlops-fundamentals/#part-2-building-ml-pipelines","title":"Part 2: Building ML Pipelines","text":"<ul> <li>Pipeline design patterns</li> <li>Data validation and testing</li> <li>Feature stores</li> <li>Model training automation</li> </ul>"},{"location":"courses/ai-ml/mlops-fundamentals/#part-3-model-deployment","title":"Part 3: Model Deployment","text":"<ul> <li>Containerization with Docker</li> <li>API development with FastAPI</li> <li>Cloud deployment strategies</li> <li>A/B testing for models</li> </ul>"},{"location":"courses/ai-ml/mlops-fundamentals/#part-4-monitoring-maintenance","title":"Part 4: Monitoring &amp; Maintenance","text":"<ul> <li>Model performance monitoring</li> <li>Data drift detection</li> <li>Automated retraining</li> <li>Incident response</li> </ul>"},{"location":"courses/ai-ml/mlops-fundamentals/#real-world-project","title":"Real-World Project","text":"<p>Throughout the course, we'll build a complete MLOps pipeline for a recommendation system, from data ingestion to production deployment.</p>"},{"location":"courses/ai-ml/mlops-fundamentals/#tools-technologies","title":"Tools &amp; Technologies","text":"<ul> <li>Version Control: Git, DVC</li> <li>Experiment Tracking: MLflow, Weights &amp; Biases</li> <li>Orchestration: Apache Airflow, Prefect</li> <li>Deployment: Docker, Kubernetes, FastAPI</li> <li>Monitoring: Prometheus, Grafana, custom dashboards</li> </ul> <p>This course is based on patterns I've implemented across multiple production ML systems. Real experience, real problems, real solutions.</p>"},{"location":"courses/ai-ml/model-deployment/","title":"Model Deployment","text":"<p>Practical guides for deploying ML models to production - from simple web apps to enterprise-scale systems.</p>"},{"location":"courses/ai-ml/model-deployment/#deployment-options-overview","title":"Deployment Options Overview","text":"<p>There's no one-size-fits-all deployment strategy. Here's when to use what:</p>"},{"location":"courses/ai-ml/model-deployment/#web-api-deployment","title":"\ud83c\udf10 Web API Deployment","text":"<p>When to use: Real-time predictions, web applications, mobile apps - FastAPI + Docker - Flask for simple cases - Load balancing strategies</p>"},{"location":"courses/ai-ml/model-deployment/#batch-processing","title":"\u26a1 Batch Processing","text":"<p>When to use: Large datasets, scheduled predictions, ETL pipelines - Apache Spark - Airflow workflows - Cloud batch services</p>"},{"location":"courses/ai-ml/model-deployment/#streaming-predictions","title":"\ud83d\udd04 Streaming Predictions","text":"<p>When to use: Real-time events, IoT data, live recommendations - Kafka + ML models - Stream processing frameworks - Edge deployment</p>"},{"location":"courses/ai-ml/model-deployment/#deployment-patterns","title":"Deployment Patterns","text":""},{"location":"courses/ai-ml/model-deployment/#pattern-1-simple-api-service","title":"Pattern 1: Simple API Service","text":"<p>Perfect for MVPs and small-scale applications.</p> <pre><code># Example FastAPI deployment\nfrom fastapi import FastAPI\nimport joblib\n\napp = FastAPI()\nmodel = joblib.load(\"model.pkl\")\n\n@app.post(\"/predict\")\ndef predict(data: dict):\n    prediction = model.predict([data])\n    return {\"prediction\": prediction.tolist()}\n</code></pre>"},{"location":"courses/ai-ml/model-deployment/#pattern-2-microservices-architecture","title":"Pattern 2: Microservices Architecture","text":"<p>For complex applications with multiple models.</p> <ul> <li>Model serving containers</li> <li>API gateway</li> <li>Service discovery</li> <li>Circuit breakers</li> </ul>"},{"location":"courses/ai-ml/model-deployment/#pattern-3-serverless-deployment","title":"Pattern 3: Serverless Deployment","text":"<p>Cost-effective for sporadic usage patterns.</p> <ul> <li>AWS Lambda</li> <li>Google Cloud Functions</li> <li>Azure Functions</li> </ul>"},{"location":"courses/ai-ml/model-deployment/#scaling-considerations","title":"Scaling Considerations","text":""},{"location":"courses/ai-ml/model-deployment/#horizontal-vs-vertical-scaling","title":"Horizontal vs Vertical Scaling","text":"<ul> <li>When to scale up vs scale out</li> <li>Auto-scaling strategies</li> <li>Cost optimization</li> </ul>"},{"location":"courses/ai-ml/model-deployment/#caching-strategies","title":"Caching Strategies","text":"<ul> <li>Redis for model caching</li> <li>Feature caching patterns</li> <li>Result caching</li> </ul>"},{"location":"courses/ai-ml/model-deployment/#model-versioning-in-production","title":"Model Versioning in Production","text":"<ul> <li>Blue-green deployments</li> <li>Canary releases</li> <li>Rollback strategies</li> </ul>"},{"location":"courses/ai-ml/model-deployment/#hands-on-tutorials","title":"Hands-On Tutorials","text":"<p>Each section includes: - Step-by-step deployment guides - Docker configurations - Monitoring setup - Performance optimization tips</p> <p>These deployment patterns are battle-tested in production environments. Learn from real implementations, not just theory.</p>"},{"location":"courses/cicd/","title":"CI/CD &amp; Automation","text":"<p>Learn to build pipelines that automate testing, building, and deploying code safely.</p>"},{"location":"courses/cicd/#quick-overview","title":"Quick Overview","text":"<ul> <li>Pipeline Design: Build fast, reliable CI/CD workflows</li> <li>Testing: Comprehensive testing strategies</li> <li>Deployment: Automated, safe deployments</li> </ul> <p>Pick a topic to start learning!</p>"},{"location":"courses/cicd/deployment-automation/","title":"Deployment Automation","text":"<p>Master the art of automated deployments that are safe, fast, and reliable. Learn deployment strategies that minimize risk and maximize confidence.</p>"},{"location":"courses/cicd/deployment-automation/#deployment-fundamentals","title":"Deployment Fundamentals","text":""},{"location":"courses/cicd/deployment-automation/#what-makes-a-good-deployment","title":"What Makes a Good Deployment?","text":"<ul> <li>Zero downtime: Users never notice the deployment</li> <li>Fast rollback: Quick recovery if something goes wrong</li> <li>Automated: No manual steps that can fail</li> <li>Observable: Know what's happening during deployment</li> <li>Repeatable: Same process works across environments</li> </ul>"},{"location":"courses/cicd/deployment-automation/#common-deployment-challenges","title":"Common Deployment Challenges","text":"<ul> <li>Database migrations: Schema changes with data</li> <li>Configuration drift: Environments get out of sync</li> <li>Service dependencies: Coordinating multiple services</li> <li>Traffic management: Routing users safely</li> <li>State management: Handling stateful applications</li> </ul>"},{"location":"courses/cicd/deployment-automation/#deployment-strategies","title":"Deployment Strategies","text":""},{"location":"courses/cicd/deployment-automation/#1-recreate-deployment","title":"1. Recreate Deployment","text":"<p>How it works: Stop old version, deploy new version Pros: Simple, clean slate Cons: Downtime during deployment</p> <pre><code># Kubernetes recreate deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  strategy:\n    type: Recreate\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myapp:v2.0\n</code></pre> <p>Use when: Development environments, maintenance windows acceptable</p>"},{"location":"courses/cicd/deployment-automation/#2-rolling-update","title":"2. Rolling Update","text":"<p>How it works: Gradually replace instances one by one Pros: No downtime, automatic rollback Cons: Mixed versions during deployment</p> <pre><code># Rolling update configuration\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n      maxSurge: 1\n  replicas: 5\n</code></pre> <p>Use when: Stateless applications, backward compatibility</p>"},{"location":"courses/cicd/deployment-automation/#3-blue-green-deployment","title":"3. Blue-Green Deployment","text":"<p>How it works: Deploy to parallel environment, switch traffic Pros: Instant rollback, test production load Cons: Double resources, database complexity</p> <pre><code># Blue-green with load balancer\n# Deploy to green environment\nkubectl apply -f green-deployment.yaml\n\n# Test green environment\ncurl -H \"Host: myapp.com\" http://green-lb-ip/health\n\n# Switch traffic to green\nkubectl patch service myapp -p '{\"spec\":{\"selector\":{\"version\":\"green\"}}}'\n\n# Monitor and rollback if needed\nkubectl patch service myapp -p '{\"spec\":{\"selector\":{\"version\":\"blue\"}}}'\n</code></pre> <p>Use when: Critical applications, complex rollback scenarios</p>"},{"location":"courses/cicd/deployment-automation/#4-canary-deployment","title":"4. Canary Deployment","text":"<p>How it works: Route small percentage of traffic to new version Pros: Risk reduction, real user feedback Cons: Complex traffic management, monitoring needed</p> <pre><code># Istio canary deployment\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: myapp\nspec:\n  http:\n  - match:\n    - headers:\n        canary:\n          exact: \"true\"\n    route:\n    - destination:\n        host: myapp\n        subset: v2\n  - route:\n    - destination:\n        host: myapp\n        subset: v1\n      weight: 95\n    - destination:\n        host: myapp\n        subset: v2\n      weight: 5  # 5% of traffic to new version\n</code></pre> <p>Use when: High-risk changes, user-facing features</p>"},{"location":"courses/cicd/deployment-automation/#5-ab-testing-deployment","title":"5. A/B Testing Deployment","text":"<p>How it works: Route traffic based on user attributes Pros: Feature validation, user segmentation Cons: Complex routing logic, data analysis needed</p> <pre><code>// Feature flag based routing\nfunction getAppVersion(user) {\n  if (user.betaUser || user.id % 100 &lt; 10) {\n    return 'v2';  // 10% of users get new version\n  }\n  return 'v1';\n}\n</code></pre> <p>Use when: Feature experiments, gradual rollouts</p>"},{"location":"courses/cicd/deployment-automation/#database-deployment-strategies","title":"Database Deployment Strategies","text":""},{"location":"courses/cicd/deployment-automation/#migration-patterns","title":"Migration Patterns","text":""},{"location":"courses/cicd/deployment-automation/#1-backward-compatible-migrations","title":"1. Backward Compatible Migrations","text":"<p>Safe migrations that work with both versions:</p> <pre><code>-- Phase 1: Add new column (nullable)\nALTER TABLE users ADD COLUMN new_email VARCHAR(255);\n\n-- Phase 2: Populate new column\nUPDATE users SET new_email = email WHERE new_email IS NULL;\n\n-- Phase 3: Make column non-nullable\nALTER TABLE users ALTER COLUMN new_email SET NOT NULL;\n\n-- Phase 4: Drop old column (after deployment)\nALTER TABLE users DROP COLUMN email;\n</code></pre>"},{"location":"courses/cicd/deployment-automation/#2-expand-contract-pattern","title":"2. Expand-Contract Pattern","text":"<pre><code>-- Expand: Add new structure\nALTER TABLE users ADD COLUMN user_preferences JSONB;\n\n-- Deploy application that writes to both old and new\n-- (Application handles dual writes)\n\n-- Contract: Remove old structure\nALTER TABLE users DROP COLUMN preference_json;\n</code></pre>"},{"location":"courses/cicd/deployment-automation/#zero-downtime-database-changes","title":"Zero-Downtime Database Changes","text":"<pre><code># Database migration job\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migration\nspec:\n  template:\n    spec:\n      containers:\n      - name: migrate\n        image: migrate/migrate\n        args: [\n          \"-path\", \"/migrations\",\n          \"-database\", \"postgres://...\",\n          \"up\"\n        ]\n      restartPolicy: OnFailure\n</code></pre>"},{"location":"courses/cicd/deployment-automation/#infrastructure-as-code-deployments","title":"Infrastructure as Code Deployments","text":""},{"location":"courses/cicd/deployment-automation/#terraform-deployments","title":"Terraform Deployments","text":"<pre><code># Terraform with workspaces\nresource \"aws_instance\" \"web\" {\n  count         = terraform.workspace == \"prod\" ? 3 : 1\n  ami           = var.ami_id\n  instance_type = terraform.workspace == \"prod\" ? \"t3.medium\" : \"t3.micro\"\n\n  tags = {\n    Name        = \"web-${terraform.workspace}-${count.index}\"\n    Environment = terraform.workspace\n  }\n}\n</code></pre>"},{"location":"courses/cicd/deployment-automation/#pulumi-deployments","title":"Pulumi Deployments","text":"<pre><code>import pulumi_aws as aws\n\n# Environment-specific configuration\nconfig = pulumi.Config()\ninstance_count = config.get_int(\"instance_count\", 1)\ninstance_type = config.get(\"instance_type\", \"t3.micro\")\n\n# Create instances\nfor i in range(instance_count):\n    aws.ec2.Instance(f\"web-{i}\",\n        ami=\"ami-0c02fb55956c7d316\",\n        instance_type=instance_type,\n        tags={\"Name\": f\"web-{i}\"}\n    )\n</code></pre>"},{"location":"courses/cicd/deployment-automation/#gitops-deployments","title":"GitOps Deployments","text":""},{"location":"courses/cicd/deployment-automation/#argocd-configuration","title":"ArgoCD Configuration","text":"<pre><code># Application manifest\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: myapp\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/myorg/myapp-config\n    targetRevision: HEAD\n    path: k8s\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: default\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n    - CreateNamespace=true\n</code></pre>"},{"location":"courses/cicd/deployment-automation/#fluxcd-with-helm","title":"FluxCD with Helm","text":"<pre><code># HelmRelease for Flux\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: myapp\nspec:\n  interval: 5m\n  chart:\n    spec:\n      chart: myapp\n      version: \"&gt;=1.0.0\"\n      sourceRef:\n        kind: HelmRepository\n        name: myapp-repo\n  values:\n    image:\n      tag: \"v1.2.3\"\n    replicas: 3\n</code></pre>"},{"location":"courses/cicd/deployment-automation/#deployment-automation-tools","title":"Deployment Automation Tools","text":""},{"location":"courses/cicd/deployment-automation/#github-actions-deployment","title":"GitHub Actions Deployment","text":"<pre><code>name: Deploy to Production\non:\n  push:\n    tags: ['v*']\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment: production\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Deploy to Kubernetes\n        uses: azure/k8s-deploy@v1\n        with:\n          manifests: |\n            k8s/deployment.yaml\n            k8s/service.yaml\n          images: |\n            myapp:${{ github.ref_name }}\n</code></pre>"},{"location":"courses/cicd/deployment-automation/#jenkins-pipeline","title":"Jenkins Pipeline","text":"<pre><code>pipeline {\n    agent any\n\n    parameters {\n        choice(\n            name: 'ENVIRONMENT',\n            choices: ['dev', 'staging', 'prod'],\n            description: 'Target environment'\n        )\n        booleanParam(\n            name: 'SKIP_TESTS',\n            defaultValue: false,\n            description: 'Skip test execution'\n        )\n    }\n\n    stages {\n        stage('Test') {\n            when {\n                not { params.SKIP_TESTS }\n            }\n            steps {\n                sh 'npm test'\n            }\n        }\n\n        stage('Build') {\n            steps {\n                sh 'docker build -t myapp:${BUILD_NUMBER} .'\n            }\n        }\n\n        stage('Deploy') {\n            steps {\n                script {\n                    if (params.ENVIRONMENT == 'prod') {\n                        input message: 'Deploy to production?', ok: 'Deploy'\n                    }\n                }\n                sh \"kubectl set image deployment/myapp myapp=myapp:${BUILD_NUMBER}\"\n            }\n        }\n    }\n\n    post {\n        failure {\n            slackSend(\n                color: 'danger',\n                message: \"Deployment failed: ${env.JOB_NAME} - ${env.BUILD_NUMBER}\"\n            )\n        }\n    }\n}\n</code></pre>"},{"location":"courses/cicd/deployment-automation/#deployment-monitoring","title":"Deployment Monitoring","text":""},{"location":"courses/cicd/deployment-automation/#health-checks","title":"Health Checks","text":"<pre><code># Kubernetes health checks\nlivenessProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 3\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 5\n  timeoutSeconds: 3\n  failureThreshold: 2\n</code></pre>"},{"location":"courses/cicd/deployment-automation/#deployment-metrics","title":"Deployment Metrics","text":"<pre><code># Custom deployment metrics\nfrom prometheus_client import Counter, Histogram, Gauge\n\nDEPLOYMENTS_TOTAL = Counter('deployments_total', 'Total deployments', ['status', 'environment'])\nDEPLOYMENT_DURATION = Histogram('deployment_duration_seconds', 'Deployment duration')\nACTIVE_VERSION = Gauge('active_version', 'Current active version', ['service'])\n\n# Track deployment\nwith DEPLOYMENT_DURATION.time():\n    try:\n        deploy_application()\n        DEPLOYMENTS_TOTAL.labels(status='success', environment='prod').inc()\n    except Exception as e:\n        DEPLOYMENTS_TOTAL.labels(status='failure', environment='prod').inc()\n        raise\n</code></pre>"},{"location":"courses/cicd/deployment-automation/#automated-rollback","title":"Automated Rollback","text":"<pre><code># Automated rollback based on metrics\n- name: Check deployment health\n  run: |\n    # Wait for deployment to stabilize\n    sleep 60\n\n    # Check error rate\n    ERROR_RATE=$(curl -s \"http://prometheus:9090/api/v1/query?query=rate(http_requests_total{status=~\\\"5..\\\"}[5m])\" | jq '.data.result[0].value[1] | tonumber')\n\n    if (( $(echo \"$ERROR_RATE &gt; 0.05\" | bc -l) )); then\n      echo \"High error rate detected: $ERROR_RATE\"\n      kubectl rollout undo deployment/myapp\n      exit 1\n    fi\n</code></pre>"},{"location":"courses/cicd/deployment-automation/#security-in-deployments","title":"Security in Deployments","text":""},{"location":"courses/cicd/deployment-automation/#secure-secret-management","title":"Secure Secret Management","text":"<pre><code># External Secrets Operator\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: vault-backend\nspec:\n  provider:\n    vault:\n      server: \"https://vault.example.com\"\n      path: \"secret\"\n      auth:\n        kubernetes:\n          mountPath: \"kubernetes\"\n          role: \"myapp\"\n\n---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: myapp-secrets\nspec:\n  secretStoreRef:\n    name: vault-backend\n    kind: SecretStore\n  target:\n    name: myapp-secrets\n  data:\n  - secretKey: database-password\n    remoteRef:\n      key: myapp\n      property: db_password\n</code></pre>"},{"location":"courses/cicd/deployment-automation/#container-scanning","title":"Container Scanning","text":"<pre><code># Trivy container scanning\n- name: Run Trivy vulnerability scanner\n  uses: aquasecurity/trivy-action@master\n  with:\n    image-ref: 'myapp:${{ github.sha }}'\n    format: 'sarif'\n    output: 'trivy-results.sarif'\n    exit-code: '1'\n    severity: 'CRITICAL,HIGH'\n</code></pre> <p>Deployment automation is about building confidence through consistency. The goal is to make deployments so routine and reliable that they become invisible.</p>"},{"location":"courses/cicd/pipeline-design/","title":"Pipeline Design","text":"<p>Learn to design CI/CD pipelines that are fast, reliable, and maintainable. Good pipeline design makes the difference between smooth deployments and 3 AM emergency fixes.</p>"},{"location":"courses/cicd/pipeline-design/#pipeline-design-principles","title":"Pipeline Design Principles","text":""},{"location":"courses/cicd/pipeline-design/#1-fail-fast","title":"1. Fail Fast","text":"<p>Catch problems as early as possible in the pipeline: - Static analysis first: Linting, security scans - Unit tests early: Quick feedback on code quality - Expensive tests last: Integration and E2E tests</p>"},{"location":"courses/cicd/pipeline-design/#2-parallel-execution","title":"2. Parallel Execution","text":"<p>Run independent jobs concurrently: - Test parallelization: Split test suites across workers - Build parallelization: Build different components simultaneously - Environment parallelization: Deploy to multiple environments</p>"},{"location":"courses/cicd/pipeline-design/#3-pipeline-as-code","title":"3. Pipeline as Code","text":"<p>Version control your pipeline definitions: - Declarative syntax: YAML/JSON configuration files - Code review process: Review pipeline changes like code - Branching strategies: Different pipelines for different branches</p>"},{"location":"courses/cicd/pipeline-design/#4-idempotency","title":"4. Idempotency","text":"<p>Pipelines should produce the same result when run multiple times: - Deterministic builds: Same inputs = same outputs - Environment consistency: Identical environments every time - Database migrations: Safe to run multiple times</p>"},{"location":"courses/cicd/pipeline-design/#pipeline-patterns","title":"Pipeline Patterns","text":""},{"location":"courses/cicd/pipeline-design/#basic-linear-pipeline","title":"Basic Linear Pipeline","text":"<pre><code>graph LR\n    A[Checkout] --&gt; B[Lint]\n    B --&gt; C[Test]\n    C --&gt; D[Build]\n    D --&gt; E[Deploy]\n</code></pre> <p>Simple and predictable, but slow for large projects.</p>"},{"location":"courses/cicd/pipeline-design/#fan-out-pattern","title":"Fan-Out Pattern","text":"<pre><code>graph LR\n    A[Checkout] --&gt; B[Lint]\n    A --&gt; C[Unit Tests]\n    A --&gt; D[Integration Tests]\n    B --&gt; E[Build]\n    C --&gt; E\n    D --&gt; E\n    E --&gt; F[Deploy]\n</code></pre> <p>Parallel execution for faster feedback.</p>"},{"location":"courses/cicd/pipeline-design/#multi-stage-pipeline","title":"Multi-Stage Pipeline","text":"<pre><code>graph LR\n    A[Checkout] --&gt; B[Test]\n    B --&gt; C[Build]\n    C --&gt; D[Deploy Dev]\n    D --&gt; E[E2E Tests]\n    E --&gt; F[Deploy Staging]\n    F --&gt; G[Performance Tests]\n    G --&gt; H[Deploy Prod]\n</code></pre> <p>Progressive validation through environments.</p>"},{"location":"courses/cicd/pipeline-design/#matrix-builds","title":"Matrix Builds","text":"<p>Test across multiple configurations: <pre><code>strategy:\n  matrix:\n    os: [ubuntu-latest, windows-latest, macos-latest]\n    node-version: [16, 18, 20]\n    include:\n      - os: ubuntu-latest\n        node-version: 16\n        experimental: true\n</code></pre></p>"},{"location":"courses/cicd/pipeline-design/#pipeline-optimization","title":"Pipeline Optimization","text":""},{"location":"courses/cicd/pipeline-design/#caching-strategies","title":"Caching Strategies","text":"<p>Speed up builds by caching dependencies:</p> <pre><code># GitHub Actions caching example\n- name: Cache dependencies\n  uses: actions/cache@v3\n  with:\n    path: ~/.npm\n    key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n    restore-keys: |\n      ${{ runner.os }}-node-\n</code></pre>"},{"location":"courses/cicd/pipeline-design/#docker-layer-caching","title":"Docker Layer Caching","text":"<p>Optimize Docker builds: <pre><code># Copy package files first (changes less frequently)\nCOPY package*.json ./\nRUN npm ci --only=production\n\n# Copy source code last (changes more frequently)\nCOPY . .\nRUN npm run build\n</code></pre></p>"},{"location":"courses/cicd/pipeline-design/#incremental-testing","title":"Incremental Testing","text":"<p>Only test what changed: <pre><code># Run tests for changed files only\nnpm test -- --findRelatedTests $(git diff --name-only HEAD~1)\n\n# Nx workspace with affected projects\nnx affected:test --base=HEAD~1\n</code></pre></p>"},{"location":"courses/cicd/pipeline-design/#environment-management","title":"Environment Management","text":""},{"location":"courses/cicd/pipeline-design/#environment-promotion","title":"Environment Promotion","text":"<pre><code>environments:\n  development:\n    auto_deploy: true\n    protection_rules: []\n\n  staging:\n    auto_deploy: false\n    protection_rules:\n      - required_reviewers: 1\n\n  production:\n    auto_deploy: false\n    protection_rules:\n      - required_reviewers: 2\n      - restrict_pushes: true\n</code></pre>"},{"location":"courses/cicd/pipeline-design/#feature-flags","title":"Feature Flags","text":"<p>Control feature rollouts independently of deployments: <pre><code># Feature flag example\nfrom feature_flags import is_enabled\n\ndef new_checkout_flow():\n    if is_enabled(\"new_checkout\", user_id=current_user.id):\n        return render_new_checkout()\n    else:\n        return render_old_checkout()\n</code></pre></p>"},{"location":"courses/cicd/pipeline-design/#environment-variables","title":"Environment Variables","text":"<p>Manage configuration across environments: <pre><code># Pipeline environment variables\nenv:\n  NODE_ENV: production\n  DATABASE_URL: ${{ secrets.DATABASE_URL }}\n  API_KEY: ${{ secrets.API_KEY }}\n</code></pre></p>"},{"location":"courses/cicd/pipeline-design/#error-handling-and-recovery","title":"Error Handling and Recovery","text":""},{"location":"courses/cicd/pipeline-design/#retry-strategies","title":"Retry Strategies","text":"<p>Handle transient failures: <pre><code># Retry flaky tests\n- name: Run tests\n  run: npm test\n  retry:\n    attempts: 3\n    delay: 5s\n</code></pre></p>"},{"location":"courses/cicd/pipeline-design/#rollback-mechanisms","title":"Rollback Mechanisms","text":"<p>Quick recovery from failed deployments: <pre><code># Blue-green deployment with quick rollback\nkubectl patch service myapp -p '{\"spec\":{\"selector\":{\"version\":\"blue\"}}}'\n\n# Helm rollback\nhelm rollback myapp 1\n</code></pre></p>"},{"location":"courses/cicd/pipeline-design/#notifications","title":"Notifications","text":"<p>Alert on pipeline failures: <pre><code># Slack notification on failure\n- name: Notify on failure\n  if: failure()\n  uses: 8398a7/action-slack@v3\n  with:\n    status: failure\n    text: \"Pipeline failed on ${{ github.ref }}\"\n</code></pre></p>"},{"location":"courses/cicd/pipeline-design/#security-in-pipelines","title":"Security in Pipelines","text":""},{"location":"courses/cicd/pipeline-design/#secret-management","title":"Secret Management","text":"<p>Never hardcode secrets: <pre><code># Use encrypted secrets\n- name: Deploy\n  env:\n    AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n  run: aws deploy create-deployment\n</code></pre></p>"},{"location":"courses/cicd/pipeline-design/#dependency-scanning","title":"Dependency Scanning","text":"<p>Check for vulnerabilities: <pre><code>- name: Security audit\n  run: npm audit --audit-level high\n\n- name: Dependency check\n  uses: dependency-check/Dependency-Check_Action@main\n</code></pre></p>"},{"location":"courses/cicd/pipeline-design/#container-scanning","title":"Container Scanning","text":"<p>Scan Docker images: <pre><code>- name: Scan image\n  uses: aquasecurity/trivy-action@master\n  with:\n    image-ref: 'myapp:${{ github.sha }}'\n    format: 'sarif'\n    output: 'trivy-results.sarif'\n</code></pre></p>"},{"location":"courses/cicd/pipeline-design/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"courses/cicd/pipeline-design/#pipeline-metrics","title":"Pipeline Metrics","text":"<p>Track pipeline performance: - Build time: How long builds take - Success rate: Percentage of successful builds - Mean time to recovery: How quickly issues are fixed - Deployment frequency: How often you deploy</p>"},{"location":"courses/cicd/pipeline-design/#pipeline-logs","title":"Pipeline Logs","text":"<p>Structured logging for debugging: <pre><code>- name: Debug info\n  run: |\n    echo \"::group::Environment Info\"\n    node --version\n    npm --version\n    echo \"::endgroup::\"\n</code></pre></p>"},{"location":"courses/cicd/pipeline-design/#build-artifacts","title":"Build Artifacts","text":"<p>Save important build outputs: <pre><code>- name: Upload coverage reports\n  uses: actions/upload-artifact@v3\n  with:\n    name: coverage-report\n    path: coverage/\n    retention-days: 30\n</code></pre></p>"},{"location":"courses/cicd/pipeline-design/#real-world-pipeline-examples","title":"Real-World Pipeline Examples","text":""},{"location":"courses/cicd/pipeline-design/#microservices-monorepo","title":"Microservices Monorepo","text":"<pre><code>name: Microservices CI/CD\non:\n  push:\n    branches: [main]\n\njobs:\n  detect-changes:\n    runs-on: ubuntu-latest\n    outputs:\n      services: ${{ steps.changes.outputs.services }}\n    steps:\n      - uses: dorny/paths-filter@v2\n        id: changes\n        with:\n          filters: |\n            user-service:\n              - 'services/user-service/**'\n            order-service:\n              - 'services/order-service/**'\n\n  build-and-test:\n    needs: detect-changes\n    if: ${{ needs.detect-changes.outputs.services != '[]' }}\n    strategy:\n      matrix:\n        service: ${{ fromJSON(needs.detect-changes.outputs.services) }}\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Test ${{ matrix.service }}\n        run: |\n          cd services/${{ matrix.service }}\n          npm test\n      - name: Build ${{ matrix.service }}\n        run: |\n          cd services/${{ matrix.service }}\n          docker build -t ${{ matrix.service }}:${{ github.sha }} .\n</code></pre>"},{"location":"courses/cicd/pipeline-design/#infrastructure-as-code-pipeline","title":"Infrastructure as Code Pipeline","text":"<pre><code>name: Terraform\non:\n  pull_request:\n    paths: ['terraform/**']\n  push:\n    branches: [main]\n    paths: ['terraform/**']\n\njobs:\n  plan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: hashicorp/setup-terraform@v2\n\n      - name: Terraform Format\n        run: terraform fmt -check\n\n      - name: Terraform Plan\n        run: terraform plan -no-color\n\n      - name: Comment PR\n        if: github.event_name == 'pull_request'\n        uses: actions/github-script@v6\n        with:\n          script: |\n            const output = `#### Terraform Plan \ud83d\udcd6\n            \\`\\`\\`\n            ${process.env.PLAN}\n            \\`\\`\\`\n            `;\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: output\n            })\n</code></pre> <p>Good pipeline design is an investment in your team's productivity and sanity. Spend time getting it right, and you'll deploy with confidence every time.</p>"},{"location":"courses/cicd/testing-strategies/","title":"Testing Strategies","text":"<p>Learn to build comprehensive testing strategies that catch bugs early, provide confidence in deployments, and don't slow down development.</p>"},{"location":"courses/cicd/testing-strategies/#the-testing-pyramid","title":"The Testing Pyramid","text":"<pre><code>       \ud83d\udd3a E2E Tests\n      \ud83d\udd38\ud83d\udd38 Integration Tests  \n   \ud83d\udd39\ud83d\udd39\ud83d\udd39\ud83d\udd39 Unit Tests\n</code></pre>"},{"location":"courses/cicd/testing-strategies/#unit-tests-base-of-pyramid","title":"Unit Tests (Base of Pyramid)","text":"<p>What: Test individual functions/components in isolation Why: Fast feedback, easy to debug, high coverage When: Every function, every edge case</p> <pre><code>// Example: Testing a utility function\ndescribe('calculateTax', () =&gt; {\n  test('calculates tax for standard rate', () =&gt; {\n    expect(calculateTax(100, 0.08)).toBe(8);\n  });\n\n  test('handles zero amount', () =&gt; {\n    expect(calculateTax(0, 0.08)).toBe(0);\n  });\n\n  test('throws error for negative amount', () =&gt; {\n    expect(() =&gt; calculateTax(-10, 0.08)).toThrow();\n  });\n});\n</code></pre>"},{"location":"courses/cicd/testing-strategies/#integration-tests-middle","title":"Integration Tests (Middle)","text":"<p>What: Test how components work together Why: Catch interface issues, validate workflows When: Critical user journeys, API endpoints</p> <pre><code># Example: Testing API endpoint\ndef test_create_user_endpoint():\n    response = client.post('/users', json={\n        'name': 'John Doe',\n        'email': 'john@example.com'\n    })\n\n    assert response.status_code == 201\n    assert response.json()['name'] == 'John Doe'\n\n    # Verify user was created in database\n    user = db.query(User).filter(User.email == 'john@example.com').first()\n    assert user is not None\n</code></pre>"},{"location":"courses/cicd/testing-strategies/#end-to-end-tests-top","title":"End-to-End Tests (Top)","text":"<p>What: Test complete user workflows through the UI Why: Validate real user scenarios When: Critical business flows, major features</p> <pre><code>// Example: Cypress E2E test\ndescribe('User Registration Flow', () =&gt; {\n  it('allows new user to register and login', () =&gt; {\n    cy.visit('/register');\n    cy.get('[data-testid=name-input]').type('John Doe');\n    cy.get('[data-testid=email-input]').type('john@example.com');\n    cy.get('[data-testid=password-input]').type('password123');\n    cy.get('[data-testid=submit-button]').click();\n\n    cy.url().should('include', '/dashboard');\n    cy.contains('Welcome, John Doe');\n  });\n});\n</code></pre>"},{"location":"courses/cicd/testing-strategies/#testing-strategy-by-application-type","title":"Testing Strategy by Application Type","text":""},{"location":"courses/cicd/testing-strategies/#web-applications","title":"Web Applications","text":"<pre><code>Testing Stack:\n  Unit: Jest, Vitest, pytest\n  Integration: Supertest, TestContainers\n  E2E: Cypress, Playwright\n  Visual: Percy, Chromatic\n  Performance: Lighthouse CI\n</code></pre>"},{"location":"courses/cicd/testing-strategies/#apismicroservices","title":"APIs/Microservices","text":"<pre><code>Testing Stack:\n  Unit: Jest, pytest, JUnit\n  Integration: Postman, REST Assured\n  Contract: Pact, Spring Cloud Contract\n  Load: k6, JMeter\n  Security: OWASP ZAP\n</code></pre>"},{"location":"courses/cicd/testing-strategies/#mobile-applications","title":"Mobile Applications","text":"<pre><code>Testing Stack:\n  Unit: XCTest, Espresso\n  Integration: Detox, Appium\n  Device: Firebase Test Lab, BrowserStack\n  Performance: Instruments, Android Profiler\n</code></pre>"},{"location":"courses/cicd/testing-strategies/#test-data-management","title":"Test Data Management","text":""},{"location":"courses/cicd/testing-strategies/#test-data-strategies","title":"Test Data Strategies","text":""},{"location":"courses/cicd/testing-strategies/#1-static-test-data","title":"1. Static Test Data","text":"<p>Predefined datasets for consistent testing: <pre><code>// fixtures/users.json\n{\n  \"validUser\": {\n    \"name\": \"John Doe\",\n    \"email\": \"john@example.com\",\n    \"age\": 30\n  },\n  \"invalidUser\": {\n    \"name\": \"\",\n    \"email\": \"invalid-email\",\n    \"age\": -5\n  }\n}\n</code></pre></p>"},{"location":"courses/cicd/testing-strategies/#2-generated-test-data","title":"2. Generated Test Data","text":"<p>Dynamic data generation for broader coverage: <pre><code># Using faker library\nfrom faker import Faker\nfake = Faker()\n\ndef generate_user():\n    return {\n        'name': fake.name(),\n        'email': fake.email(),\n        'address': fake.address()\n    }\n</code></pre></p>"},{"location":"courses/cicd/testing-strategies/#3-database-seeding","title":"3. Database Seeding","text":"<p>Consistent database state for tests: <pre><code>-- seeds/test_data.sql\nINSERT INTO users (id, name, email, created_at) VALUES\n(1, 'Test User 1', 'test1@example.com', '2023-01-01'),\n(2, 'Test User 2', 'test2@example.com', '2023-01-02');\n\nINSERT INTO orders (id, user_id, total, status) VALUES\n(1, 1, 99.99, 'completed'),\n(2, 1, 149.99, 'pending');\n</code></pre></p>"},{"location":"courses/cicd/testing-strategies/#test-environment-management","title":"Test Environment Management","text":"<pre><code># docker-compose.test.yml\nversion: '3.8'\nservices:\n  app:\n    build: .\n    environment:\n      NODE_ENV: test\n      DATABASE_URL: postgresql://test:test@db:5432/test_db\n    depends_on:\n      - db\n\n  db:\n    image: postgres:13\n    environment:\n      POSTGRES_DB: test_db\n      POSTGRES_USER: test\n      POSTGRES_PASSWORD: test\n    tmpfs:\n      - /var/lib/postgresql/data  # In-memory for speed\n</code></pre>"},{"location":"courses/cicd/testing-strategies/#testing-in-cicd-pipelines","title":"Testing in CI/CD Pipelines","text":""},{"location":"courses/cicd/testing-strategies/#parallel-test-execution","title":"Parallel Test Execution","text":"<pre><code># GitHub Actions parallel testing\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        test-group: [1, 2, 3, 4]\n    steps:\n      - uses: actions/checkout@v3\n      - name: Run tests\n        run: |\n          npm test -- --shard=${{ matrix.test-group }}/4\n</code></pre>"},{"location":"courses/cicd/testing-strategies/#test-result-reporting","title":"Test Result Reporting","text":"<pre><code># Test reporting with artifacts\n- name: Run tests\n  run: npm test -- --coverage --reporter=junit\n\n- name: Publish test results\n  uses: dorny/test-reporter@v1\n  if: always()\n  with:\n    name: 'Test Results'\n    path: 'test-results.xml'\n    reporter: 'jest-junit'\n\n- name: Upload coverage\n  uses: codecov/codecov-action@v3\n  with:\n    files: ./coverage/lcov.info\n</code></pre>"},{"location":"courses/cicd/testing-strategies/#flaky-test-management","title":"Flaky Test Management","text":"<pre><code># Retry flaky tests\n- name: Run tests with retry\n  uses: nick-invision/retry@v2\n  with:\n    timeout_minutes: 10\n    max_attempts: 3\n    command: npm test\n</code></pre>"},{"location":"courses/cicd/testing-strategies/#advanced-testing-patterns","title":"Advanced Testing Patterns","text":""},{"location":"courses/cicd/testing-strategies/#test-doubles","title":"Test Doubles","text":""},{"location":"courses/cicd/testing-strategies/#mocks","title":"Mocks","text":"<p>Replace dependencies with controlled implementations: <pre><code>// Mock external API\njest.mock('axios');\nconst mockedAxios = axios as jest.Mocked&lt;typeof axios&gt;;\n\ntest('fetches user data', async () =&gt; {\n  mockedAxios.get.mockResolvedValue({\n    data: { id: 1, name: 'John' }\n  });\n\n  const user = await fetchUser(1);\n  expect(user.name).toBe('John');\n});\n</code></pre></p>"},{"location":"courses/cicd/testing-strategies/#stubs","title":"Stubs","text":"<p>Provide fixed responses: <pre><code># Stub payment service\nclass PaymentServiceStub:\n    def charge(self, amount):\n        if amount &gt; 1000:\n            return {'status': 'failed', 'error': 'Amount too high'}\n        return {'status': 'success', 'transaction_id': '12345'}\n</code></pre></p>"},{"location":"courses/cicd/testing-strategies/#fakes","title":"Fakes","text":"<p>Working implementations with shortcuts: <pre><code>// In-memory database fake\nclass FakeUserRepository {\n  constructor() {\n    this.users = new Map();\n  }\n\n  async save(user) {\n    this.users.set(user.id, user);\n    return user;\n  }\n\n  async findById(id) {\n    return this.users.get(id);\n  }\n}\n</code></pre></p>"},{"location":"courses/cicd/testing-strategies/#contract-testing","title":"Contract Testing","text":"<p>Ensure service compatibility: <pre><code>// Provider contract test\nconst { Verifier } = require('@pact-foundation/pact');\n\ndescribe('User Service Provider', () =&gt; {\n  it('validates the expectations of UserConsumer', () =&gt; {\n    return new Verifier({\n      provider: 'UserService',\n      providerBaseUrl: 'http://localhost:3000',\n      pactUrls: ['./pacts/userconsumer-userservice.json']\n    }).verifyProvider();\n  });\n});\n</code></pre></p>"},{"location":"courses/cicd/testing-strategies/#mutation-testing","title":"Mutation Testing","text":"<p>Test the quality of your tests: <pre><code># Install and run mutation testing\nnpm install --save-dev stryker-cli\nnpx stryker run\n</code></pre></p>"},{"location":"courses/cicd/testing-strategies/#performance-testing","title":"Performance Testing","text":""},{"location":"courses/cicd/testing-strategies/#load-testing-with-k6","title":"Load Testing with k6","text":"<pre><code>import http from 'k6/http';\nimport { check, sleep } from 'k6';\n\nexport let options = {\n  stages: [\n    { duration: '2m', target: 100 },  // Ramp up\n    { duration: '5m', target: 100 },  // Stay at 100 users\n    { duration: '2m', target: 200 },  // Ramp up\n    { duration: '5m', target: 200 },  // Stay at 200 users\n    { duration: '2m', target: 0 },    // Ramp down\n  ],\n};\n\nexport default function () {\n  let response = http.get('https://api.example.com/users');\n  check(response, {\n    'status is 200': (r) =&gt; r.status === 200,\n    'response time &lt; 500ms': (r) =&gt; r.timings.duration &lt; 500,\n  });\n  sleep(1);\n}\n</code></pre>"},{"location":"courses/cicd/testing-strategies/#database-performance-testing","title":"Database Performance Testing","text":"<pre><code>-- Test query performance\nEXPLAIN ANALYZE \nSELECT u.name, COUNT(o.id) as order_count\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2023-01-01'\nGROUP BY u.id, u.name\nORDER BY order_count DESC\nLIMIT 100;\n</code></pre>"},{"location":"courses/cicd/testing-strategies/#test-metrics-and-quality-gates","title":"Test Metrics and Quality Gates","text":""},{"location":"courses/cicd/testing-strategies/#coverage-metrics","title":"Coverage Metrics","text":"<pre><code># Coverage thresholds\ncoverage:\n  threshold:\n    global:\n      branches: 80\n      functions: 80\n      lines: 80\n      statements: 80\n</code></pre>"},{"location":"courses/cicd/testing-strategies/#quality-gates","title":"Quality Gates","text":"<pre><code># SonarQube quality gate\nsonar.qualitygate.wait=true\nsonar.coverage.exclusions=**/*test*/**,**/migrations/**\nsonar.test.inclusions=**/*test*/**\n</code></pre>"},{"location":"courses/cicd/testing-strategies/#test-reporting-dashboard","title":"Test Reporting Dashboard","text":"<pre><code># Allure test reporting\n- name: Generate Allure Report\n  uses: simple-elf/allure-report-action@master\n  if: always()\n  with:\n    allure_results: allure-results\n    allure_history: allure-history\n</code></pre> <p>Good testing strategy is about finding the right balance: enough tests to catch bugs and give confidence, but not so many that they slow down development.</p>"},{"location":"courses/data-engineering/","title":"Data Engineering","text":"<p>Build scalable data systems and pipelines that power analytics and ML.</p>"},{"location":"courses/data-engineering/#quick-overview","title":"Quick Overview","text":"<ul> <li>Pipelines: Design reliable data workflows</li> <li>Databases: Master different database technologies  </li> <li>Data Quality: Ensure trustworthy data</li> </ul> <p>Pick a topic to start learning!</p>"},{"location":"courses/data-engineering/data-quality/","title":"Data Quality","text":"<p>Learn to build data systems that deliver trustworthy, accurate data. Because garbage in, garbage out is still true.</p>"},{"location":"courses/data-engineering/data-quality/#why-data-quality-matters","title":"Why Data Quality Matters","text":"<p>Bad data leads to: - Wrong business decisions - Failed ML models - Angry stakeholders - Wasted engineering time - Lost revenue</p> <p>Good data quality practices save time, money, and sanity.</p>"},{"location":"courses/data-engineering/data-quality/#the-data-quality-framework","title":"The Data Quality Framework","text":""},{"location":"courses/data-engineering/data-quality/#1-accuracy","title":"1. Accuracy","text":"<p>Is the data correct and error-free?</p> <p>Common Issues: - Typos and data entry errors - Outdated information - Wrong data types - Invalid values</p> <p>Solutions: - Input validation - Regular data audits - Source system improvements - Business rule validation</p>"},{"location":"courses/data-engineering/data-quality/#2-completeness","title":"2. Completeness","text":"<p>Is all expected data present?</p> <p>Common Issues: - Missing records - NULL values where they shouldn't be - Incomplete data loads - Partial system failures</p> <p>Solutions: - Completeness checks - Required field validation - Data load monitoring - Source system SLAs</p>"},{"location":"courses/data-engineering/data-quality/#3-consistency","title":"3. Consistency","text":"<p>Is data uniform across systems and time?</p> <p>Common Issues: - Different formats across systems - Conflicting business rules - Time zone inconsistencies - Naming convention variations</p> <p>Solutions: - Data standardization - Master data management - Cross-system validation - Canonical data models</p>"},{"location":"courses/data-engineering/data-quality/#4-timeliness","title":"4. Timeliness","text":"<p>Is data available when needed?</p> <p>Common Issues: - Delayed data loads - Stale cache data - Batch processing delays - Network issues</p> <p>Solutions: - Real-time monitoring - SLA definitions - Alerting systems - Data freshness checks</p>"},{"location":"courses/data-engineering/data-quality/#5-validity","title":"5. Validity","text":"<p>Does data conform to defined rules?</p> <p>Common Issues: - Invalid email formats - Out-of-range values - Foreign key violations - Schema mismatches</p> <p>Solutions: - Schema validation - Business rule engines - Data type enforcement - Constraint checking</p>"},{"location":"courses/data-engineering/data-quality/#implementing-data-quality","title":"Implementing Data Quality","text":""},{"location":"courses/data-engineering/data-quality/#data-quality-checks","title":"Data Quality Checks","text":"<pre><code># Example: Email validation\ndef validate_email(email):\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return re.match(pattern, email) is not None\n\n# Example: Completeness check\ndef check_completeness(df, required_columns):\n    missing_data = {}\n    for col in required_columns:\n        missing_count = df[col].isnull().sum()\n        missing_data[col] = {\n            'missing_count': missing_count,\n            'missing_percentage': (missing_count / len(df)) * 100\n        }\n    return missing_data\n</code></pre>"},{"location":"courses/data-engineering/data-quality/#data-profiling","title":"Data Profiling","text":"<p>Understand your data before you work with it:</p> <ul> <li>Statistical analysis: Min, max, mean, median, standard deviation</li> <li>Data distribution: Histograms, frequency counts</li> <li>Pattern analysis: Common formats, outliers</li> <li>Relationship analysis: Correlations, dependencies</li> </ul>"},{"location":"courses/data-engineering/data-quality/#data-quality-metrics","title":"Data Quality Metrics","text":"<p>Track these KPIs: - Error rate: Percentage of records with errors - Completeness score: Percentage of complete records - Freshness: Time since last update - Consistency score: Cross-system data agreement - Business rule compliance: Percentage passing validation</p>"},{"location":"courses/data-engineering/data-quality/#tools-and-frameworks","title":"Tools and Frameworks","text":""},{"location":"courses/data-engineering/data-quality/#open-source","title":"Open Source","text":"<ul> <li>Great Expectations: Data validation framework</li> <li>Apache Griffin: Data quality service</li> <li>Deequ: Data quality library for Apache Spark</li> <li>Pandas Profiling: Automated data profiling</li> </ul>"},{"location":"courses/data-engineering/data-quality/#commercial","title":"Commercial","text":"<ul> <li>Talend Data Quality: Enterprise data quality platform</li> <li>Informatica Data Quality: Comprehensive DQ suite</li> <li>AWS Glue DataBrew: Visual data preparation with quality checks</li> </ul>"},{"location":"courses/data-engineering/data-quality/#practical-implementations","title":"Practical Implementations","text":""},{"location":"courses/data-engineering/data-quality/#project-1-building-a-data-quality-dashboard","title":"Project 1: Building a Data Quality Dashboard","text":"<p>Create a real-time dashboard showing: - Data quality scores by dataset - Trend analysis over time - Alert notifications - Root cause analysis</p>"},{"location":"courses/data-engineering/data-quality/#project-2-automated-data-quality-pipeline","title":"Project 2: Automated Data Quality Pipeline","text":"<p>Build a pipeline that: - Validates incoming data - Quarantines bad records - Alerts on quality issues - Reports quality metrics</p>"},{"location":"courses/data-engineering/data-quality/#project-3-cross-system-data-validation","title":"Project 3: Cross-System Data Validation","text":"<p>Implement checks that: - Compare data across systems - Detect data drift - Validate business rules - Monitor data lineage</p>"},{"location":"courses/data-engineering/data-quality/#best-practices","title":"Best Practices","text":""},{"location":"courses/data-engineering/data-quality/#1-fail-fast","title":"1. Fail Fast","text":"<p>Catch data quality issues as early as possible in the pipeline.</p>"},{"location":"courses/data-engineering/data-quality/#2-document-everything","title":"2. Document Everything","text":"<p>Keep clear documentation of: - Data quality rules - Expected data formats - Business logic - Known issues and workarounds</p>"},{"location":"courses/data-engineering/data-quality/#3-automate-quality-checks","title":"3. Automate Quality Checks","text":"<p>Manual checks don't scale. Automate everything you can.</p>"},{"location":"courses/data-engineering/data-quality/#4-make-quality-visible","title":"4. Make Quality Visible","text":"<p>Data quality should be visible to everyone who uses the data.</p>"},{"location":"courses/data-engineering/data-quality/#5-continuous-improvement","title":"5. Continuous Improvement","text":"<p>Data quality is not a one-time effort - it's an ongoing process.</p> <p>Data quality isn't glamorous, but it's the foundation of everything else you'll build. Get this right and everything else becomes easier.</p>"},{"location":"courses/data-engineering/database-fundamentals/","title":"Database Fundamentals","text":"<p>Master the databases that power modern data systems - from traditional SQL to cutting-edge time-series databases.</p>"},{"location":"courses/data-engineering/database-fundamentals/#the-database-landscape","title":"The Database Landscape","text":"<p>Choosing the right database is crucial. Here's a practical guide to when to use what:</p>"},{"location":"courses/data-engineering/database-fundamentals/#relational-databases-sql","title":"\ud83d\uddc3\ufe0f Relational Databases (SQL)","text":"<p>Best for: Transactions, complex queries, data integrity - PostgreSQL: The Swiss Army knife of databases - MySQL: Fast and reliable for web applications - SQLite: Perfect for development and small applications</p>"},{"location":"courses/data-engineering/database-fundamentals/#analytical-databases","title":"\ud83d\udcca Analytical Databases","text":"<p>Best for: Analytics, reporting, data warehousing - ClickHouse: Blazing fast for time-series and analytics - BigQuery: Serverless analytics at scale - Snowflake: Cloud data warehouse with separation of compute/storage</p>"},{"location":"courses/data-engineering/database-fundamentals/#nosql-databases","title":"\ud83d\udd04 NoSQL Databases","text":"<p>Best for: Flexible schemas, horizontal scaling - MongoDB: Document database for JSON-like data - Cassandra: Distributed database for high availability - DynamoDB: Serverless NoSQL on AWS</p>"},{"location":"courses/data-engineering/database-fundamentals/#in-memory-databases","title":"\u26a1 In-Memory Databases","text":"<p>Best for: Caching, real-time applications - Redis: The go-to cache and pub/sub system - Memcached: Simple distributed caching</p>"},{"location":"courses/data-engineering/database-fundamentals/#time-series-databases","title":"\ud83d\udcc8 Time-Series Databases","text":"<p>Best for: IoT data, monitoring, financial data - InfluxDB: Purpose-built for time-series data - TimescaleDB: PostgreSQL extension for time-series</p>"},{"location":"courses/data-engineering/database-fundamentals/#database-design-principles","title":"Database Design Principles","text":""},{"location":"courses/data-engineering/database-fundamentals/#normalization-vs-denormalization","title":"Normalization vs Denormalization","text":"<p>Understanding when to normalize and when to denormalize:</p> <p>Normalize when: - Data integrity is critical - Storage cost is high - Write-heavy workloads</p> <p>Denormalize when: - Query performance is critical - Read-heavy workloads - Analytics use cases</p>"},{"location":"courses/data-engineering/database-fundamentals/#indexing-strategies","title":"Indexing Strategies","text":"<p>Indexes are your friend (but choose wisely):</p> <pre><code>-- Primary indexes for unique identification\nCREATE UNIQUE INDEX idx_user_email ON users(email);\n\n-- Composite indexes for multi-column queries\nCREATE INDEX idx_order_date_status ON orders(order_date, status);\n\n-- Partial indexes for filtered queries\nCREATE INDEX idx_active_users ON users(created_at) \nWHERE status = 'active';\n</code></pre>"},{"location":"courses/data-engineering/database-fundamentals/#partitioning","title":"Partitioning","text":"<p>Split large tables for better performance:</p> <ul> <li>Horizontal partitioning: Split by rows (e.g., by date)</li> <li>Vertical partitioning: Split by columns</li> <li>Functional partitioning: Split by feature/service</li> </ul>"},{"location":"courses/data-engineering/database-fundamentals/#hands-on-tutorials","title":"Hands-On Tutorials","text":""},{"location":"courses/data-engineering/database-fundamentals/#tutorial-1-postgresql-deep-dive","title":"Tutorial 1: PostgreSQL Deep Dive","text":"<ul> <li>Advanced SQL techniques</li> <li>Query optimization</li> <li>Index strategies</li> <li>JSON support in PostgreSQL</li> </ul>"},{"location":"courses/data-engineering/database-fundamentals/#tutorial-2-building-a-data-warehouse","title":"Tutorial 2: Building a Data Warehouse","text":"<ul> <li>Star vs snowflake schemas</li> <li>Slowly changing dimensions</li> <li>ETL vs ELT patterns</li> <li>Performance optimization</li> </ul>"},{"location":"courses/data-engineering/database-fundamentals/#tutorial-3-nosql-with-mongodb","title":"Tutorial 3: NoSQL with MongoDB","text":"<ul> <li>Document modeling</li> <li>Aggregation pipelines</li> <li>Sharding strategies</li> <li>Replica sets</li> </ul>"},{"location":"courses/data-engineering/database-fundamentals/#tutorial-4-time-series-with-influxdb","title":"Tutorial 4: Time-Series with InfluxDB","text":"<ul> <li>Schema design for time-series</li> <li>Retention policies</li> <li>Continuous queries</li> <li>Grafana integration</li> </ul>"},{"location":"courses/data-engineering/database-fundamentals/#performance-optimization","title":"Performance Optimization","text":""},{"location":"courses/data-engineering/database-fundamentals/#query-optimization","title":"Query Optimization","text":"<ul> <li>Explain plans and execution strategies</li> <li>Common anti-patterns to avoid</li> <li>Index usage optimization</li> <li>Query rewriting techniques</li> </ul>"},{"location":"courses/data-engineering/database-fundamentals/#hardware-considerations","title":"Hardware Considerations","text":"<ul> <li>CPU vs I/O bound workloads</li> <li>Memory sizing for databases</li> <li>Storage types and their impact</li> <li>Network considerations</li> </ul> <p>Database fundamentals never go out of style. Master these concepts and you'll be valuable in any data role.</p>"},{"location":"courses/data-engineering/pipeline-design/","title":"Pipeline Design","text":"<p>Learn to build data pipelines that don't break at 3 AM and won't make you hate your life.</p>"},{"location":"courses/data-engineering/pipeline-design/#philosophy-simple-reliable-observable","title":"Philosophy: Simple, Reliable, Observable","text":"<p>Good pipeline design is about: - Simplicity: Easy to understand and modify - Reliability: Handles failures gracefully - Observability: You know what's happening</p>"},{"location":"courses/data-engineering/pipeline-design/#pipeline-patterns","title":"Pipeline Patterns","text":""},{"location":"courses/data-engineering/pipeline-design/#pattern-1-batch-etl","title":"Pattern 1: Batch ETL","text":"<p>The classic approach - extract, transform, load.</p> <p>When to use:  - Daily/weekly reporting - Historical data processing - Complex transformations</p> <p>Tools: Airflow, dbt, Spark</p>"},{"location":"courses/data-engineering/pipeline-design/#pattern-2-streaming-etl","title":"Pattern 2: Streaming ETL","text":"<p>Real-time data processing for immediate insights.</p> <p>When to use: - Real-time dashboards - Fraud detection - Live recommendations</p> <p>Tools: Kafka, Flink, Kinesis</p>"},{"location":"courses/data-engineering/pipeline-design/#pattern-3-lambda-architecture","title":"Pattern 3: Lambda Architecture","text":"<p>Combine batch and stream processing.</p> <p>When to use: - Need both real-time and historical views - Complex analytics requirements - High-volume data</p>"},{"location":"courses/data-engineering/pipeline-design/#pattern-4-kappa-architecture","title":"Pattern 4: Kappa Architecture","text":"<p>Stream-first approach with reprocessing capability.</p> <p>When to use: - Primarily real-time requirements - Simpler than Lambda - Event-driven architecture</p>"},{"location":"courses/data-engineering/pipeline-design/#design-principles","title":"Design Principles","text":""},{"location":"courses/data-engineering/pipeline-design/#1-idempotency","title":"1. Idempotency","text":"<p>Your pipeline should produce the same result if run multiple times.</p> <pre><code># Bad: Appends data every run\nINSERT INTO daily_sales VALUES (...)\n\n# Good: Upsert pattern\nINSERT INTO daily_sales VALUES (...) \nON CONFLICT (date) DO UPDATE SET ...\n</code></pre>"},{"location":"courses/data-engineering/pipeline-design/#2-error-handling","title":"2. Error Handling","text":"<p>Plan for failures because they will happen.</p> <ul> <li>Dead letter queues</li> <li>Retry strategies</li> <li>Circuit breakers</li> <li>Graceful degradation</li> </ul>"},{"location":"courses/data-engineering/pipeline-design/#3-monitoring-alerting","title":"3. Monitoring &amp; Alerting","text":"<p>Know when things break before your users do.</p> <ul> <li>Data freshness checks</li> <li>Volume anomaly detection</li> <li>Schema evolution monitoring</li> <li>SLA tracking</li> </ul>"},{"location":"courses/data-engineering/pipeline-design/#4-scalability","title":"4. Scalability","text":"<p>Design for tomorrow's data volume, not today's.</p> <ul> <li>Partitioning strategies</li> <li>Horizontal scaling</li> <li>Resource optimization</li> <li>Cost management</li> </ul>"},{"location":"courses/data-engineering/pipeline-design/#hands-on-projects","title":"Hands-On Projects","text":""},{"location":"courses/data-engineering/pipeline-design/#project-1-e-commerce-analytics-pipeline","title":"Project 1: E-commerce Analytics Pipeline","text":"<p>Build a complete pipeline processing: - User events from web/mobile - Transaction data from databases - Product catalog updates - Real-time recommendations</p>"},{"location":"courses/data-engineering/pipeline-design/#project-2-iot-data-processing","title":"Project 2: IoT Data Processing","text":"<p>Handle sensor data at scale: - Time-series data ingestion - Real-time anomaly detection - Historical trend analysis - Predictive maintenance alerts</p>"},{"location":"courses/data-engineering/pipeline-design/#project-3-social-media-analytics","title":"Project 3: Social Media Analytics","text":"<p>Process social media data: - API data ingestion - Sentiment analysis pipeline - Trend detection - Influence scoring</p> <p>Every pattern and principle here comes from real production systems. Learn from successes and failures in the field.</p>"},{"location":"courses/devops/","title":"DevOps &amp; Infrastructure","text":"<p>Learn to build, deploy, and manage scalable infrastructure.</p>"},{"location":"courses/devops/#quick-overview","title":"Quick Overview","text":"<ul> <li>Infrastructure as Code: Manage infrastructure like code</li> <li>Containers: Docker and Kubernetes orchestration</li> <li>Monitoring: Build observable systems</li> </ul> <p>Pick a topic to start learning!</p>"},{"location":"courses/devops/container-orchestration/","title":"Container Orchestration","text":"<p>Master Docker and Kubernetes to deploy applications that scale, heal themselves, and run anywhere.</p>"},{"location":"courses/devops/container-orchestration/#why-containers-and-orchestration","title":"Why Containers and Orchestration?","text":""},{"location":"courses/devops/container-orchestration/#the-container-revolution","title":"The Container Revolution","text":"<p>Containers solve the \"it works on my machine\" problem: - Consistency: Same environment everywhere - Portability: Run anywhere containers run - Efficiency: Better resource utilization than VMs - Speed: Fast startup and deployment</p>"},{"location":"courses/devops/container-orchestration/#why-orchestration","title":"Why Orchestration?","text":"<p>Managing containers manually doesn't scale: - Service discovery: How do containers find each other? - Load balancing: Distribute traffic across instances - Health checks: Restart failed containers - Rolling deployments: Update without downtime - Scaling: Add/remove instances based on demand</p>"},{"location":"courses/devops/container-orchestration/#docker-fundamentals","title":"Docker Fundamentals","text":""},{"location":"courses/devops/container-orchestration/#container-basics","title":"Container Basics","text":"<pre><code># Dockerfile example\nFROM python:3.9-slim\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\nEXPOSE 8000\n\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"courses/devops/container-orchestration/#multi-stage-builds","title":"Multi-Stage Builds","text":"<pre><code># Build stage\nFROM node:16 AS builder\nWORKDIR /app\nCOPY package*.json ./\nRUN npm install\nCOPY . .\nRUN npm run build\n\n# Production stage\nFROM nginx:alpine\nCOPY --from=builder /app/dist /usr/share/nginx/html\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre>"},{"location":"courses/devops/container-orchestration/#docker-compose-for-local-development","title":"Docker Compose for Local Development","text":"<pre><code>version: '3.8'\nservices:\n  app:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - DATABASE_URL=postgresql://user:pass@db:5432/myapp\n    depends_on:\n      - db\n\n  db:\n    image: postgres:13\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: pass\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\nvolumes:\n  postgres_data:\n</code></pre>"},{"location":"courses/devops/container-orchestration/#kubernetes-architecture","title":"Kubernetes Architecture","text":""},{"location":"courses/devops/container-orchestration/#core-concepts","title":"Core Concepts","text":""},{"location":"courses/devops/container-orchestration/#pods","title":"Pods","text":"<p>The smallest deployable unit: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: app\n    image: my-app:v1.0\n    ports:\n    - containerPort: 8080\n</code></pre></p>"},{"location":"courses/devops/container-orchestration/#deployments","title":"Deployments","text":"<p>Manage replica sets and rolling updates: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n      - name: app\n        image: my-app:v1.0\n        ports:\n        - containerPort: 8080\n</code></pre></p>"},{"location":"courses/devops/container-orchestration/#services","title":"Services","text":"<p>Expose applications to network traffic: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-app-service\nspec:\n  selector:\n    app: my-app\n  ports:\n  - port: 80\n    targetPort: 8080\n  type: LoadBalancer\n</code></pre></p>"},{"location":"courses/devops/container-orchestration/#advanced-kubernetes-concepts","title":"Advanced Kubernetes Concepts","text":""},{"location":"courses/devops/container-orchestration/#configmaps-and-secrets","title":"ConfigMaps and Secrets","text":"<pre><code># ConfigMap for configuration\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  database_host: \"db.example.com\"\n  debug_mode: \"false\"\n\n---\n# Secret for sensitive data\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\ntype: Opaque\ndata:\n  database_password: cGFzc3dvcmQ=  # base64 encoded\n</code></pre>"},{"location":"courses/devops/container-orchestration/#persistent-volumes","title":"Persistent Volumes","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"courses/devops/container-orchestration/#ingress-controllers","title":"Ingress Controllers","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-app-ingress\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  tls:\n  - hosts:\n    - myapp.example.com\n    secretName: myapp-tls\n  rules:\n  - host: myapp.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-app-service\n            port:\n              number: 80\n</code></pre>"},{"location":"courses/devops/container-orchestration/#helm-kubernetes-package-manager","title":"Helm: Kubernetes Package Manager","text":""},{"location":"courses/devops/container-orchestration/#chart-structure","title":"Chart Structure","text":"<pre><code>my-app-chart/\n\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 values.yaml\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u2514\u2500\u2500 ingress.yaml\n\u2514\u2500\u2500 charts/\n</code></pre>"},{"location":"courses/devops/container-orchestration/#templating-with-helm","title":"Templating with Helm","text":"<pre><code># templates/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Values.app.name }}\nspec:\n  replicas: {{ .Values.app.replicas }}\n  selector:\n    matchLabels:\n      app: {{ .Values.app.name }}\n  template:\n    spec:\n      containers:\n      - name: {{ .Values.app.name }}\n        image: {{ .Values.app.image }}:{{ .Values.app.tag }}\n</code></pre>"},{"location":"courses/devops/container-orchestration/#service-mesh-with-istio","title":"Service Mesh with Istio","text":""},{"location":"courses/devops/container-orchestration/#traffic-management","title":"Traffic Management","text":"<ul> <li>Intelligent routing</li> <li>Load balancing</li> <li>Circuit breakers</li> <li>Retries and timeouts</li> </ul>"},{"location":"courses/devops/container-orchestration/#security","title":"Security","text":"<ul> <li>mTLS between services</li> <li>Authorization policies</li> <li>Certificate management</li> </ul>"},{"location":"courses/devops/container-orchestration/#observability","title":"Observability","text":"<ul> <li>Distributed tracing</li> <li>Metrics collection</li> <li>Service graphs</li> </ul>"},{"location":"courses/devops/container-orchestration/#production-best-practices","title":"Production Best Practices","text":""},{"location":"courses/devops/container-orchestration/#resource-management","title":"Resource Management","text":"<pre><code>resources:\n  requests:\n    memory: \"256Mi\"\n    cpu: \"250m\"\n  limits:\n    memory: \"512Mi\"\n    cpu: \"500m\"\n</code></pre>"},{"location":"courses/devops/container-orchestration/#health-checks","title":"Health Checks","text":"<pre><code>livenessProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 10\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 5\n</code></pre>"},{"location":"courses/devops/container-orchestration/#security-contexts","title":"Security Contexts","text":"<pre><code>securityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n  readOnlyRootFilesystem: true\n  allowPrivilegeEscalation: false\n</code></pre>"},{"location":"courses/devops/container-orchestration/#hands-on-projects","title":"Hands-On Projects","text":""},{"location":"courses/devops/container-orchestration/#project-1-microservices-on-kubernetes","title":"Project 1: Microservices on Kubernetes","text":"<p>Deploy a complete microservices application: - Multiple services with different languages - Service-to-service communication - Shared databases and caches - API gateway with authentication</p>"},{"location":"courses/devops/container-orchestration/#project-2-gitops-with-argocd","title":"Project 2: GitOps with ArgoCD","text":"<p>Implement GitOps deployment: - Git-based configuration management - Automated deployments - Environment promotion - Rollback capabilities</p>"},{"location":"courses/devops/container-orchestration/#project-3-multi-cluster-setup","title":"Project 3: Multi-Cluster Setup","text":"<p>Deploy across multiple Kubernetes clusters: - Cluster federation - Cross-cluster service discovery - Multi-region deployments - Disaster recovery</p> <p>Container orchestration is the foundation of modern application deployment. Master these concepts and you'll be ready for any scale.</p>"},{"location":"courses/devops/infrastructure-as-code/","title":"Infrastructure as Code","text":"<p>Stop clicking buttons and start managing infrastructure like a developer. Version control, code reviews, and automated deployments for your infrastructure.</p>"},{"location":"courses/devops/infrastructure-as-code/#why-infrastructure-as-code","title":"Why Infrastructure as Code?","text":""},{"location":"courses/devops/infrastructure-as-code/#problems-with-manual-infrastructure","title":"Problems with Manual Infrastructure","text":"<ul> <li>Inconsistency: \"It works on my machine\" for infrastructure</li> <li>No version control: Can't track changes or rollback</li> <li>Documentation drift: Reality doesn't match documentation</li> <li>Scaling issues: Manual processes don't scale</li> <li>Human error: Mistakes happen when clicking buttons</li> </ul>"},{"location":"courses/devops/infrastructure-as-code/#benefits-of-iac","title":"Benefits of IaC","text":"<ul> <li>Reproducible: Same code = same infrastructure</li> <li>Version controlled: Track all changes</li> <li>Collaborative: Code reviews for infrastructure changes</li> <li>Automated: Deploy with confidence</li> <li>Cost effective: Destroy when not needed</li> </ul>"},{"location":"courses/devops/infrastructure-as-code/#iac-tools-comparison","title":"IaC Tools Comparison","text":""},{"location":"courses/devops/infrastructure-as-code/#terraform","title":"Terraform","text":"<p>Best for: Multi-cloud, complex infrastructures - Declarative language (HCL) - Strong state management - Large provider ecosystem - Plan before apply</p>"},{"location":"courses/devops/infrastructure-as-code/#cloudformation","title":"CloudFormation","text":"<p>Best for: AWS-only environments - Native AWS integration - JSON/YAML templates - Stack-based management - AWS support included</p>"},{"location":"courses/devops/infrastructure-as-code/#pulumi","title":"Pulumi","text":"<p>Best for: Developers who prefer real programming languages - Use Python, TypeScript, Go, C# - Object-oriented approach - Strong typing - Familiar development patterns</p>"},{"location":"courses/devops/infrastructure-as-code/#ansible","title":"Ansible","text":"<p>Best for: Configuration management + some provisioning - Agentless - YAML playbooks - Great for OS configuration - Limited cloud resource support</p>"},{"location":"courses/devops/infrastructure-as-code/#terraform-deep-dive","title":"Terraform Deep Dive","text":""},{"location":"courses/devops/infrastructure-as-code/#basic-concepts","title":"Basic Concepts","text":"<pre><code># Provider configuration\nprovider \"aws\" {\n  region = \"us-west-2\"\n}\n\n# Resource definition\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-0c02fb55956c7d316\"\n  instance_type = \"t2.micro\"\n\n  tags = {\n    Name = \"WebServer\"\n    Environment = \"Development\"\n  }\n}\n\n# Output values\noutput \"instance_ip\" {\n  value = aws_instance.web.public_ip\n}\n</code></pre>"},{"location":"courses/devops/infrastructure-as-code/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"courses/devops/infrastructure-as-code/#modules-for-reusability","title":"Modules for Reusability","text":"<pre><code># modules/web-server/main.tf\nvariable \"instance_type\" {\n  description = \"EC2 instance type\"\n  type        = string\n  default     = \"t2.micro\"\n}\n\nresource \"aws_instance\" \"web\" {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = var.instance_type\n  # ... more configuration\n}\n\n# Using the module\nmodule \"web_server\" {\n  source        = \"./modules/web-server\"\n  instance_type = \"t2.small\"\n}\n</code></pre>"},{"location":"courses/devops/infrastructure-as-code/#state-management","title":"State Management","text":"<ul> <li>Local state for development</li> <li>Remote state for production</li> <li>State locking with DynamoDB</li> <li>State encryption</li> </ul>"},{"location":"courses/devops/infrastructure-as-code/#workspaces-for-environments","title":"Workspaces for Environments","text":"<pre><code># Create environments\nterraform workspace new development\nterraform workspace new staging\nterraform workspace new production\n\n# Switch between environments\nterraform workspace select production\n</code></pre>"},{"location":"courses/devops/infrastructure-as-code/#best-practices","title":"Best Practices","text":""},{"location":"courses/devops/infrastructure-as-code/#1-structure-your-code","title":"1. Structure Your Code","text":"<pre><code>terraform/\n\u251c\u2500\u2500 environments/\n\u2502   \u251c\u2500\u2500 dev/\n\u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2514\u2500\u2500 prod/\n\u251c\u2500\u2500 modules/\n\u2502   \u251c\u2500\u2500 vpc/\n\u2502   \u251c\u2500\u2500 compute/\n\u2502   \u2514\u2500\u2500 database/\n\u251c\u2500\u2500 policies/\n\u2514\u2500\u2500 scripts/\n</code></pre>"},{"location":"courses/devops/infrastructure-as-code/#2-use-variables-and-locals","title":"2. Use Variables and Locals","text":"<pre><code># Variables for inputs\nvariable \"environment\" {\n  description = \"Environment name\"\n  type        = string\n}\n\n# Locals for computed values\nlocals {\n  common_tags = {\n    Environment = var.environment\n    Project     = \"my-project\"\n    ManagedBy   = \"terraform\"\n  }\n}\n</code></pre>"},{"location":"courses/devops/infrastructure-as-code/#3-plan-before-apply","title":"3. Plan Before Apply","text":"<pre><code># Always review changes\nterraform plan -out=tfplan\n\n# Apply only after review\nterraform apply tfplan\n</code></pre>"},{"location":"courses/devops/infrastructure-as-code/#4-use-remote-state","title":"4. Use Remote State","text":"<pre><code>terraform {\n  backend \"s3\" {\n    bucket = \"my-terraform-state\"\n    key    = \"prod/terraform.tfstate\"\n    region = \"us-west-2\"\n\n    # State locking\n    dynamodb_table = \"terraform-locks\"\n    encrypt        = true\n  }\n}\n</code></pre>"},{"location":"courses/devops/infrastructure-as-code/#hands-on-projects","title":"Hands-On Projects","text":""},{"location":"courses/devops/infrastructure-as-code/#project-1-three-tier-web-application","title":"Project 1: Three-Tier Web Application","text":"<p>Build a complete web application infrastructure: - VPC with public/private subnets - Load balancer - Auto Scaling Group - RDS database - S3 for static assets</p>"},{"location":"courses/devops/infrastructure-as-code/#project-2-kubernetes-cluster","title":"Project 2: Kubernetes Cluster","text":"<p>Deploy a production-ready Kubernetes cluster: - EKS/GKE cluster - Node groups with auto-scaling - Ingress controllers - Monitoring and logging</p>"},{"location":"courses/devops/infrastructure-as-code/#project-3-multi-cloud-setup","title":"Project 3: Multi-Cloud Setup","text":"<p>Deploy the same application across multiple clouds: - Terraform modules for portability - Cloud-specific optimizations - Cross-cloud networking - Disaster recovery setup</p>"},{"location":"courses/devops/infrastructure-as-code/#advanced-topics","title":"Advanced Topics","text":""},{"location":"courses/devops/infrastructure-as-code/#gitops-workflows","title":"GitOps Workflows","text":"<ul> <li>Infrastructure CI/CD pipelines</li> <li>Automated testing for infrastructure</li> <li>Policy as code with Sentinel/OPA</li> <li>Drift detection and remediation</li> </ul>"},{"location":"courses/devops/infrastructure-as-code/#security-best-practices","title":"Security Best Practices","text":"<ul> <li>Least privilege IAM policies</li> <li>Secret management</li> <li>Network security</li> <li>Compliance scanning</li> </ul>"},{"location":"courses/devops/infrastructure-as-code/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Resource tagging strategies</li> <li>Right-sizing instances</li> <li>Spot instances and reserved capacity</li> <li>Cost monitoring and alerts</li> </ul> <p>Infrastructure as Code isn't just about automation - it's about bringing software engineering practices to infrastructure management.</p>"},{"location":"courses/devops/monitoring-logging/","title":"Monitoring &amp; Logging","text":"<p>Build observable systems that tell you what's happening, what's broken, and what's about to break.</p>"},{"location":"courses/devops/monitoring-logging/#the-three-pillars-of-observability","title":"The Three Pillars of Observability","text":""},{"location":"courses/devops/monitoring-logging/#1-metrics","title":"1. Metrics","text":"<p>Numerical data about system behavior over time. - System metrics: CPU, memory, disk, network - Application metrics: Request rate, response time, error rate - Business metrics: User signups, revenue, conversion rates</p>"},{"location":"courses/devops/monitoring-logging/#2-logs","title":"2. Logs","text":"<p>Detailed records of events happening in your system. - Structured logs: JSON format for easy parsing - Centralized logging: All logs in one place - Log levels: DEBUG, INFO, WARN, ERROR, FATAL</p>"},{"location":"courses/devops/monitoring-logging/#3-traces","title":"3. Traces","text":"<p>End-to-end view of requests as they flow through distributed systems. - Distributed tracing: Follow requests across services - Span analysis: Understand bottlenecks - Dependency mapping: Visualize service interactions</p>"},{"location":"courses/devops/monitoring-logging/#the-prometheus-ecosystem","title":"The Prometheus Ecosystem","text":""},{"location":"courses/devops/monitoring-logging/#prometheus-fundamentals","title":"Prometheus Fundamentals","text":"<p>Time-series database designed for monitoring:</p> <pre><code># prometheus.yml\nglobal:\n  scrape_interval: 15s\n\nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n\n  - job_name: 'node-exporter'\n    static_configs:\n      - targets: ['localhost:9100']\n\n  - job_name: 'my-app'\n    static_configs:\n      - targets: ['localhost:8080']\n</code></pre>"},{"location":"courses/devops/monitoring-logging/#promql-query-language","title":"PromQL Query Language","text":"<pre><code># CPU usage percentage\n100 - (avg(irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)\n\n# HTTP request rate\nrate(http_requests_total[5m])\n\n# 95th percentile response time\nhistogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))\n\n# Error rate percentage\nrate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m]) * 100\n</code></pre>"},{"location":"courses/devops/monitoring-logging/#application-metrics-with-client-libraries","title":"Application Metrics with Client Libraries","text":"<pre><code># Python example with prometheus_client\nfrom prometheus_client import Counter, Histogram, start_http_server\n\nREQUEST_COUNT = Counter('requests_total', 'Total requests', ['method', 'endpoint'])\nREQUEST_DURATION = Histogram('request_duration_seconds', 'Request duration')\n\n@REQUEST_DURATION.time()\ndef process_request():\n    REQUEST_COUNT.labels(method='GET', endpoint='/api/users').inc()\n    # Your application logic here\n</code></pre>"},{"location":"courses/devops/monitoring-logging/#grafana-for-visualization","title":"Grafana for Visualization","text":""},{"location":"courses/devops/monitoring-logging/#dashboard-design-principles","title":"Dashboard Design Principles","text":"<ul> <li>User-focused: Start with what users care about</li> <li>Layered approach: Overview \u2192 drill-down \u2192 details</li> <li>Context matters: Add annotations and descriptions</li> <li>Alert on what matters: Not everything needs an alert</li> </ul>"},{"location":"courses/devops/monitoring-logging/#dashboard-examples","title":"Dashboard Examples","text":""},{"location":"courses/devops/monitoring-logging/#system-overview-dashboard","title":"System Overview Dashboard","text":"<ul> <li>CPU, Memory, Disk, Network usage</li> <li>System load and uptime</li> <li>Top processes and resource consumers</li> </ul>"},{"location":"courses/devops/monitoring-logging/#application-performance-dashboard","title":"Application Performance Dashboard","text":"<ul> <li>Request rate and response time</li> <li>Error rates and types</li> <li>Database connection pools</li> <li>Cache hit ratios</li> </ul>"},{"location":"courses/devops/monitoring-logging/#business-metrics-dashboard","title":"Business Metrics Dashboard","text":"<ul> <li>Active users</li> <li>Revenue metrics</li> <li>Feature usage</li> <li>Conversion funnels</li> </ul>"},{"location":"courses/devops/monitoring-logging/#grafana-alerting","title":"Grafana Alerting","text":"<pre><code># Alert rule example\n- alert: HighErrorRate\n  expr: rate(http_requests_total{status=~\"5..\"}[5m]) &gt; 0.1\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High error rate detected\"\n    description: \"Error rate is {{ $value }} requests/second\"\n</code></pre>"},{"location":"courses/devops/monitoring-logging/#elk-stack-for-logging","title":"ELK Stack for Logging","text":""},{"location":"courses/devops/monitoring-logging/#elasticsearch","title":"Elasticsearch","text":"<p>Distributed search and analytics engine: <pre><code># Index mapping for application logs\nPUT /app-logs\n{\n  \"mappings\": {\n    \"properties\": {\n      \"timestamp\": {\"type\": \"date\"},\n      \"level\": {\"type\": \"keyword\"},\n      \"message\": {\"type\": \"text\"},\n      \"service\": {\"type\": \"keyword\"},\n      \"trace_id\": {\"type\": \"keyword\"}\n    }\n  }\n}\n</code></pre></p>"},{"location":"courses/devops/monitoring-logging/#logstash","title":"Logstash","text":"<p>Data processing pipeline: <pre><code># logstash.conf\ninput {\n  beats {\n    port =&gt; 5044\n  }\n}\n\nfilter {\n  if [fields][service] == \"web-app\" {\n    grok {\n      match =&gt; { \"message\" =&gt; \"%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:msg}\" }\n    }\n    date {\n      match =&gt; [ \"timestamp\", \"ISO8601\" ]\n    }\n  }\n}\n\noutput {\n  elasticsearch {\n    hosts =&gt; [\"elasticsearch:9200\"]\n    index =&gt; \"app-logs-%{+YYYY.MM.dd}\"\n  }\n}\n</code></pre></p>"},{"location":"courses/devops/monitoring-logging/#kibana","title":"Kibana","text":"<p>Visualization and exploration: - Log search and filtering - Dashboard creation - Index pattern management - Alerting and monitoring</p>"},{"location":"courses/devops/monitoring-logging/#distributed-tracing-with-jaeger","title":"Distributed Tracing with Jaeger","text":""},{"location":"courses/devops/monitoring-logging/#instrumenting-applications","title":"Instrumenting Applications","text":"<pre><code># Python with OpenTelemetry\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n\n# Setup tracing\ntrace.set_tracer_provider(TracerProvider())\ntracer = trace.get_tracer(__name__)\n\njaeger_exporter = JaegerExporter(\n    agent_host_name=\"jaeger\",\n    agent_port=6831,\n)\n\nspan_processor = BatchSpanProcessor(jaeger_exporter)\ntrace.get_tracer_provider().add_span_processor(span_processor)\n\n# Use in application\n@tracer.start_as_current_span(\"process_order\")\ndef process_order(order_id):\n    with tracer.start_as_current_span(\"validate_order\") as span:\n        span.set_attribute(\"order.id\", order_id)\n        # Validation logic\n\n    with tracer.start_as_current_span(\"charge_payment\"):\n        # Payment processing\n        pass\n</code></pre>"},{"location":"courses/devops/monitoring-logging/#sli-slo-and-sla","title":"SLI, SLO, and SLA","text":""},{"location":"courses/devops/monitoring-logging/#service-level-indicators-slis","title":"Service Level Indicators (SLIs)","text":"<p>Specific metrics that matter to users: - Availability: Percentage of successful requests - Latency: Time to process requests - Throughput: Requests handled per second</p>"},{"location":"courses/devops/monitoring-logging/#service-level-objectives-slos","title":"Service Level Objectives (SLOs)","text":"<p>Target values for SLIs: - 99.9% availability - 95% of requests under 200ms - Handle 1000 RPS</p>"},{"location":"courses/devops/monitoring-logging/#service-level-agreements-slas","title":"Service Level Agreements (SLAs)","text":"<p>Contractual commitments with consequences: - Customer-facing commitments - Penalties for not meeting SLAs - Usually more lenient than SLOs</p>"},{"location":"courses/devops/monitoring-logging/#error-budgets","title":"Error Budgets","text":"<p>Mathematical way to balance reliability and feature velocity:</p> <pre><code>Error Budget = 1 - SLO\nIf SLO is 99.9%, error budget is 0.1%\nMonthly budget = 43.8 minutes of downtime\n</code></pre>"},{"location":"courses/devops/monitoring-logging/#alerting-best-practices","title":"Alerting Best Practices","text":""},{"location":"courses/devops/monitoring-logging/#alert-on-symptoms-not-causes","title":"Alert on Symptoms, Not Causes","text":"<pre><code># Good: Alert on user impact\n- alert: HighLatency\n  expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) &gt; 0.5\n\n# Not ideal: Alert on resource usage\n- alert: HighCPU\n  expr: cpu_usage &gt; 80\n</code></pre>"},{"location":"courses/devops/monitoring-logging/#runbook-integration","title":"Runbook Integration","text":"<p>Every alert should have: - Clear description of the problem - Impact on users - Investigation steps - Escalation procedures</p>"},{"location":"courses/devops/monitoring-logging/#alert-fatigue-prevention","title":"Alert Fatigue Prevention","text":"<ul> <li>Only alert on actionable items</li> <li>Use different severity levels</li> <li>Implement alert suppression</li> <li>Regular alert review and cleanup</li> </ul>"},{"location":"courses/devops/monitoring-logging/#hands-on-projects","title":"Hands-On Projects","text":""},{"location":"courses/devops/monitoring-logging/#project-1-complete-monitoring-stack","title":"Project 1: Complete Monitoring Stack","text":"<p>Deploy and configure: - Prometheus for metrics collection - Grafana for visualization - Alertmanager for notifications - Custom exporters for application metrics</p>"},{"location":"courses/devops/monitoring-logging/#project-2-centralized-logging","title":"Project 2: Centralized Logging","text":"<p>Build a logging pipeline: - Filebeat for log shipping - Logstash for processing - Elasticsearch for storage - Kibana for visualization</p>"},{"location":"courses/devops/monitoring-logging/#project-3-distributed-tracing","title":"Project 3: Distributed Tracing","text":"<p>Implement tracing across microservices: - Jaeger for trace collection - OpenTelemetry instrumentation - Service dependency mapping - Performance bottleneck identification</p> <p>Good monitoring is invisible when everything works and invaluable when things break. Invest time in observability and your future self will thank you.</p>"},{"location":"courses/sysadmin/bash-shell/","title":"Bash &amp; Shell Scripting","text":"<p>Master the command line and shell scripting for automation, system administration, and daily productivity. From basic commands to advanced scripting patterns.</p>"},{"location":"courses/sysadmin/bash-shell/#why-learn-bash","title":"Why Learn Bash?","text":"<p>Shell scripting is the foundation of system automation: - Automation: Reduce repetitive tasks - System administration: Manage servers and services - DevOps: CI/CD pipelines and deployment scripts - Data processing: Text manipulation and file operations - Universal: Available on almost every Unix-like system</p>"},{"location":"courses/sysadmin/bash-shell/#bash-fundamentals","title":"Bash Fundamentals","text":""},{"location":"courses/sysadmin/bash-shell/#basic-commands","title":"Basic Commands","text":"<pre><code># File and directory operations\nls -la                  # List files with details\ncd /path/to/directory   # Change directory\npwd                     # Print working directory\nmkdir -p dir1/dir2     # Create directories recursively\ncp -r source dest      # Copy files/directories\nmv source dest         # Move/rename files\nrm -rf directory       # Remove files/directories\nfind . -name \"*.txt\"   # Find files by pattern\n\n# File content operations\ncat file.txt           # Display file content\nless file.txt          # View file with pagination\nhead -n 10 file.txt    # Show first 10 lines\ntail -f file.txt       # Follow file changes\ngrep \"pattern\" file.txt # Search for pattern\nsed 's/old/new/g' file # Replace text\nawk '{print $1}' file  # Print first column\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#command-chaining","title":"Command Chaining","text":"<pre><code># Pipes - pass output to next command\nls -la | grep \"\\.txt\"              # List only .txt files\nps aux | grep \"nginx\" | awk '{print $2}'  # Get nginx PIDs\n\n# Command sequencing\ncommand1 &amp;&amp; command2               # Run command2 if command1 succeeds\ncommand1 || command2               # Run command2 if command1 fails\ncommand1; command2                 # Run both commands regardless\n\n# Input/Output redirection\ncommand &gt; file.txt                 # Redirect output to file\ncommand &gt;&gt; file.txt                # Append output to file\ncommand &lt; input.txt                # Read input from file\ncommand 2&gt; error.log               # Redirect error output\ncommand &amp;&gt; output.log              # Redirect both stdout and stderr\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#variables-and-environment","title":"Variables and Environment","text":""},{"location":"courses/sysadmin/bash-shell/#variable-assignment","title":"Variable Assignment","text":"<pre><code># Variable assignment (no spaces around =)\nname=\"John Doe\"\nage=30\npath=\"/usr/local/bin\"\n\n# Using variables\necho \"Hello, $name\"\necho \"Age: ${age} years\"\necho \"Path: $path\"\n\n# Command substitution\ncurrent_date=$(date)\nfiles_count=$(ls -1 | wc -l)\nuser_home=$(eval echo ~$USER)\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#environment-variables","title":"Environment Variables","text":"<pre><code># Common environment variables\necho $HOME          # Home directory\necho $PATH          # Executable search path\necho $USER          # Current user\necho $PWD           # Current directory\necho $SHELL         # Current shell\n\n# Setting environment variables\nexport DATABASE_URL=\"postgresql://localhost:5432/mydb\"\nexport PATH=\"$PATH:/usr/local/bin\"\n\n# Making variables persistent\necho 'export MY_VAR=\"value\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#special-variables","title":"Special Variables","text":"<pre><code># Script parameters\necho $0             # Script name\necho $1 $2 $3       # First three arguments\necho $#             # Number of arguments\necho $*             # All arguments as single string\necho $@             # All arguments as separate strings\necho $?             # Exit status of last command\necho $$             # Process ID of current script\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#control-structures","title":"Control Structures","text":""},{"location":"courses/sysadmin/bash-shell/#conditional-statements","title":"Conditional Statements","text":"<pre><code># If-then-else\nif [ \"$age\" -gt 18 ]; then\n    echo \"Adult\"\nelif [ \"$age\" -eq 18 ]; then\n    echo \"Just turned adult\"\nelse\n    echo \"Minor\"\nfi\n\n# File/directory tests\nif [ -f \"file.txt\" ]; then\n    echo \"File exists\"\nfi\n\nif [ -d \"directory\" ]; then\n    echo \"Directory exists\"\nfi\n\nif [ -r \"file.txt\" ]; then\n    echo \"File is readable\"\nfi\n\n# String comparisons\nif [ \"$string1\" = \"$string2\" ]; then\n    echo \"Strings are equal\"\nfi\n\nif [ -z \"$variable\" ]; then\n    echo \"Variable is empty\"\nfi\n\n# Numeric comparisons\nif [ \"$num1\" -eq \"$num2\" ]; then echo \"Equal\"; fi\nif [ \"$num1\" -lt \"$num2\" ]; then echo \"Less than\"; fi\nif [ \"$num1\" -gt \"$num2\" ]; then echo \"Greater than\"; fi\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#loops","title":"Loops","text":"<pre><code># For loop with list\nfor item in apple banana orange; do\n    echo \"Fruit: $item\"\ndone\n\n# For loop with range\nfor i in {1..10}; do\n    echo \"Number: $i\"\ndone\n\n# For loop with files\nfor file in *.txt; do\n    echo \"Processing: $file\"\n    # Process file here\ndone\n\n# While loop\ncounter=1\nwhile [ $counter -le 10 ]; do\n    echo \"Count: $counter\"\n    ((counter++))\ndone\n\n# Until loop\nuntil [ $counter -gt 20 ]; do\n    echo \"Count: $counter\"\n    ((counter++))\ndone\n\n# Reading file line by line\nwhile IFS= read -r line; do\n    echo \"Line: $line\"\ndone &lt; file.txt\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#case-statements","title":"Case Statements","text":"<pre><code>case \"$1\" in\n    \"start\")\n        echo \"Starting service...\"\n        ;;\n    \"stop\")\n        echo \"Stopping service...\"\n        ;;\n    \"restart\")\n        echo \"Restarting service...\"\n        ;;\n    *)\n        echo \"Usage: $0 {start|stop|restart}\"\n        exit 1\n        ;;\nesac\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#functions","title":"Functions","text":""},{"location":"courses/sysadmin/bash-shell/#function-definition","title":"Function Definition","text":"<pre><code># Simple function\ngreet() {\n    echo \"Hello, $1!\"\n}\n\n# Function with local variables\ncalculate_disk_usage() {\n    local directory=\"$1\"\n    local usage=$(du -sh \"$directory\" 2&gt;/dev/null | cut -f1)\n    echo \"Directory '$directory' uses: $usage\"\n}\n\n# Function with return value\nis_service_running() {\n    local service_name=\"$1\"\n    if systemctl is-active --quiet \"$service_name\"; then\n        return 0  # Success\n    else\n        return 1  # Failure\n    fi\n}\n\n# Using functions\ngreet \"World\"\ncalculate_disk_usage \"/var/log\"\n\nif is_service_running \"nginx\"; then\n    echo \"Nginx is running\"\nelse\n    echo \"Nginx is not running\"\nfi\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#text-processing","title":"Text Processing","text":""},{"location":"courses/sysadmin/bash-shell/#grep-pattern-matching","title":"grep - Pattern Matching","text":"<pre><code># Basic pattern matching\ngrep \"error\" logfile.txt\ngrep -i \"error\" logfile.txt           # Case insensitive\ngrep -v \"debug\" logfile.txt           # Invert match (exclude)\ngrep -n \"error\" logfile.txt           # Show line numbers\ngrep -r \"TODO\" /project/src/          # Recursive search\n\n# Regular expressions\ngrep \"^Error\" logfile.txt             # Lines starting with \"Error\"\ngrep \"Error$\" logfile.txt             # Lines ending with \"Error\"\ngrep \"Error.*critical\" logfile.txt    # Lines with \"Error\" followed by \"critical\"\ngrep -E \"[0-9]{3}-[0-9]{3}-[0-9]{4}\" contacts.txt  # Phone number pattern\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#sed-stream-editor","title":"sed - Stream Editor","text":"<pre><code># Substitution\nsed 's/old/new/' file.txt             # Replace first occurrence per line\nsed 's/old/new/g' file.txt            # Replace all occurrences\nsed 's/old/new/gi' file.txt           # Case insensitive replacement\n\n# Line operations\nsed -n '5p' file.txt                  # Print line 5\nsed -n '5,10p' file.txt               # Print lines 5-10\nsed '5d' file.txt                     # Delete line 5\nsed '/pattern/d' file.txt             # Delete lines matching pattern\n\n# In-place editing\nsed -i 's/old/new/g' file.txt         # Edit file in place\nsed -i.bak 's/old/new/g' file.txt     # Create backup before editing\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#awk-text-processing","title":"awk - Text Processing","text":"<pre><code># Field processing\nawk '{print $1}' file.txt             # Print first field\nawk '{print $1, $3}' file.txt         # Print first and third fields\nawk -F',' '{print $2}' file.csv       # Use comma as field separator\n\n# Patterns and conditions\nawk '/error/ {print $0}' logfile.txt  # Print lines containing \"error\"\nawk '$3 &gt; 100 {print $1, $3}' data.txt # Print if third field &gt; 100\nawk 'NR &gt; 1 {print}' file.txt         # Skip header line\n\n# Built-in variables\nawk '{print NR, $0}' file.txt         # Print line number and line\nawk 'END {print NR}' file.txt         # Print total line count\n\n# Calculations\nawk '{sum += $1} END {print sum}' numbers.txt  # Sum first column\nawk '{print $1, $2, $1*$2}' data.txt          # Calculate product\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#advanced-scripting-patterns","title":"Advanced Scripting Patterns","text":""},{"location":"courses/sysadmin/bash-shell/#error-handling","title":"Error Handling","text":"<pre><code>#!/bin/bash\nset -e  # Exit on any error\nset -u  # Exit on undefined variable\nset -o pipefail  # Exit on pipe failure\n\n# Error handling function\nhandle_error() {\n    echo \"Error occurred in script $0 at line $1\"\n    exit 1\n}\ntrap 'handle_error $LINENO' ERR\n\n# Check if command exists\nif ! command -v git &amp;&gt; /dev/null; then\n    echo \"Git is required but not installed.\"\n    exit 1\nfi\n\n# Check file existence before processing\nif [[ ! -f \"config.txt\" ]]; then\n    echo \"Config file not found!\"\n    exit 1\nfi\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#logging","title":"Logging","text":"<pre><code># Simple logging function\nlog() {\n    local level=\"$1\"\n    shift\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] [$level] $*\" | tee -a app.log\n}\n\n# Usage\nlog \"INFO\" \"Starting application\"\nlog \"ERROR\" \"Failed to connect to database\"\nlog \"DEBUG\" \"Processing user ID: $user_id\"\n\n# Colored output\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nNC='\\033[0m' # No Color\n\nlog_error() {\n    echo -e \"${RED}[ERROR]${NC} $*\" &gt;&amp;2\n}\n\nlog_success() {\n    echo -e \"${GREEN}[SUCCESS]${NC} $*\"\n}\n\nlog_warning() {\n    echo -e \"${YELLOW}[WARNING]${NC} $*\"\n}\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#configuration-management","title":"Configuration Management","text":"<pre><code># Configuration file parsing\nload_config() {\n    local config_file=\"$1\"\n\n    if [[ -f \"$config_file\" ]]; then\n        # Source the config file\n        source \"$config_file\"\n    else\n        echo \"Config file $config_file not found!\"\n        exit 1\n    fi\n}\n\n# Default values\nDB_HOST=\"localhost\"\nDB_PORT=\"5432\"\nDB_NAME=\"myapp\"\n\n# Load configuration\nload_config \"config.sh\"\n\n# Validate required configuration\nrequired_vars=(\"DB_HOST\" \"DB_USER\" \"DB_PASSWORD\")\nfor var in \"${required_vars[@]}\"; do\n    if [[ -z \"${!var:-}\" ]]; then\n        log_error \"Required variable $var is not set\"\n        exit 1\n    fi\ndone\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#system-administration-scripts","title":"System Administration Scripts","text":""},{"location":"courses/sysadmin/bash-shell/#service-management","title":"Service Management","text":"<pre><code>#!/bin/bash\n# Service management script\n\nSERVICE_NAME=\"myapp\"\nPID_FILE=\"/var/run/${SERVICE_NAME}.pid\"\nLOG_FILE=\"/var/log/${SERVICE_NAME}.log\"\n\nstart_service() {\n    if [[ -f \"$PID_FILE\" ]]; then\n        local pid=$(cat \"$PID_FILE\")\n        if ps -p \"$pid\" &gt; /dev/null 2&gt;&amp;1; then\n            echo \"Service is already running (PID: $pid)\"\n            return 1\n        else\n            rm -f \"$PID_FILE\"\n        fi\n    fi\n\n    echo \"Starting $SERVICE_NAME...\"\n    nohup /usr/local/bin/myapp &gt; \"$LOG_FILE\" 2&gt;&amp;1 &amp; \n    echo $! &gt; \"$PID_FILE\"\n    echo \"Service started (PID: $!)\"\n}\n\nstop_service() {\n    if [[ -f \"$PID_FILE\" ]]; then\n        local pid=$(cat \"$PID_FILE\")\n        echo \"Stopping $SERVICE_NAME (PID: $pid)...\"\n        kill \"$pid\"\n        rm -f \"$PID_FILE\"\n        echo \"Service stopped\"\n    else\n        echo \"Service is not running\"\n    fi\n}\n\ncase \"$1\" in\n    start)   start_service ;;\n    stop)    stop_service ;;\n    restart) stop_service; sleep 2; start_service ;;\n    *)       echo \"Usage: $0 {start|stop|restart}\" ;;\nesac\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#backup-script","title":"Backup Script","text":"<pre><code>#!/bin/bash\n# Automated backup script\n\nBACKUP_SOURCE=\"/home/user/important_data\"\nBACKUP_DEST=\"/backup\"\nBACKUP_NAME=\"backup_$(date +%Y%m%d_%H%M%S)\"\nRETENTION_DAYS=7\n\ncreate_backup() {\n    local full_backup_path=\"$BACKUP_DEST/$BACKUP_NAME.tar.gz\"\n\n    echo \"Creating backup: $full_backup_path\"\n\n    if tar -czf \"$full_backup_path\" -C \"$(dirname \"$BACKUP_SOURCE\")\" \"$(basename \"$BACKUP_SOURCE\")\"; then\n        echo \"Backup created successfully\"\n        return 0\n    else\n        echo \"Backup failed\"\n        return 1\n    fi\n}\n\ncleanup_old_backups() {\n    echo \"Cleaning up backups older than $RETENTION_DAYS days...\"\n    find \"$BACKUP_DEST\" -name \"backup_*.tar.gz\" -mtime +$RETENTION_DAYS -delete\n    echo \"Cleanup completed\"\n}\n\n# Main execution\nif [[ ! -d \"$BACKUP_SOURCE\" ]]; then\n    log_error \"Source directory does not exist: $BACKUP_SOURCE\"\n    exit 1\nfi\n\nif [[ ! -d \"$BACKUP_DEST\" ]]; then\n    mkdir -p \"$BACKUP_DEST\"\nfi\n\nif create_backup; then\n    cleanup_old_backups\n    log_success \"Backup process completed successfully\"\nelse\n    log_error \"Backup process failed\"\n    exit 1\nfi\n</code></pre>"},{"location":"courses/sysadmin/bash-shell/#system-monitoring","title":"System Monitoring","text":"<pre><code>#!/bin/bash\n# System monitoring script\n\ncheck_disk_usage() {\n    echo \"=== Disk Usage ===\"\n    df -h | awk '$5 &gt; 80 {print \"WARNING: \" $1 \" is \" $5 \" full\"}'\n}\n\ncheck_memory_usage() {\n    echo \"=== Memory Usage ===\"\n    local mem_usage=$(free | awk 'NR==2{printf \"%.1f\", $3*100/$2}')\n    if (( $(echo \"$mem_usage &gt; 80\" | bc -l) )); then\n        echo \"WARNING: Memory usage is ${mem_usage}%\"\n    else\n        echo \"Memory usage: ${mem_usage}%\"\n    fi\n}\n\ncheck_cpu_load() {\n    echo \"=== CPU Load ===\"\n    local load_avg=$(uptime | awk -F'load average:' '{print $2}' | awk '{print $1}' | sed 's/,//')\n    local cpu_cores=$(nproc)\n    local load_percent=$(echo \"scale=2; $load_avg / $cpu_cores * 100\" | bc)\n\n    if (( $(echo \"$load_percent &gt; 80\" | bc -l) )); then\n        echo \"WARNING: CPU load is ${load_percent}%\"\n    else\n        echo \"CPU load: ${load_percent}%\"\n    fi\n}\n\ncheck_services() {\n    echo \"=== Service Status ===\"\n    local services=(\"nginx\" \"mysql\" \"redis\")\n\n    for service in \"${services[@]}\"; do\n        if systemctl is-active --quiet \"$service\"; then\n            echo \"$service: Running\"\n        else\n            echo \"WARNING: $service is not running\"\n        fi\n    done\n}\n\n# Generate report\necho \"System Health Report - $(date)\"\necho \"==================================\"\ncheck_disk_usage\necho\ncheck_memory_usage\necho\ncheck_cpu_load\necho\ncheck_services\n</code></pre> <p>Bash scripting is the Swiss Army knife of system administration. Master these patterns and you'll be able to automate almost anything on Unix-like systems.</p>"},{"location":"courses/sysadmin/linux-fundamentals/","title":"Linux Fundamentals","text":"<p>Master the Linux operating system - from basic commands to system administration. Essential knowledge for any technical role.</p>"},{"location":"courses/sysadmin/linux-fundamentals/#why-linux-matters","title":"Why Linux Matters","text":"<p>Linux powers the modern world: - Servers: 96% of web servers run Linux - Cloud computing: AWS, GCP, Azure all run on Linux - Containers: Docker and Kubernetes are Linux-native - Mobile: Android is based on Linux - IoT: Most embedded systems use Linux - Development: Popular choice for developers</p>"},{"location":"courses/sysadmin/linux-fundamentals/#getting-started-with-linux","title":"Getting Started with Linux","text":""},{"location":"courses/sysadmin/linux-fundamentals/#linux-distributions","title":"Linux Distributions","text":"<p>Choose the right distribution for your needs:</p>"},{"location":"courses/sysadmin/linux-fundamentals/#ubuntu","title":"Ubuntu","text":"<ul> <li>Best for: Beginners, desktop use, development</li> <li>Pros: User-friendly, great community, extensive documentation</li> <li>Cons: Can be resource-heavy</li> </ul>"},{"location":"courses/sysadmin/linux-fundamentals/#centosrhel","title":"CentOS/RHEL","text":"<ul> <li>Best for: Enterprise environments, servers</li> <li>Pros: Stable, long-term support, enterprise features</li> <li>Cons: Older packages, learning curve</li> </ul>"},{"location":"courses/sysadmin/linux-fundamentals/#debian","title":"Debian","text":"<ul> <li>Best for: Servers, stability-focused environments</li> <li>Pros: Very stable, lightweight, secure</li> <li>Cons: Conservative package updates</li> </ul>"},{"location":"courses/sysadmin/linux-fundamentals/#arch-linux","title":"Arch Linux","text":"<ul> <li>Best for: Advanced users, customization</li> <li>Pros: Cutting-edge packages, minimal base system</li> <li>Cons: Steep learning curve, manual configuration</li> </ul>"},{"location":"courses/sysadmin/linux-fundamentals/#linux-file-system-hierarchy","title":"Linux File System Hierarchy","text":"<pre><code>/                 # Root directory\n\u251c\u2500\u2500 bin/          # Essential user binaries\n\u251c\u2500\u2500 boot/         # Boot loader files\n\u251c\u2500\u2500 dev/          # Device files\n\u251c\u2500\u2500 etc/          # Configuration files\n\u251c\u2500\u2500 home/         # User home directories\n\u251c\u2500\u2500 lib/          # Shared libraries\n\u251c\u2500\u2500 media/        # Removable media mount points\n\u251c\u2500\u2500 mnt/          # Temporary mount points\n\u251c\u2500\u2500 opt/          # Optional software packages\n\u251c\u2500\u2500 proc/         # Process and kernel information\n\u251c\u2500\u2500 root/         # Root user home directory\n\u251c\u2500\u2500 run/          # Runtime variable data\n\u251c\u2500\u2500 sbin/         # System binaries\n\u251c\u2500\u2500 srv/          # Service data\n\u251c\u2500\u2500 sys/          # System information\n\u251c\u2500\u2500 tmp/          # Temporary files\n\u251c\u2500\u2500 usr/          # User utilities and applications\n\u251c\u2500\u2500 var/          # Variable data (logs, databases)\n\u2514\u2500\u2500 snap/         # Snap packages (Ubuntu)\n</code></pre>"},{"location":"courses/sysadmin/linux-fundamentals/#essential-commands","title":"Essential Commands","text":""},{"location":"courses/sysadmin/linux-fundamentals/#navigation-and-file-operations","title":"Navigation and File Operations","text":"<pre><code># Directory navigation\npwd                    # Print working directory\nls -la                 # List files with details\ncd /path/to/directory  # Change directory\ncd ~                   # Go to home directory\ncd ..                  # Go up one directory\ncd -                   # Go to previous directory\n\n# File operations\ntouch filename         # Create empty file\nmkdir -p path/to/dir   # Create directories recursively\ncp file1 file2         # Copy file\ncp -r dir1/ dir2/      # Copy directory recursively\nmv old_name new_name   # Move/rename file\nrm file                # Remove file\nrm -rf directory       # Remove directory recursively\nln -s target link      # Create symbolic link\n\n# File viewing\ncat file               # Display entire file\nless file              # View file with pagination\nhead -n 20 file        # Show first 20 lines\ntail -n 20 file        # Show last 20 lines\ntail -f file           # Follow file changes (live)\n</code></pre>"},{"location":"courses/sysadmin/linux-fundamentals/#file-permissions","title":"File Permissions","text":"<pre><code># Understanding permissions\n# drwxrwxrwx\n# d = directory, - = file\n# rwx = read, write, execute (user, group, others)\n\n# Viewing permissions\nls -l filename\n\n# Changing permissions\nchmod 755 filename     # rwxr-xr-x\nchmod u+x filename     # Add execute for user\nchmod g-w filename     # Remove write for group\nchmod o=r filename     # Set others to read only\n\n# Changing ownership\nchown user:group filename\nchown -R user:group directory/\n\n# Special permissions\nchmod +t directory     # Sticky bit\nchmod u+s filename     # SUID\nchmod g+s filename     # SGID\n</code></pre>"},{"location":"courses/sysadmin/linux-fundamentals/#process-management","title":"Process Management","text":"<pre><code># Viewing processes\nps aux                 # Show all processes\nps aux | grep nginx    # Find specific process\ntop                    # Real-time process viewer\nhtop                   # Enhanced process viewer (if installed)\npstree                 # Process tree view\n\n# Process control\njobs                   # Show active jobs\nbg                     # Put job in background\nfg                     # Bring job to foreground\nnohup command &amp;        # Run command immune to hangups\nkill PID               # Kill process by PID\nkill -9 PID            # Force kill process\nkillall process_name   # Kill all processes by name\npkill -f pattern       # Kill processes matching pattern\n\n# System information\nuptime                 # System uptime and load\nwho                    # Who is logged in\nw                      # Who is logged in and what they're doing\nid                     # Current user and group IDs\nuname -a               # System information\n</code></pre>"},{"location":"courses/sysadmin/linux-fundamentals/#system-administration","title":"System Administration","text":""},{"location":"courses/sysadmin/linux-fundamentals/#package-management","title":"Package Management","text":"<pre><code># Ubuntu/Debian (APT)\napt update             # Update package list\napt upgrade            # Upgrade installed packages\napt install package    # Install package\napt remove package     # Remove package\napt search keyword     # Search for packages\napt list --installed   # List installed packages\n\n# CentOS/RHEL (YUM/DNF)\nyum update             # Update packages\nyum install package    # Install package\nyum remove package     # Remove package\nyum search keyword     # Search packages\ndnf install package    # DNF (newer than YUM)\n\n# Arch Linux (Pacman)\npacman -Syu            # Update system\npacman -S package      # Install package\npacman -R package      # Remove package\npacman -Ss keyword     # Search packages\n\n# Universal (Snap)\nsnap install package   # Install snap package\nsnap list              # List installed snaps\nsnap refresh           # Update all snaps\n</code></pre>"},{"location":"courses/sysadmin/linux-fundamentals/#service-management-systemd","title":"Service Management (systemd)","text":"<pre><code># Service control\nsystemctl start service_name      # Start service\nsystemctl stop service_name       # Stop service\nsystemctl restart service_name    # Restart service\nsystemctl reload service_name     # Reload configuration\nsystemctl enable service_name     # Enable auto-start\nsystemctl disable service_name    # Disable auto-start\n\n# Service status\nsystemctl status service_name     # Show service status\nsystemctl is-active service_name  # Check if service is active\nsystemctl is-enabled service_name # Check if service is enabled\n\n# System control\nsystemctl reboot              # Reboot system\nsystemctl poweroff            # Shutdown system\nsystemctl suspend             # Suspend system\n\n# Logs\njournalctl -u service_name    # Show service logs\njournalctl -f                 # Follow logs\njournalctl --since \"1 hour ago\" # Recent logs\n</code></pre>"},{"location":"courses/sysadmin/linux-fundamentals/#network-configuration","title":"Network Configuration","text":"<pre><code># Network information\nip addr show           # Show IP addresses\nip route show          # Show routing table\nnetstat -tuln          # Show listening ports\nss -tuln               # Modern alternative to netstat\nlsof -i :80            # Show what's using port 80\n\n# Network configuration\nip addr add 192.168.1.100/24 dev eth0  # Add IP address\nip route add default via 192.168.1.1   # Add default route\n\n# DNS configuration\ncat /etc/resolv.conf   # Show DNS servers\nnslookup domain.com    # DNS lookup\ndig domain.com         # Advanced DNS lookup\n\n# Network testing\nping -c 4 google.com   # Ping with count\ntraceroute google.com  # Trace route to destination\nwget http://example.com/file  # Download file\ncurl -I http://example.com    # HTTP headers only\n</code></pre>"},{"location":"courses/sysadmin/linux-fundamentals/#disk-and-storage-management","title":"Disk and Storage Management","text":"<pre><code># Disk usage\ndf -h                  # Show disk usage\ndu -sh /path/to/dir    # Show directory size\ndu -h --max-depth=1    # Show subdirectory sizes\nncdu                   # Interactive disk usage (if installed)\n\n# Disk partitioning\nlsblk                  # List block devices\nfdisk -l               # List partitions\nfdisk /dev/sda         # Partition disk (interactive)\nparted /dev/sda        # Advanced partitioning tool\n\n# File systems\nmkfs.ext4 /dev/sda1    # Format as ext4\nmkfs.xfs /dev/sda1     # Format as XFS\nmount /dev/sda1 /mnt   # Mount filesystem\numount /mnt            # Unmount filesystem\nfsck /dev/sda1         # Check filesystem\n\n# Mounting\nmount                  # Show mounted filesystems\ncat /etc/fstab         # Permanent mount configuration\nmount -a               # Mount all filesystems in fstab\n</code></pre>"},{"location":"courses/sysadmin/linux-fundamentals/#system-monitoring-and-performance","title":"System Monitoring and Performance","text":""},{"location":"courses/sysadmin/linux-fundamentals/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code># CPU and memory\ntop                    # Real-time system monitor\nhtop                   # Enhanced system monitor\nvmstat 1               # Virtual memory statistics\niostat 1               # I/O statistics\nsar -u 1               # CPU utilization (sysstat package)\n\n# Memory\nfree -h                # Memory usage\ncat /proc/meminfo      # Detailed memory information\nps aux --sort=-%mem | head  # Processes by memory usage\n\n# Disk I/O\niotop                  # Real-time disk I/O monitor\nlsof                   # List open files\nfuser -v /path         # Show processes using file/directory\n\n# Network\niftop                  # Network bandwidth monitor\nnethogs                # Network usage by process\ntcpdump -i eth0        # Packet capture\nwireshark              # GUI packet analyzer\n</code></pre>"},{"location":"courses/sysadmin/linux-fundamentals/#log-management","title":"Log Management","text":"<pre><code># System logs\ntail -f /var/log/syslog      # Follow system log\ntail -f /var/log/auth.log    # Authentication log\ntail -f /var/log/kern.log    # Kernel log\ndmesg                        # Kernel ring buffer\n\n# Application logs\ntail -f /var/log/nginx/access.log    # Web server access log\ntail -f /var/log/mysql/error.log     # Database error log\n\n# Log rotation\nlogrotate -d /etc/logrotate.conf     # Test log rotation\ncat /etc/logrotate.conf              # Log rotation configuration\n\n# Journal logs (systemd)\njournalctl                    # All journal logs\njournalctl -f                 # Follow journal logs\njournalctl -u nginx           # Service-specific logs\njournalctl --since yesterday  # Recent logs\n</code></pre>"},{"location":"courses/sysadmin/linux-fundamentals/#security-basics","title":"Security Basics","text":""},{"location":"courses/sysadmin/linux-fundamentals/#user-management","title":"User Management","text":"<pre><code># User operations\nuseradd username            # Add user\nusermod -aG group username  # Add user to group\nuserdel username            # Delete user\npasswd username             # Change password\nsu - username               # Switch user\nsudo command                # Run command as root\n\n# Group operations\ngroupadd groupname          # Add group\ngroupdel groupname          # Delete group\ngroups username             # Show user's groups\nid username                 # Show user and group IDs\n</code></pre>"},{"location":"courses/sysadmin/linux-fundamentals/#firewall-management","title":"Firewall Management","text":"<pre><code># UFW (Ubuntu Firewall)\nufw enable                  # Enable firewall\nufw disable                 # Disable firewall\nufw status                  # Show firewall status\nufw allow 22                # Allow SSH\nufw allow 80/tcp            # Allow HTTP\nufw deny from 192.168.1.100 # Block specific IP\n\n# iptables (traditional)\niptables -L                 # List rules\niptables -A INPUT -p tcp --dport 22 -j ACCEPT  # Allow SSH\niptables-save &gt; /etc/iptables/rules.v4  # Save rules\n\n# firewalld (CentOS/RHEL)\nfirewall-cmd --state        # Check firewall state\nfirewall-cmd --list-all     # List all rules\nfirewall-cmd --add-port=80/tcp --permanent  # Add rule\nfirewall-cmd --reload       # Reload rules\n</code></pre>"},{"location":"courses/sysadmin/linux-fundamentals/#ssh-configuration","title":"SSH Configuration","text":"<pre><code># SSH client\nssh user@hostname           # Connect to remote host\nssh -i keyfile user@host    # Connect with specific key\nscp file user@host:/path    # Copy file to remote host\nrsync -av local/ user@host:remote/  # Sync directories\n\n# SSH server configuration\nsudo nano /etc/ssh/sshd_config\n# Key settings:\n# Port 22\n# PermitRootLogin no\n# PasswordAuthentication no\n# PubkeyAuthentication yes\n\n# SSH keys\nssh-keygen -t rsa -b 4096   # Generate key pair\nssh-copy-id user@host       # Copy public key to remote host\n</code></pre>"},{"location":"courses/sysadmin/linux-fundamentals/#advanced-topics","title":"Advanced Topics","text":""},{"location":"courses/sysadmin/linux-fundamentals/#shell-scripting-integration","title":"Shell Scripting Integration","text":"<pre><code># System information script\n#!/bin/bash\necho \"=== System Information ===\"\necho \"Hostname: $(hostname)\"\necho \"Uptime: $(uptime -p)\"\necho \"Load Average: $(uptime | awk -F'load average:' '{print $2}')\"\necho \"Memory Usage: $(free -h | awk '/^Mem:/ {print $3 \"/\" $2}')\"\necho \"Disk Usage: $(df -h / | awk 'NR==2 {print $3 \"/\" $2 \" (\" $5 \")\"}')\"\n</code></pre>"},{"location":"courses/sysadmin/linux-fundamentals/#automation-with-cron","title":"Automation with Cron","text":"<pre><code># Edit crontab\ncrontab -e\n\n# Cron examples\n0 2 * * *     /path/to/backup.sh              # Daily at 2 AM\n*/15 * * * *  /path/to/check_service.sh       # Every 15 minutes\n0 0 1 * *     /path/to/monthly_cleanup.sh     # Monthly\n\n# View cron jobs\ncrontab -l\ncat /etc/crontab\n</code></pre>"},{"location":"courses/sysadmin/linux-fundamentals/#environment-variables","title":"Environment Variables","text":"<pre><code># System-wide environment\n/etc/environment            # System environment variables\n/etc/profile               # System-wide shell initialization\n/etc/bash.bashrc           # System-wide bash configuration\n\n# User-specific environment\n~/.bashrc                  # User bash configuration\n~/.profile                 # User shell initialization\n~/.bash_profile            # User bash login configuration\n\n# Setting variables\nexport PATH=$PATH:/usr/local/bin\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk\n</code></pre> <p>Linux is the foundation of modern computing. Master these fundamentals and you'll have the skills to work with any Unix-like system.</p>"},{"location":"courses/sysadmin/network-basics/","title":"Network Basics","text":"<p>Understanding networking fundamentals for system administrators, DevOps engineers, and developers. From basic concepts to practical troubleshooting.</p>"},{"location":"courses/sysadmin/network-basics/#why-network-knowledge-matters","title":"Why Network Knowledge Matters","text":"<p>In today's connected world, everything depends on networks: - Application deployment: Services need to communicate - Security: Understand attack vectors and protections - Troubleshooting: Diagnose connectivity issues - Performance: Optimize network-dependent applications - Cloud computing: Virtual networks and load balancers</p>"},{"location":"courses/sysadmin/network-basics/#osi-model-and-tcpip-stack","title":"OSI Model and TCP/IP Stack","text":""},{"location":"courses/sysadmin/network-basics/#osi-model-7-layers","title":"OSI Model (7 Layers)","text":"<pre><code>7. Application    - HTTP, HTTPS, FTP, SSH, DNS\n6. Presentation   - Encryption, compression, formatting\n5. Session        - Session management, authentication\n4. Transport      - TCP, UDP (reliability, flow control)\n3. Network        - IP routing, addressing\n2. Data Link      - Ethernet, WiFi (frame formatting)\n1. Physical       - Cables, radio waves, electrical signals\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#tcpip-model-4-layers","title":"TCP/IP Model (4 Layers)","text":"<pre><code>4. Application    - HTTP, DNS, SSH, FTP\n3. Transport      - TCP, UDP\n2. Internet       - IP, ICMP, ARP\n1. Network Access - Ethernet, WiFi\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#ip-addressing-and-subnetting","title":"IP Addressing and Subnetting","text":""},{"location":"courses/sysadmin/network-basics/#ipv4-addressing","title":"IPv4 Addressing","text":"<pre><code># IPv4 address format: x.x.x.x (32 bits)\n192.168.1.100\n\n# Address classes\nClass A: 1.0.0.0    - 126.255.255.255  (/8)\nClass B: 128.0.0.0  - 191.255.255.255  (/16)\nClass C: 192.0.0.0  - 223.255.255.255  (/24)\n\n# Private address ranges\n10.0.0.0     - 10.255.255.255    (Class A)\n172.16.0.0   - 172.31.255.255    (Class B)\n192.168.0.0  - 192.168.255.255   (Class C)\n\n# Special addresses\n127.0.0.1    - Loopback (localhost)\n0.0.0.0      - Default route/any address\n255.255.255.255 - Broadcast address\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#subnet-masks-and-cidr","title":"Subnet Masks and CIDR","text":"<pre><code># Subnet mask notation\n255.255.255.0   = /24 (24 bits for network, 8 for hosts)\n255.255.240.0   = /20 (20 bits for network, 12 for hosts)\n255.255.255.128 = /25 (25 bits for network, 7 for hosts)\n\n# CIDR calculation examples\n192.168.1.0/24    # Network: 192.168.1.0, Hosts: 1-254\n192.168.1.0/25    # Network: 192.168.1.0, Hosts: 1-126\n192.168.1.128/25  # Network: 192.168.1.128, Hosts: 129-254\n\n# Usable hosts calculation\nHosts = 2^(32-prefix) - 2\n/24 = 2^8 - 2 = 254 hosts\n/25 = 2^7 - 2 = 126 hosts\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#ipv6-basics","title":"IPv6 Basics","text":"<pre><code># IPv6 address format (128 bits)\n2001:0db8:85a3:0000:0000:8a2e:0370:7334\n\n# Compressed notation\n2001:db8:85a3::8a2e:370:7334\n\n# Address types\n::1/128           # Loopback\n2001:db8::/32     # Documentation prefix\nfe80::/10         # Link-local\nfc00::/7          # Unique local (private)\n2000::/3          # Global unicast\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#network-protocols","title":"Network Protocols","text":""},{"location":"courses/sysadmin/network-basics/#tcp-vs-udp","title":"TCP vs UDP","text":""},{"location":"courses/sysadmin/network-basics/#tcp-transmission-control-protocol","title":"TCP (Transmission Control Protocol)","text":"<pre><code># Characteristics\n- Connection-oriented\n- Reliable delivery\n- Ordered delivery\n- Flow control\n- Error checking\n- Higher overhead\n\n# Use cases\n- Web browsing (HTTP/HTTPS)\n- Email (SMTP, IMAP, POP3)\n- File transfer (FTP, SSH)\n- Database connections\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#udp-user-datagram-protocol","title":"UDP (User Datagram Protocol)","text":"<pre><code># Characteristics\n- Connectionless\n- No delivery guarantee\n- No ordering guarantee\n- No flow control\n- Minimal overhead\n- Fast\n\n# Use cases\n- DNS queries\n- Video streaming\n- Online gaming\n- DHCP\n- NTP (time synchronization)\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#common-ports","title":"Common Ports","text":"<pre><code># Well-known ports (0-1023)\n20/21    - FTP (data/control)\n22       - SSH\n23       - Telnet\n25       - SMTP\n53       - DNS\n67/68    - DHCP\n80       - HTTP\n110      - POP3\n143      - IMAP\n443      - HTTPS\n993      - IMAPS\n995      - POP3S\n\n# Registered ports (1024-49151)\n3306     - MySQL\n5432     - PostgreSQL\n6379     - Redis\n8080     - HTTP alternate\n9200     - Elasticsearch\n\n# Dynamic/private ports (49152-65535)\n# Used for outbound connections\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#network-configuration","title":"Network Configuration","text":""},{"location":"courses/sysadmin/network-basics/#linux-network-configuration","title":"Linux Network Configuration","text":"<pre><code># Modern approach (ip command)\nip addr show                    # Show IP addresses\nip addr add 192.168.1.100/24 dev eth0  # Add IP address\nip addr del 192.168.1.100/24 dev eth0  # Remove IP address\nip route show                   # Show routing table\nip route add default via 192.168.1.1   # Add default route\nip link show                    # Show network interfaces\nip link set eth0 up             # Bring interface up\nip link set eth0 down           # Bring interface down\n\n# Legacy approach (deprecated but still common)\nifconfig eth0 192.168.1.100 netmask 255.255.255.0\nroute add default gw 192.168.1.1\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#network-interface-configuration-files","title":"Network Interface Configuration Files","text":"<pre><code># Ubuntu/Debian (/etc/network/interfaces)\nauto eth0\niface eth0 inet static\n    address 192.168.1.100\n    netmask 255.255.255.0\n    gateway 192.168.1.1\n    dns-nameservers 8.8.8.8 8.8.4.4\n\n# CentOS/RHEL (/etc/sysconfig/network-scripts/ifcfg-eth0)\nDEVICE=eth0\nBOOTPROTO=static\nIPADDR=192.168.1.100\nNETMASK=255.255.255.0\nGATEWAY=192.168.1.1\nDNS1=8.8.8.8\nDNS2=8.8.4.4\nONBOOT=yes\n\n# Modern systemd (netplan on Ubuntu)\n# /etc/netplan/01-network-manager-all.yaml\nnetwork:\n  version: 2\n  ethernets:\n    eth0:\n      dhcp4: false\n      addresses: [192.168.1.100/24]\n      gateway4: 192.168.1.1\n      nameservers:\n        addresses: [8.8.8.8, 8.8.4.4]\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#dns-configuration","title":"DNS Configuration","text":"<pre><code># /etc/resolv.conf\nnameserver 8.8.8.8\nnameserver 8.8.4.4\nsearch company.local\ndomain company.local\n\n# /etc/hosts (local hostname resolution)\n127.0.0.1   localhost\n192.168.1.10 server.company.local server\n192.168.1.20 database.company.local database\n\n# systemd-resolved (modern systems)\nresolvectl status\nresolvectl query google.com\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#network-services","title":"Network Services","text":""},{"location":"courses/sysadmin/network-basics/#dhcp-dynamic-host-configuration-protocol","title":"DHCP (Dynamic Host Configuration Protocol)","text":"<pre><code># DHCP client\ndhclient eth0               # Request IP address\ndhclient -r eth0           # Release IP address\ncat /var/lib/dhcp/dhclient.leases  # View lease information\n\n# DHCP server configuration (/etc/dhcp/dhcpd.conf)\nsubnet 192.168.1.0 netmask 255.255.255.0 {\n    range 192.168.1.100 192.168.1.200;\n    option routers 192.168.1.1;\n    option domain-name-servers 8.8.8.8, 8.8.4.4;\n    option domain-name \"company.local\";\n    default-lease-time 86400;\n    max-lease-time 86400;\n}\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#dns-domain-name-system","title":"DNS (Domain Name System)","text":"<pre><code># DNS lookup tools\nnslookup google.com         # Basic DNS lookup\ndig google.com              # Advanced DNS lookup\ndig @8.8.8.8 google.com     # Query specific DNS server\ndig google.com MX           # Query mail exchange records\ndig google.com NS           # Query name server records\ndig -x 8.8.8.8              # Reverse DNS lookup\n\n# DNS record types\nA       - IPv4 address\nAAAA    - IPv6 address\nCNAME   - Canonical name (alias)\nMX      - Mail exchange\nNS      - Name server\nPTR     - Pointer (reverse DNS)\nSOA     - Start of authority\nTXT     - Text record\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#network-troubleshooting","title":"Network Troubleshooting","text":""},{"location":"courses/sysadmin/network-basics/#connectivity-testing","title":"Connectivity Testing","text":"<pre><code># Basic connectivity\nping -c 4 google.com        # Test reachability\nping6 -c 4 ipv6.google.com  # IPv6 ping\ntraceroute google.com       # Trace path to destination\ntraceroute6 ipv6.google.com # IPv6 traceroute\nmtr google.com              # Continuous traceroute\n\n# Port testing\ntelnet google.com 80        # Test TCP port\nnc -zv google.com 80        # Test TCP port with netcat\nnc -zvu google.com 53       # Test UDP port with netcat\ntimeout 5 bash -c \"&lt;/dev/tcp/google.com/80\"  # Bash TCP test\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#network-analysis","title":"Network Analysis","text":"<pre><code># Active connections\nnetstat -tuln               # Show listening ports\nnetstat -tun                # Show all connections\nss -tuln                    # Modern alternative to netstat\nss -tulpn                   # Show process names\nlsof -i                     # Show all network connections\nlsof -i :80                 # Show what's using port 80\n\n# Packet capture\ntcpdump -i eth0             # Capture on interface\ntcpdump -i eth0 port 80     # Capture HTTP traffic\ntcpdump -i eth0 host 192.168.1.100  # Capture specific host\ntcpdump -w capture.pcap -i eth0      # Save to file\ntcpdump -r capture.pcap     # Read from file\n\n# Network statistics\ncat /proc/net/dev           # Interface statistics\niftop                       # Real-time bandwidth monitor\nnethogs                     # Network usage by process\nvnstat                      # Network traffic statistics\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#common-network-issues","title":"Common Network Issues","text":""},{"location":"courses/sysadmin/network-basics/#dns-resolution-problems","title":"DNS Resolution Problems","text":"<pre><code># Test DNS resolution\nnslookup domain.com\ndig domain.com\n\n# Check DNS configuration\ncat /etc/resolv.conf\nsystemd-resolve --status\n\n# Flush DNS cache\nsudo systemd-resolve --flush-caches  # systemd\nsudo service nscd restart           # nscd\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#routing-issues","title":"Routing Issues","text":"<pre><code># Check routing table\nip route show\nroute -n\n\n# Test gateway connectivity\nping $(ip route | grep default | awk '{print $3}')\n\n# Trace route to destination\ntraceroute -n destination\n\n# Check for asymmetric routing\n# Ensure return path uses same route\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#firewall-issues","title":"Firewall Issues","text":"<pre><code># Check firewall status\nufw status verbose          # Ubuntu\nfirewall-cmd --list-all     # CentOS/RHEL\niptables -L -n -v           # iptables\n\n# Temporarily disable firewall for testing\nufw disable                 # Ubuntu\nsystemctl stop firewalld    # CentOS/RHEL\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#network-security-basics","title":"Network Security Basics","text":""},{"location":"courses/sysadmin/network-basics/#port-scanning-detection","title":"Port Scanning Detection","text":"<pre><code># Monitor for port scans\nnetstat -an | grep SYN_RECV | wc -l  # Count half-open connections\nss -tan state syn-recv | wc -l       # Modern alternative\n\n# Log connection attempts\niptables -A INPUT -p tcp --dport 22 -j LOG --log-prefix \"SSH: \"\ntail -f /var/log/kern.log | grep \"SSH:\"\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#basic-network-access-control","title":"Basic Network Access Control","text":"<pre><code># Block specific IP with iptables\niptables -A INPUT -s 192.168.1.100 -j DROP\n\n# Allow only specific networks\niptables -A INPUT -s 192.168.1.0/24 -j ACCEPT\niptables -A INPUT -j DROP\n\n# Rate limiting SSH connections\niptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --set\niptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --update --seconds 60 --hitcount 3 -j DROP\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#network-monitoring","title":"Network Monitoring","text":"<pre><code># Monitor network connections\nwatch -n 1 'netstat -tuln | grep LISTEN'\n\n# Monitor bandwidth usage\niftop -i eth0               # Interface bandwidth\nnload eth0                  # Interface load\n\n# Monitor for suspicious activity\ntail -f /var/log/auth.log | grep \"Failed password\"\ngrep \"Invalid user\" /var/log/auth.log\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#virtual-networking","title":"Virtual Networking","text":""},{"location":"courses/sysadmin/network-basics/#vlans-virtual-lans","title":"VLANs (Virtual LANs)","text":"<pre><code># Create VLAN interface\nip link add link eth0 name eth0.100 type vlan id 100\nip addr add 192.168.100.1/24 dev eth0.100\nip link set eth0.100 up\n\n# Configure VLAN permanently (Ubuntu)\nauto eth0.100\niface eth0.100 inet static\n    address 192.168.100.1\n    netmask 255.255.255.0\n    vlan-raw-device eth0\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#network-bridges","title":"Network Bridges","text":"<pre><code># Create bridge\nip link add br0 type bridge\nip link set br0 up\n\n# Add interfaces to bridge\nip link set eth0 master br0\nip link set eth1 master br0\n\n# Assign IP to bridge\nip addr add 192.168.1.1/24 dev br0\n</code></pre>"},{"location":"courses/sysadmin/network-basics/#network-namespaces","title":"Network Namespaces","text":"<pre><code># Create network namespace\nip netns add test_ns\n\n# Run command in namespace\nip netns exec test_ns ip addr show\n\n# Create virtual ethernet pair\nip link add veth0 type veth peer name veth1\n\n# Move interface to namespace\nip link set veth1 netns test_ns\n\n# Configure interfaces\nip addr add 10.0.0.1/24 dev veth0\nip netns exec test_ns ip addr add 10.0.0.2/24 dev veth1\nip link set veth0 up\nip netns exec test_ns ip link set veth1 up\n</code></pre> <p>Network knowledge is essential for any IT role. Understanding these fundamentals will help you troubleshoot connectivity issues, secure your systems, and design scalable network architectures.</p>"},{"location":"courses/sysadmin/windows/","title":"Windows Administration","text":"<p>Master Windows system administration for modern environments. Learn PowerShell, Active Directory, and Windows Server management.</p>"},{"location":"courses/sysadmin/windows/#why-windows-administration-matters","title":"Why Windows Administration Matters","text":"<p>Even in a Linux-heavy world, Windows systems are everywhere: - Desktop environments: Most developers use Windows workstations - Enterprise applications: Many business apps run on Windows - Hybrid environments: Mixed Windows/Linux infrastructures - Legacy systems: Existing Windows infrastructure needs management</p>"},{"location":"courses/sysadmin/windows/#powershell-fundamentals","title":"PowerShell Fundamentals","text":""},{"location":"courses/sysadmin/windows/#getting-started-with-powershell","title":"Getting Started with PowerShell","text":"<pre><code># Check PowerShell version\n$PSVersionTable\n\n# Get help for any command\nGet-Help Get-Process -Examples\n\n# Find commands\nGet-Command *Service*\n\n# Get object properties\nGet-Service | Get-Member\n</code></pre>"},{"location":"courses/sysadmin/windows/#object-oriented-approach","title":"Object-Oriented Approach","text":"<p>Unlike traditional shells, PowerShell works with objects: <pre><code># Get processes and work with objects\n$processes = Get-Process\n$processes | Where-Object { $_.CPU -gt 100 }\n$processes | Sort-Object CPU -Descending | Select-Object -First 5\n\n# Work with services\nGet-Service | Where-Object { $_.Status -eq 'Running' } | \n    Select-Object Name, Status, StartType\n</code></pre></p>"},{"location":"courses/sysadmin/windows/#variables-and-data-types","title":"Variables and Data Types","text":"<pre><code># Variables\n$name = \"Server01\"\n$port = 443\n$isOnline = $true\n\n# Arrays\n$servers = @(\"Server01\", \"Server02\", \"Server03\")\n$numbers = 1..10\n\n# Hash tables\n$serverInfo = @{\n    Name = \"Server01\"\n    IP = \"192.168.1.100\"\n    OS = \"Windows Server 2022\"\n}\n</code></pre>"},{"location":"courses/sysadmin/windows/#functions-and-scripts","title":"Functions and Scripts","text":"<pre><code># Simple function\nfunction Get-DiskSpace {\n    param(\n        [string]$ComputerName = $env:COMPUTERNAME\n    )\n\n    Get-WmiObject -Class Win32_LogicalDisk -ComputerName $ComputerName |\n        Select-Object DeviceID, \n                     @{Name=\"Size(GB)\";Expression={[math]::Round($_.Size/1GB,2)}},\n                     @{Name=\"FreeSpace(GB)\";Expression={[math]::Round($_.FreeSpace/1GB,2)}},\n                     @{Name=\"PercentFree\";Expression={[math]::Round(($_.FreeSpace/$_.Size)*100,2)}}\n}\n\n# Advanced function with parameters\nfunction Install-Software {\n    [CmdletBinding()]\n    param(\n        [Parameter(Mandatory=$true)]\n        [string]$SoftwareName,\n\n        [Parameter(Mandatory=$true)]\n        [string]$InstallerPath,\n\n        [string[]]$ComputerName = $env:COMPUTERNAME,\n\n        [switch]$Silent\n    )\n\n    foreach ($Computer in $ComputerName) {\n        Write-Verbose \"Installing $SoftwareName on $Computer\"\n\n        if ($Silent) {\n            $arguments = \"/S\"\n        } else {\n            $arguments = \"\"\n        }\n\n        Invoke-Command -ComputerName $Computer -ScriptBlock {\n            Start-Process -FilePath $using:InstallerPath -ArgumentList $using:arguments -Wait\n        }\n    }\n}\n</code></pre>"},{"location":"courses/sysadmin/windows/#active-directory-management","title":"Active Directory Management","text":""},{"location":"courses/sysadmin/windows/#user-management","title":"User Management","text":"<pre><code># Import Active Directory module\nImport-Module ActiveDirectory\n\n# Create new user\nNew-ADUser -Name \"John Doe\" -GivenName \"John\" -Surname \"Doe\" -SamAccountName \"jdoe\" -UserPrincipalName \"jdoe@company.com\" -Path \"OU=Users,DC=company,DC=com\" -AccountPassword (ConvertTo-SecureString \"TempPassword123!\" -AsPlainText -Force) -Enabled $true\n\n# Bulk user creation from CSV\n$users = Import-Csv -Path \"C:\\Users\\new_users.csv\"\nforeach ($user in $users) {\n    New-ADUser -Name \"$($user.FirstName) $($user.LastName)\" -GivenName $user.FirstName -Surname $user.LastName -SamAccountName $user.Username -UserPrincipalName \"$($user.Username)@company.com\" -Path $user.OU -AccountPassword (ConvertTo-SecureString $user.Password -AsPlainText -Force) -Enabled $true\n}\n\n# Modify user properties\nSet-ADUser -Identity \"jdoe\" -Department \"IT\" -Title \"System Administrator\"\n\n# Disable inactive users\n$inactiveUsers = Search-ADAccount -AccountInactive -TimeSpan (New-TimeSpan -Days 90) -UsersOnly\n$inactiveUsers | Disable-ADAccount\n</code></pre>"},{"location":"courses/sysadmin/windows/#group-management","title":"Group Management","text":"<pre><code># Create security group\nNew-ADGroup -Name \"IT Support\" -GroupScope Global -GroupCategory Security -Path \"OU=Groups,DC=company,DC=com\"\n\n# Add users to group\nAdd-ADGroupMember -Identity \"IT Support\" -Members \"jdoe\", \"asmith\"\n\n# Get group membership\nGet-ADGroupMember -Identity \"IT Support\" | Select-Object Name, SamAccountName\n\n# Find groups a user belongs to\nGet-ADUser -Identity \"jdoe\" -Properties MemberOf | Select-Object -ExpandProperty MemberOf\n</code></pre>"},{"location":"courses/sysadmin/windows/#organizational-units-ous","title":"Organizational Units (OUs)","text":"<pre><code># Create OU structure\nNew-ADOrganizationalUnit -Name \"Company\" -Path \"DC=company,DC=com\"\nNew-ADOrganizationalUnit -Name \"Departments\" -Path \"OU=Company,DC=company,DC=com\"\nNew-ADOrganizationalUnit -Name \"IT\" -Path \"OU=Departments,OU=Company,DC=company,DC=com\"\nNew-ADOrganizationalUnit -Name \"Sales\" -Path \"OU=Departments,OU=Company,DC=company,DC=com\"\n\n# Move users to appropriate OUs\nGet-ADUser -Filter \"Department -eq 'IT'\" | Move-ADObject -TargetPath \"OU=IT,OU=Departments,OU=Company,DC=company,DC=com\"\n</code></pre>"},{"location":"courses/sysadmin/windows/#group-policy-management","title":"Group Policy Management","text":""},{"location":"courses/sysadmin/windows/#creating-and-linking-gpos","title":"Creating and Linking GPOs","text":"<pre><code># Import Group Policy module\nImport-Module GroupPolicy\n\n# Create new GPO\nNew-GPO -Name \"Desktop Security Policy\" -Comment \"Security settings for desktop computers\"\n\n# Link GPO to OU\nNew-GPLink -Name \"Desktop Security Policy\" -Target \"OU=Desktops,OU=Company,DC=company,DC=com\"\n\n# Set registry value via GPO\nSet-GPRegistryValue -Name \"Desktop Security Policy\" -Key \"HKLM\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\" -ValueName \"EnableLUA\" -Type DWord -Value 1\n\n# Import security template\nImport-GPO -BackupGpoName \"Security Template\" -Path \"C:\\GPOBackups\" -TargetName \"Desktop Security Policy\"\n</code></pre>"},{"location":"courses/sysadmin/windows/#gpo-reporting","title":"GPO Reporting","text":"<pre><code># Generate GPO report\nGet-GPOReport -Name \"Desktop Security Policy\" -ReportType Html -Path \"C:\\Reports\\GPOReport.html\"\n\n# Get all GPOs linked to an OU\nGet-GPInheritance -Target \"OU=IT,OU=Departments,OU=Company,DC=company,DC=com\"\n\n# Find unlinked GPOs\nGet-GPO -All | Where-Object { $_.GpoStatus -eq \"AllSettingsDisabled\" }\n</code></pre>"},{"location":"courses/sysadmin/windows/#windows-server-management","title":"Windows Server Management","text":""},{"location":"courses/sysadmin/windows/#server-roles-and-features","title":"Server Roles and Features","text":"<pre><code># List available features\nGet-WindowsFeature\n\n# Install IIS\nInstall-WindowsFeature -Name IIS-WebServerRole -IncludeManagementTools\n\n# Install Active Directory Domain Services\nInstall-WindowsFeature -Name AD-Domain-Services -IncludeManagementTools\n\n# Promote server to domain controller\nImport-Module ADDSDeployment\nInstall-ADDSForest -DomainName \"company.com\" -InstallDns -SafeModeAdministratorPassword (ConvertTo-SecureString \"SafeModePassword123!\" -AsPlainText -Force)\n</code></pre>"},{"location":"courses/sysadmin/windows/#windows-updates","title":"Windows Updates","text":"<pre><code># Install PSWindowsUpdate module\nInstall-Module PSWindowsUpdate\n\n# Check for updates\nGet-WUList\n\n# Install all updates\nGet-WUInstall -AcceptAll -AutoReboot\n\n# Install specific updates\nGet-WUInstall -KBArticleID KB5000802 -AcceptAll\n\n# Schedule update installation\nGet-WUInstall -AcceptAll -ScheduleJob (Get-Date).AddHours(2)\n</code></pre>"},{"location":"courses/sysadmin/windows/#event-log-management","title":"Event Log Management","text":"<pre><code># Get recent errors from System log\nGet-EventLog -LogName System -EntryType Error -Newest 10\n\n# Get specific events\nGet-WinEvent -FilterHashtable @{LogName='Security'; ID=4625} -MaxEvents 50\n\n# Clear event logs\nClear-EventLog -LogName Application\n\n# Create custom event log\nNew-EventLog -LogName \"CustomApp\" -Source \"MyApplication\"\nWrite-EventLog -LogName \"CustomApp\" -Source \"MyApplication\" -EventId 1001 -EntryType Information -Message \"Application started successfully\"\n</code></pre>"},{"location":"courses/sysadmin/windows/#registry-management","title":"Registry Management","text":""},{"location":"courses/sysadmin/windows/#reading-and-writing-registry","title":"Reading and Writing Registry","text":"<pre><code># Read registry value\nGet-ItemProperty -Path \"HKLM:\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" -Name \"ProductName\"\n\n# Create registry key\nNew-Item -Path \"HKLM:\\SOFTWARE\\MyApp\" -Force\n\n# Set registry value\nSet-ItemProperty -Path \"HKLM:\\SOFTWARE\\MyApp\" -Name \"Version\" -Value \"1.0.0\"\n\n# Remove registry key\nRemove-Item -Path \"HKLM:\\SOFTWARE\\MyApp\" -Recurse\n\n# Export registry key\nreg export \"HKLM\\SOFTWARE\\MyApp\" \"C:\\Backup\\MyApp.reg\"\n\n# Import registry file\nreg import \"C:\\Backup\\MyApp.reg\"\n</code></pre>"},{"location":"courses/sysadmin/windows/#file-system-management","title":"File System Management","text":""},{"location":"courses/sysadmin/windows/#ntfs-permissions","title":"NTFS Permissions","text":"<pre><code># Get ACL for a folder\nGet-Acl -Path \"C:\\SharedFolder\"\n\n# Set permissions\n$acl = Get-Acl -Path \"C:\\SharedFolder\"\n$accessRule = New-Object System.Security.AccessControl.FileSystemAccessRule(\"DOMAIN\\User\", \"FullControl\", \"ContainerInherit,ObjectInherit\", \"None\", \"Allow\")\n$acl.SetAccessRule($accessRule)\nSet-Acl -Path \"C:\\SharedFolder\" -AclObject $acl\n\n# Remove permissions\n$acl.RemoveAccessRule($accessRule)\nSet-Acl -Path \"C:\\SharedFolder\" -AclObject $acl\n</code></pre>"},{"location":"courses/sysadmin/windows/#disk-management","title":"Disk Management","text":"<pre><code># Get disk information\nGet-Disk\n\n# Initialize new disk\nInitialize-Disk -Number 1 -PartitionStyle GPT\n\n# Create partition\nNew-Partition -DiskNumber 1 -UseMaximumSize -DriveLetter D\n\n# Format volume\nFormat-Volume -DriveLetter D -FileSystem NTFS -NewFileSystemLabel \"Data\"\n\n# Resize partition\nResize-Partition -DriveLetter D -Size 500GB\n</code></pre>"},{"location":"courses/sysadmin/windows/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"courses/sysadmin/windows/#performance-counters","title":"Performance Counters","text":"<pre><code># Get CPU usage\nGet-Counter \"\\Processor(_Total)\\% Processor Time\"\n\n# Monitor multiple counters\n$counters = @(\n    \"\\Processor(_Total)\\% Processor Time\",\n    \"\\Memory\\Available MBytes\",\n    \"\\PhysicalDisk(_Total)\\% Disk Time\"\n)\nGet-Counter -Counter $counters -SampleInterval 5 -MaxSamples 10\n\n# Create custom performance counter set\n$counterSetName = \"MyApp Performance\"\n$counters = @(\"Requests per Second\", \"Response Time\")\n</code></pre>"},{"location":"courses/sysadmin/windows/#system-information","title":"System Information","text":"<pre><code># Get system information\nGet-ComputerInfo\n\n# Get installed software\nGet-WmiObject -Class Win32_Product | Select-Object Name, Version, Vendor\n\n# Get running processes\nGet-Process | Sort-Object CPU -Descending | Select-Object -First 10\n\n# Get network configuration\nGet-NetIPConfiguration\nGet-NetAdapter | Where-Object Status -eq \"Up\"\n</code></pre>"},{"location":"courses/sysadmin/windows/#automation-and-scripting","title":"Automation and Scripting","text":""},{"location":"courses/sysadmin/windows/#scheduled-tasks","title":"Scheduled Tasks","text":"<pre><code># Create scheduled task\n$action = New-ScheduledTaskAction -Execute \"PowerShell.exe\" -Argument \"-File C:\\Scripts\\Backup.ps1\"\n$trigger = New-ScheduledTaskTrigger -Daily -At \"2:00AM\"\n$settings = New-ScheduledTaskSettingsSet -ExecutionTimeLimit (New-TimeSpan -Hours 2)\nRegister-ScheduledTask -TaskName \"Daily Backup\" -Action $action -Trigger $trigger -Settings $settings -User \"SYSTEM\"\n\n# Modify scheduled task\nSet-ScheduledTask -TaskName \"Daily Backup\" -Trigger (New-ScheduledTaskTrigger -Weekly -DaysOfWeek Monday,Wednesday,Friday -At \"3:00AM\")\n\n# Run scheduled task\nStart-ScheduledTask -TaskName \"Daily Backup\"\n</code></pre>"},{"location":"courses/sysadmin/windows/#remote-management","title":"Remote Management","text":"<pre><code># Enable PowerShell remoting\nEnable-PSRemoting -Force\n\n# Execute commands on remote computers\nInvoke-Command -ComputerName \"Server01\", \"Server02\" -ScriptBlock { Get-Service | Where-Object Status -eq \"Stopped\" }\n\n# Create persistent session\n$session = New-PSSession -ComputerName \"Server01\"\nInvoke-Command -Session $session -ScriptBlock { Import-Module ActiveDirectory }\nInvoke-Command -Session $session -ScriptBlock { Get-ADUser -Filter * }\n</code></pre>"},{"location":"courses/sysadmin/windows/#security-best-practices","title":"Security Best Practices","text":""},{"location":"courses/sysadmin/windows/#user-account-control-uac","title":"User Account Control (UAC)","text":"<pre><code># Check UAC status\nGet-ItemProperty -Path \"HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\" -Name \"EnableLUA\"\n\n# Configure UAC levels\nSet-ItemProperty -Path \"HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\" -Name \"ConsentPromptBehaviorAdmin\" -Value 2\n</code></pre>"},{"location":"courses/sysadmin/windows/#windows-defender","title":"Windows Defender","text":"<pre><code># Get Windows Defender status\nGet-MpComputerStatus\n\n# Update definitions\nUpdate-MpSignature\n\n# Run quick scan\nStart-MpScan -ScanType QuickScan\n\n# Configure exclusions\nAdd-MpPreference -ExclusionPath \"C:\\MyApp\"\nAdd-MpPreference -ExclusionExtension \".log\"\n</code></pre> <p>Windows administration combines traditional GUI tools with powerful command-line automation. Master PowerShell and you'll be able to manage Windows environments at any scale.</p>"},{"location":"projects/","title":"My Projects","text":""},{"location":"projects/#things-ive-built-learned-from","title":"\ufffd\ufe0f Things I've Built &amp; Learned From","text":"<p>Here are some projects I've worked on - both successful ones and learning experiences. I try to document not just what worked, but also what didn't and what I'd do differently next time.</p>"},{"location":"projects/#current-projects","title":"\ufffd Current Projects","text":"<p>[Add your current/ongoing projects here]</p>"},{"location":"projects/#coming-soon","title":"Coming Soon...","text":"<ul> <li>[Project you're working on]</li> <li>[Something you're planning to build]</li> </ul>"},{"location":"projects/#completed-projects","title":"\ufffd Completed Projects","text":""},{"location":"projects/#infrastructure-devops","title":"Infrastructure &amp; DevOps","text":"<p>[List your infrastructure projects with brief descriptions]</p>"},{"location":"projects/#data-engineering","title":"Data Engineering","text":"<p>[List your data projects with brief descriptions]</p>"},{"location":"projects/#ai-machine-learning","title":"AI &amp; Machine Learning","text":"<p>[List your ML projects with brief descriptions]</p>"},{"location":"projects/#project-philosophy","title":"\ufffd Project Philosophy","text":"<p>I like to work on projects that teach me something new, solve a real problem (even if it's just my own), or let me experiment with technologies I'm curious about.</p> <p>Most of my projects start as \"I wonder if I can...\" and sometimes they actually work out!</p>"},{"location":"projects/#find-my-code","title":"\ud83d\udd17 Find My Code","text":"<p>[Add links to your repositories, if you want to share them]</p>"},{"location":"projects/#ideas-experiments","title":"\ufffd Ideas &amp; Experiments","text":"<p>[Maybe a section for project ideas you want to try]</p>"},{"location":"projects/#next-things-i-want-to-try","title":"Next Things I Want to Try:","text":"<ul> <li>[List future project ideas]</li> </ul> <p>Read About My Journey Get in Touch</p>"},{"location":"projects/terraform-proxmox/","title":"Multi-Cluster Proxmox Infrastructure","text":""},{"location":"projects/terraform-proxmox/#enterprise-virtualization-platform","title":"\ud83c\udfd7\ufe0f Enterprise Virtualization Platform","text":"<p>A comprehensive Infrastructure as Code solution for managing multiple Proxmox clusters across different environments. This project demonstrates advanced automation, scalability, and operational excellence in virtualization infrastructure management.</p>"},{"location":"projects/terraform-proxmox/#project-overview","title":"\ud83c\udfaf Project Overview","text":""},{"location":"projects/terraform-proxmox/#challenge","title":"Challenge","text":"<p>Managing multiple Proxmox clusters manually across different environments (datacenter, homeserver, officeserver) was time-consuming, error-prone, and difficult to scale. The need for consistent, reproducible infrastructure deployments was critical for supporting development, staging, and production workloads.</p>"},{"location":"projects/terraform-proxmox/#solution","title":"Solution","text":"<p>Implemented a comprehensive Infrastructure as Code solution using Terraform with modular architecture, enabling automated provisioning and management of virtual machines, LXC containers, and backup systems across multiple Proxmox clusters.</p>"},{"location":"projects/terraform-proxmox/#impact","title":"Impact","text":"<ul> <li>90% reduction in deployment time</li> <li>100% infrastructure automation achieved</li> <li>Zero manual configuration required for new environments</li> <li>Consistent environments across all clusters</li> </ul>"},{"location":"projects/terraform-proxmox/#architecture-design","title":"\ud83c\udfd7\ufe0f Architecture Design","text":""},{"location":"projects/terraform-proxmox/#multi-cluster-layout","title":"Multi-Cluster Layout","text":"<pre><code>infrastructure_overview:\n  clusters:\n    datacenter:\n      location: \"Primary production data center\"\n      proxmox_version: \"8.1\"\n      nodes: 3\n      purpose: \"Production workloads\"\n      specs:\n        cpu_cores: 32\n        memory: \"128GB\"\n        storage: \"ZFS RAID-Z2\"\n        network: \"10Gb backbone\"\n\n    homeserver:\n      location: \"Home lab environment\" \n      proxmox_version: \"8.1\"\n      nodes: 1\n      purpose: \"Development and testing\"\n      specs:\n        cpu_cores: 16\n        memory: \"64GB\"\n        storage: \"ZFS mirror\"\n        network: \"1Gb connection\"\n\n    officeserver:\n      location: \"Office staging environment\"\n      proxmox_version: \"8.1\" \n      nodes: 2\n      purpose: \"Staging and demonstrations\"\n      specs:\n        cpu_cores: 24\n        memory: \"96GB\"\n        storage: \"ZFS RAID-Z1\"\n        network: \"1Gb connection\"\n\n  total_resources:\n    physical_nodes: 6\n    managed_vms: 15+\n    lxc_containers: 35+\n    storage_pools: 9\n    network_vlans: 12\n</code></pre>"},{"location":"projects/terraform-proxmox/#network-architecture","title":"Network Architecture","text":"<pre><code>graph TB\n    subgraph \"External Network\"\n        A[Internet Gateway]\n        B[VPN Server]\n    end\n\n    subgraph \"Datacenter Cluster\"\n        C[DC Node 1]\n        D[DC Node 2] \n        E[DC Node 3]\n        F[Production VLANs]\n    end\n\n    subgraph \"Office Cluster\"\n        G[Office Node 1]\n        H[Office Node 2]\n        I[Staging VLANs]\n    end\n\n    subgraph \"Home Cluster\"\n        J[Home Node 1]\n        K[Development VLANs]\n    end\n\n    A --&gt; B\n    B --&gt; C\n    B --&gt; D\n    B --&gt; E\n    B --&gt; G\n    B --&gt; H\n    B --&gt; J\n\n    C --&gt; F\n    D --&gt; F\n    E --&gt; F\n    G --&gt; I\n    H --&gt; I\n    J --&gt; K\n</code></pre>"},{"location":"projects/terraform-proxmox/#technical-implementation","title":"\ud83d\udee0\ufe0f Technical Implementation","text":""},{"location":"projects/terraform-proxmox/#terraform-modular-architecture","title":"Terraform Modular Architecture","text":"<p>The infrastructure is organized using a modular Terraform architecture for maximum reusability and maintainability:</p> <pre><code># Root module for datacenter cluster\n# cluster-datacenter/main.tf\n\nterraform {\n  required_version = \"&gt;= 1.0\"\n  required_providers {\n    proxmox = {\n      source  = \"telmate/proxmox\"\n      version = \"~&gt; 2.9\"\n    }\n  }\n\n  backend \"s3\" {\n    bucket         = \"terraform-state-proxmox\"\n    key            = \"datacenter/terraform.tfstate\"\n    region         = \"us-west-2\"\n    encrypt        = true\n    dynamodb_table = \"terraform-locks\"\n  }\n}\n\nprovider \"proxmox\" {\n  pm_api_url          = var.pm_api_url\n  pm_api_token_id     = var.pm_api_token_id\n  pm_api_token_secret = var.pm_api_token_secret\n  pm_tls_insecure     = true\n  pm_parallel         = 3\n  pm_timeout          = 600\n}\n\n# Local variables for cluster configuration\nlocals {\n  cluster_name = \"datacenter\"\n  environment  = \"production\"\n\n  common_tags = {\n    Terraform   = \"true\"\n    Environment = local.environment\n    Cluster     = local.cluster_name\n    ManagedBy   = \"dataops-team\"\n  }\n\n  # Network configuration\n  network_config = {\n    bridge = \"vmbr0\"\n    vlan   = 100\n    cidr   = \"192.168.100.0/24\"\n    gateway = \"192.168.100.1\"\n  }\n}\n\n# LXC Container Module\nmodule \"lxc_containers\" {\n  source = \"../modules/lxc\"\n\n  cluster_name = local.cluster_name\n  environment  = local.environment\n  node_name    = var.node_name\n\n  # Container specifications\n  lxc_configs = [\n    {\n      hostname = \"web-server-01\"\n      template = \"ubuntu-22.04-standard\"\n      cores    = 2\n      memory   = 2048\n      disk     = 20\n      ip       = \"192.168.100.10\"\n      purpose  = \"web-services\"\n    },\n    {\n      hostname = \"database-01\"\n      template = \"ubuntu-22.04-standard\"\n      cores    = 4\n      memory   = 8192\n      disk     = 100\n      ip       = \"192.168.100.11\"\n      purpose  = \"database\"\n    },\n    {\n      hostname = \"monitoring-01\"\n      template = \"ubuntu-22.04-standard\"\n      cores    = 2\n      memory   = 4096\n      disk     = 50\n      ip       = \"192.168.100.12\"\n      purpose  = \"monitoring\"\n    }\n  ]\n\n  network_config = local.network_config\n  common_tags    = local.common_tags\n}\n\n# Virtual Machine Module\nmodule \"virtual_machines\" {\n  source = \"../modules/vm\"\n\n  cluster_name = local.cluster_name\n  environment  = local.environment\n  node_name    = var.node_name\n\n  # VM specifications\n  vm_configs = [\n    {\n      hostname     = \"k8s-master-01\"\n      template     = \"ubuntu-22.04-cloud\"\n      cores        = 8\n      memory       = 16384\n      disk         = 100\n      ip           = \"192.168.100.20\"\n      purpose      = \"kubernetes-master\"\n      storage_pool = \"local-zfs\"\n    },\n    {\n      hostname     = \"k8s-worker-01\"\n      template     = \"ubuntu-22.04-cloud\"\n      cores        = 6\n      memory       = 12288\n      disk         = 80\n      ip           = \"192.168.100.21\"\n      purpose      = \"kubernetes-worker\"\n      storage_pool = \"local-zfs\"\n    }\n  ]\n\n  network_config = local.network_config\n  common_tags    = local.common_tags\n}\n\n# Backup Module\nmodule \"backup_system\" {\n  source = \"../modules/backup\"\n\n  cluster_name    = local.cluster_name\n  environment     = local.environment\n  backup_storage  = var.backup_storage\n  backup_schedule = var.backup_schedule\n\n  # Backup configuration\n  backup_config = {\n    retention_daily   = 7\n    retention_weekly  = 4\n    retention_monthly = 6\n    compression       = \"zstd\"\n    encryption        = true\n  }\n\n  # Resources to backup\n  vm_ids  = module.virtual_machines.vm_ids\n  lxc_ids = module.lxc_containers.lxc_ids\n\n  common_tags = local.common_tags\n}\n\n# Monitoring Module\nmodule \"monitoring\" {\n  source = \"../modules/monitoring\"\n\n  cluster_name = local.cluster_name\n  environment  = local.environment\n\n  # Monitoring configuration\n  monitoring_config = {\n    prometheus_enabled = true\n    grafana_enabled    = true\n    alertmanager_enabled = true\n    node_exporter_enabled = true\n  }\n\n  # Alert configuration\n  alert_config = {\n    email_notifications = true\n    slack_notifications = true\n    pagerduty_enabled   = false\n  }\n\n  common_tags = local.common_tags\n}\n</code></pre>"},{"location":"projects/terraform-proxmox/#lxc-container-module","title":"LXC Container Module","text":"<pre><code># modules/lxc/main.tf\n# Reusable LXC container module\n\nterraform {\n  required_providers {\n    proxmox = {\n      source = \"telmate/proxmox\"\n    }\n  }\n}\n\nresource \"proxmox_lxc\" \"container\" {\n  count = length(var.lxc_configs)\n\n  target_node = var.node_name\n  hostname    = var.lxc_configs[count.index].hostname\n\n  # Container template and OS\n  ostemplate   = \"local:vztmpl/${var.lxc_configs[count.index].template}.tar.gz\"\n  password     = var.lxc_password\n  unprivileged = true\n\n  # Resource allocation\n  cores  = var.lxc_configs[count.index].cores\n  memory = var.lxc_configs[count.index].memory\n  swap   = var.lxc_configs[count.index].memory / 2\n\n  # Root filesystem\n  rootfs {\n    storage = var.storage_pool\n    size    = \"${var.lxc_configs[count.index].disk}G\"\n  }\n\n  # Network configuration\n  network {\n    name   = \"eth0\"\n    bridge = var.network_config.bridge\n    ip     = \"${var.lxc_configs[count.index].ip}/${split(\"/\", var.network_config.cidr)[1]}\"\n    gw     = var.network_config.gateway\n    vlan   = var.network_config.vlan\n  }\n\n  # Features\n  features {\n    fuse    = true\n    nesting = true\n    mount   = \"nfs;cifs\"\n  }\n\n  # Startup configuration\n  onboot = true\n  start  = true\n\n  # SSH key management\n  ssh_public_keys = var.ssh_public_keys\n\n  # Tags for organization\n  tags = join(\",\", [\n    var.common_tags.Environment,\n    var.common_tags.Cluster,\n    var.lxc_configs[count.index].purpose,\n    \"lxc\"\n  ])\n\n  lifecycle {\n    create_before_destroy = false\n    ignore_changes = [\n      network,\n      rootfs\n    ]\n  }\n}\n\n# Null resource for post-provisioning configuration\nresource \"null_resource\" \"lxc_provisioning\" {\n  count = length(var.lxc_configs)\n\n  # Trigger on container changes\n  triggers = {\n    container_id = proxmox_lxc.container[count.index].id\n    hostname     = var.lxc_configs[count.index].hostname\n  }\n\n  # Connection configuration\n  connection {\n    type        = \"ssh\"\n    user        = \"root\"\n    private_key = file(var.private_key_path)\n    host        = var.lxc_configs[count.index].ip\n    timeout     = \"300s\"\n  }\n\n  # Wait for container to be ready\n  provisioner \"remote-exec\" {\n    inline = [\n      \"while [ ! -f /var/lib/dpkg/lock-frontend ]; do sleep 1; done\",\n      \"apt-get update\",\n      \"apt-get install -y curl wget htop vim\"\n    ]\n  }\n\n  # Container-specific provisioning\n  provisioner \"remote-exec\" {\n    inline = var.lxc_configs[count.index].purpose == \"database\" ? [\n      \"apt-get install -y postgresql-14\",\n      \"systemctl enable postgresql\",\n      \"systemctl start postgresql\"\n    ] : var.lxc_configs[count.index].purpose == \"web-services\" ? [\n      \"apt-get install -y nginx\",\n      \"systemctl enable nginx\", \n      \"systemctl start nginx\"\n    ] : [\n      \"echo 'Container provisioned successfully'\"\n    ]\n  }\n\n  depends_on = [proxmox_lxc.container]\n}\n</code></pre>"},{"location":"projects/terraform-proxmox/#vm-module-with-cloud-init","title":"VM Module with Cloud-Init","text":"<pre><code># modules/vm/main.tf\n# Advanced VM module with cloud-init support\n\nresource \"proxmox_vm_qemu\" \"vm\" {\n  count = length(var.vm_configs)\n\n  name        = var.vm_configs[count.index].hostname\n  target_node = var.node_name\n  clone       = var.vm_configs[count.index].template\n\n  # VM Configuration\n  agent    = 1\n  cores    = var.vm_configs[count.index].cores\n  sockets  = 1\n  cpu      = \"host\"\n  memory   = var.vm_configs[count.index].memory\n  scsihw   = \"virtio-scsi-pci\"\n  bootdisk = \"scsi0\"\n\n  # Disk configuration\n  disk {\n    slot     = 0\n    size     = \"${var.vm_configs[count.index].disk}G\"\n    type     = \"scsi\"\n    storage  = var.vm_configs[count.index].storage_pool\n    iothread = 1\n    ssd      = 1\n    backup   = true\n  }\n\n  # Network configuration\n  network {\n    model  = \"virtio\"\n    bridge = var.network_config.bridge\n    vlan   = var.network_config.vlan\n  }\n\n  # Cloud-init configuration\n  ciuser     = var.vm_user\n  cipassword = var.vm_password\n  sshkeys    = var.ssh_public_keys\n\n  # IP configuration\n  ipconfig0 = \"ip=${var.vm_configs[count.index].ip}/${split(\"/\", var.network_config.cidr)[1]},gw=${var.network_config.gateway}\"\n\n  # DNS configuration\n  nameserver = \"8.8.8.8 1.1.1.1\"\n  searchdomain = var.search_domain\n\n  # Cloud-init user data\n  cicustom = \"user=local:snippets/${var.vm_configs[count.index].hostname}-user-data.yml\"\n\n  # VM options\n  onboot = true\n  tablet = false\n\n  # Tags\n  tags = join(\",\", [\n    var.common_tags.Environment,\n    var.common_tags.Cluster,\n    var.vm_configs[count.index].purpose,\n    \"vm\"\n  ])\n\n  lifecycle {\n    ignore_changes = [\n      network,\n      disk\n    ]\n  }\n}\n\n# Cloud-init user data template\nresource \"local_file\" \"cloud_init_user_data\" {\n  count = length(var.vm_configs)\n\n  content = templatefile(\"${path.module}/templates/cloud-init-user-data.yml.tpl\", {\n    hostname = var.vm_configs[count.index].hostname\n    purpose  = var.vm_configs[count.index].purpose\n    packages = var.vm_configs[count.index].purpose == \"kubernetes-master\" ? [\n      \"kubeadm\", \"kubelet\", \"kubectl\", \"docker.io\"\n    ] : var.vm_configs[count.index].purpose == \"kubernetes-worker\" ? [\n      \"kubeadm\", \"kubelet\", \"docker.io\"\n    ] : [\n      \"htop\", \"vim\", \"curl\", \"wget\"\n    ]\n  })\n\n  filename = \"/var/lib/vz/snippets/${var.vm_configs[count.index].hostname}-user-data.yml\"\n}\n</code></pre>"},{"location":"projects/terraform-proxmox/#backup-automation-module","title":"Backup Automation Module","text":"<pre><code># modules/backup/main.tf\n# Comprehensive backup automation\n\nresource \"proxmox_virtual_environment_backup_file\" \"vm_backup\" {\n  count = length(var.vm_ids)\n\n  content_type = \"backup\"\n  datastore_id = var.backup_storage\n  node_name    = var.node_name\n\n  # Backup configuration\n  vm_id = var.vm_ids[count.index]\n\n  # Retention policy\n  keep_daily   = var.backup_config.retention_daily\n  keep_weekly  = var.backup_config.retention_weekly\n  keep_monthly = var.backup_config.retention_monthly\n\n  # Compression and encryption\n  compression_type = var.backup_config.compression\n  encryption_key   = var.backup_config.encryption ? var.encryption_key : null\n\n  # Backup scheduling\n  schedule = var.backup_schedule\n\n  # Email notifications\n  notification_mode   = \"auto\"\n  notification_target = var.notification_email\n\n  # Performance settings\n  bandwidth_limit = 100000  # KB/s\n\n  tags = var.common_tags\n}\n\n# LXC backup jobs\nresource \"proxmox_virtual_environment_backup_file\" \"lxc_backup\" {\n  count = length(var.lxc_ids)\n\n  content_type = \"backup\"\n  datastore_id = var.backup_storage\n  node_name    = var.node_name\n\n  # Backup configuration\n  vm_id = var.lxc_ids[count.index]\n\n  # Retention policy\n  keep_daily   = var.backup_config.retention_daily\n  keep_weekly  = var.backup_config.retention_weekly\n  keep_monthly = var.backup_config.retention_monthly\n\n  # Compression and encryption\n  compression_type = var.backup_config.compression\n  encryption_key   = var.backup_config.encryption ? var.encryption_key : null\n\n  # Backup scheduling (offset for LXC)\n  schedule = var.backup_schedule\n\n  tags = var.common_tags\n}\n\n# Backup verification script\nresource \"local_file\" \"backup_verification\" {\n  content = templatefile(\"${path.module}/scripts/verify-backups.sh.tpl\", {\n    backup_storage = var.backup_storage\n    cluster_name   = var.cluster_name\n    environment    = var.environment\n  })\n\n  filename        = \"${path.module}/scripts/verify-backups-${var.cluster_name}.sh\"\n  file_permission = \"0755\"\n}\n</code></pre>"},{"location":"projects/terraform-proxmox/#resource-management","title":"\ud83d\udcca Resource Management","text":""},{"location":"projects/terraform-proxmox/#current-deployment-overview","title":"Current Deployment Overview","text":"Cluster VMs LXC Total CPU Total RAM Storage Datacenter 5 12 48 cores 128GB 2.5TB Office 3 8 28 cores 64GB 1.2TB Home 2 6 16 cores 32GB 800GB Total 10 26 92 cores 224GB 4.5TB"},{"location":"projects/terraform-proxmox/#service-distribution","title":"Service Distribution","text":"<pre><code>services_by_purpose:\n  web_services:\n    count: 8\n    resources: \"16 cores, 32GB RAM\"\n    technologies: [\"Nginx\", \"Apache\", \"Node.js\"]\n\n  databases:\n    count: 6\n    resources: \"24 cores, 64GB RAM\"\n    technologies: [\"PostgreSQL\", \"MongoDB\", \"Redis\"]\n\n  monitoring:\n    count: 4\n    resources: \"8 cores, 16GB RAM\"\n    technologies: [\"Prometheus\", \"Grafana\", \"AlertManager\"]\n\n  development:\n    count: 5\n    resources: \"20 cores, 40GB RAM\"\n    technologies: [\"Docker\", \"Jupyter\", \"Git\"]\n\n  kubernetes:\n    count: 3\n    resources: \"24 cores, 72GB RAM\"\n    technologies: [\"K8s Masters\", \"K8s Workers\"]\n</code></pre>"},{"location":"projects/terraform-proxmox/#operations-automation","title":"\ud83d\udd27 Operations &amp; Automation","text":""},{"location":"projects/terraform-proxmox/#deployment-workflow","title":"Deployment Workflow","text":"<pre><code>#!/bin/bash\n# deploy-cluster.sh - Automated cluster deployment script\n\nset -euo pipefail\n\nCLUSTER_NAME=\"${1:-datacenter}\"\nACTION=\"${2:-plan}\"\n\necho \"\ud83d\ude80 Starting deployment for cluster: $CLUSTER_NAME\"\n\n# Validate prerequisites\ncheck_prerequisites() {\n    echo \"Checking prerequisites...\"\n    command -v terraform &gt;/dev/null 2&gt;&amp;1 || { echo \"Terraform is required\"; exit 1; }\n    command -v ansible &gt;/dev/null 2&gt;&amp;1 || { echo \"Ansible is required\"; exit 1; }\n\n    # Check Proxmox connectivity\n    if ! curl -s -k \"https://192.168.0.99:8006/api2/json/version\" &gt;/dev/null; then\n        echo \"\u274c Cannot connect to Proxmox cluster\"\n        exit 1\n    fi\n    echo \"\u2705 Prerequisites validated\"\n}\n\n# Initialize Terraform\ninit_terraform() {\n    echo \"Initializing Terraform...\"\n    cd \"cluster-${CLUSTER_NAME}\"\n    terraform init -upgrade\n    terraform validate\n    echo \"\u2705 Terraform initialized\"\n}\n\n# Plan deployment\nplan_deployment() {\n    echo \"Planning deployment...\"\n    terraform plan -out=tfplan -var-file=\"terraform.tfvars\"\n    echo \"\u2705 Deployment planned\"\n}\n\n# Apply deployment\napply_deployment() {\n    echo \"Applying deployment...\"\n    terraform apply tfplan\n    echo \"\u2705 Infrastructure deployed\"\n}\n\n# Configure with Ansible\nconfigure_services() {\n    echo \"Configuring services with Ansible...\"\n\n    # Generate dynamic inventory\n    terraform output -json &gt; ../ansible/inventory/terraform_output.json\n\n    # Run Ansible playbooks\n    cd ../ansible\n    ansible-playbook -i inventory/dynamic.py playbooks/site.yml \\\n        --extra-vars \"cluster_name=${CLUSTER_NAME}\"\n\n    echo \"\u2705 Services configured\"\n}\n\n# Verify deployment\nverify_deployment() {\n    echo \"Verifying deployment...\"\n\n    # Health checks\n    cd \"../cluster-${CLUSTER_NAME}\"\n    terraform output vm_ips | jq -r '.[]' | while read ip; do\n        if ping -c 1 \"$ip\" &gt;/dev/null 2&gt;&amp;1; then\n            echo \"\u2705 $ip is reachable\"\n        else\n            echo \"\u274c $ip is not reachable\"\n        fi\n    done\n\n    echo \"\u2705 Deployment verified\"\n}\n\n# Main execution\nmain() {\n    check_prerequisites\n    init_terraform\n\n    case $ACTION in\n        \"plan\")\n            plan_deployment\n            ;;\n        \"apply\")\n            plan_deployment\n            apply_deployment\n            configure_services\n            verify_deployment\n            ;;\n        \"destroy\")\n            terraform destroy -var-file=\"terraform.tfvars\"\n            ;;\n        *)\n            echo \"Usage: $0 &lt;cluster_name&gt; &lt;plan|apply|destroy&gt;\"\n            exit 1\n            ;;\n    esac\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"projects/terraform-proxmox/#monitoring-alerting","title":"Monitoring &amp; Alerting","text":"<pre><code># monitoring/prometheus.yml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nrule_files:\n  - \"rules/*.yml\"\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n          - alertmanager:9093\n\nscrape_configs:\n  - job_name: 'proxmox'\n    static_configs:\n      - targets: ['192.168.0.99:9221', '192.168.0.100:9221', '192.168.0.98:9221']\n    scrape_interval: 30s\n\n  - job_name: 'node-exporter'\n    static_configs:\n      - targets: \n          - '192.168.100.10:9100'  # web-server-01\n          - '192.168.100.11:9100'  # database-01\n          - '192.168.100.12:9100'  # monitoring-01\n    scrape_interval: 15s\n\n  - job_name: 'containers'\n    consul_sd_configs:\n      - server: 'consul:8500'\n        services: ['container-metrics']\n</code></pre>"},{"location":"projects/terraform-proxmox/#performance-metrics","title":"\ud83d\udcc8 Performance Metrics","text":""},{"location":"projects/terraform-proxmox/#deployment-performance","title":"Deployment Performance","text":"<ul> <li>Initial Setup Time: 15 minutes (from zero to running cluster)</li> <li>New VM Deployment: 3 minutes average</li> <li>Configuration Updates: 30 seconds via Ansible</li> <li>Backup Operations: 99.9% success rate, &lt;10 minutes for typical VM</li> </ul>"},{"location":"projects/terraform-proxmox/#resource-utilization","title":"Resource Utilization","text":"<ul> <li>Average CPU Usage: 35% across all clusters</li> <li>Memory Utilization: 60% average, with auto-scaling for peaks</li> <li>Storage Efficiency: 80% utilization with ZFS compression</li> <li>Network Performance: &lt;2ms latency between cluster nodes</li> </ul>"},{"location":"projects/terraform-proxmox/#operational-excellence","title":"Operational Excellence","text":"<ul> <li>Uptime: 99.9% across all managed infrastructure</li> <li>Recovery Time: &lt;5 minutes for automated failover</li> <li>Backup Success Rate: 99.9% with automated verification</li> <li>Security Updates: Automated monthly patching schedule</li> </ul>"},{"location":"projects/terraform-proxmox/#security-compliance","title":"\ud83d\udd12 Security &amp; Compliance","text":""},{"location":"projects/terraform-proxmox/#security-measures","title":"Security Measures","text":"<ul> <li>Network Segmentation: VLANs isolate different environments</li> <li>Access Control: SSH key-based authentication only</li> <li>Encryption: All backups encrypted with industry-standard algorithms</li> <li>Monitoring: Real-time security event monitoring and alerting</li> </ul>"},{"location":"projects/terraform-proxmox/#compliance-best-practices","title":"Compliance &amp; Best Practices","text":"<ul> <li>Infrastructure as Code: All infrastructure versioned and auditable</li> <li>Change Management: GitOps workflow for all infrastructure changes</li> <li>Documentation: Comprehensive documentation for all procedures</li> <li>Disaster Recovery: Tested backup and recovery procedures</li> </ul>"},{"location":"projects/terraform-proxmox/#future-enhancements","title":"\ud83d\ude80 Future Enhancements","text":""},{"location":"projects/terraform-proxmox/#planned-improvements","title":"Planned Improvements","text":"<ul> <li>Kubernetes Integration: Full container orchestration deployment</li> <li>Service Mesh: Istio implementation for advanced traffic management</li> <li>GitOps Advanced: ArgoCD integration for continuous deployment</li> <li>Multi-Cloud: Terraform modules for AWS/Azure integration</li> </ul>"},{"location":"projects/terraform-proxmox/#scaling-considerations","title":"Scaling Considerations","text":"<ul> <li>Additional Clusters: Template ready for new geographic locations</li> <li>High Availability: Proxmox clustering for zero-downtime operations</li> <li>Performance Optimization: Resource right-sizing based on usage analytics</li> <li>Cost Optimization: Automated resource scaling based on demand</li> </ul>"},{"location":"projects/terraform-proxmox/#technical-documentation","title":"\ud83d\udcda Technical Documentation","text":""},{"location":"projects/terraform-proxmox/#repository-structure","title":"Repository Structure","text":"<pre><code>terraform-proxmox/\n\u251c\u2500\u2500 cluster-datacenter/\n\u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u251c\u2500\u2500 outputs.tf\n\u2502   \u2514\u2500\u2500 terraform.tfvars\n\u251c\u2500\u2500 cluster-homeserver/\n\u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u251c\u2500\u2500 outputs.tf\n\u2502   \u2514\u2500\u2500 terraform.tfvars\n\u251c\u2500\u2500 cluster-officeserver/\n\u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u251c\u2500\u2500 outputs.tf\n\u2502   \u2514\u2500\u2500 terraform.tfvars\n\u251c\u2500\u2500 modules/\n\u2502   \u251c\u2500\u2500 lxc/\n\u2502   \u251c\u2500\u2500 vm/\n\u2502   \u251c\u2500\u2500 backup/\n\u2502   \u2514\u2500\u2500 monitoring/\n\u251c\u2500\u2500 ansible/\n\u2502   \u251c\u2500\u2500 playbooks/\n\u2502   \u251c\u2500\u2500 roles/\n\u2502   \u2514\u2500\u2500 inventory/\n\u2514\u2500\u2500 scripts/\n    \u251c\u2500\u2500 deploy-cluster.sh\n    \u251c\u2500\u2500 backup-verify.sh\n    \u2514\u2500\u2500 health-check.sh\n</code></pre>"},{"location":"projects/terraform-proxmox/#key-learnings","title":"Key Learnings","text":"<ol> <li>Modular Design: Reusable modules significantly reduce development time</li> <li>State Management: Remote state with locking prevents configuration drift</li> <li>Automation: Complete automation reduces human error and improves consistency</li> <li>Monitoring: Comprehensive monitoring enables proactive issue resolution</li> </ol> <p>Ready to implement Infrastructure as Code for your organization?</p> <p>View More Projects Contact Me</p>"}]}